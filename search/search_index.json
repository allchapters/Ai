{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Task1/","title":"Pre-Requisites and Setup","text":""},{"location":"Task1/#google-colab-accessing-google-colab-and-creating-account","title":"Google Colab - Accessing Google Colab and creating account","text":"<p>Google Colab is a free, cloud-based platform that provides a convenient environment for running notebooks. If you want to create a machine learning model but don't have a computer that can handle the workload, Google Colab is the platform for you. In our lab, we will be using Google Colab to test and run our code. However, if you have your own Python environment and prefer to run the code on your local machine, please feel free to do so.</p> <p>Here are some reasons why using Google Colab can be beneficial for this lab:</p> <ul> <li>Free Access to GPUs and TPUs: Google Colab offers free access to powerful GPUs and TPUs, which can significantly accelerate the training and fine-tuning of machine learning models.</li> <li>No Setup Required: With Colab, there is no need to set up your local environment. Everything runs in the cloud, which saves time and avoids configuration issues.</li> <li>Easy Collaboration: Colab notebooks can be easily shared and collaborated on with team members, making it an ideal tool for collaborative projects.</li> <li>Integration with Google Drive: Colab integrates seamlessly with Google Drive, allowing you to save and manage your work conveniently.</li> <li>Pre-installed Libraries: Many popular machine learning libraries, including TensorFlow and PyTorch, come pre-installed in Colab, making it easy to start working on your projects immediately.</li> </ul>"},{"location":"Task1/#getting-started-with-google-colab","title":"Getting Started With Google Colab","text":"<p>To start working with Google Collaboratory Notebook you first need to log in to your Google account, then go to this link Google Colab</p> <ul> <li>Create a new Jupyter Notebook  </li> </ul> <p></p> <ul> <li>On creating a new notebook, it will create a Jupyter notebook with Untitled0.ipynb and save it to your google drive in a folder named Colab Notebooks. Now as it is essentially a Jupyter Notebook, all commands of Jupyter Notebooks will work here. </li> </ul> <p></p> <ul> <li> <p>There might be times when we need to fine-tune models or perform specific tasks that require changing the runtime environment in Colab. Google Colab offers different runtime environments that can be selected based on your requirements:</p> </li> <li> <p>Python Versions: You can select between different versions of Python (e.g., Python 2 or Python 3)depending on the compatibility of the code and libraries. We will be using Python3 for our lab.</p> </li> <li> <p>Hardware Accelerators: Colab provides access to hardware accelerators, which can be particularly useful for intensive computations. You can choose between:</p> </li> </ul> <pre><code>None: No hardware acceleration, suitable for basic tasks.\n\nGPU: Accelerate your computations with a Graphics Processing Unit.\n\nTPU: Use a Tensor Processing Unit for even faster performance, especially beneficial for deep learning tasks.\n</code></pre> <ul> <li> <p>Click the arrow next to \u201cConnect\u201d to open the dropdown</p> </li> <li> <p>Change Runtime Environment: Click the \u201cRuntime\u201d dropdown menu at the top of the Colab interface.</p> </li> </ul> <p></p> <ul> <li> <p>Select \u201cChange runtime type\u201d: This will open a dialog box where you can configure the runtime environment.</p> </li> <li> <p>Select Python Version: Choose Python 3 from the \u201cRuntime type\u201d dropdown menu.</p> </li> <li> <p>Select Hardware Accelerator: From the \u201cHardware accelerator\u201d dropdown menu, choose  GPU, or TPU . </p> <ul> <li> <p>GPU (Graphics Processing Unit): Best for tasks requiring extensive parallel processing, such as training neural networks (the main focus in our lab guide).</p> </li> <li> <p>TPU (Tensor Processing Unit): Optimized for deep learning tasks</p> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>Save Settings: Click \u201cSave\u201d to apply the changes.</p> </li> <li> <p>New Cell: Whenever you want to copy the code in Google Colab and run it, be sure to click on + Code to add a new code cell.</p> </li> </ul> <p></p> <ul> <li>Execute Code: Click the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\" while the cell is selected.</li> </ul> <p></p>"},{"location":"Task1/#notes-on-gpu-and-tpu-access","title":"Notes: On GPU and TPU Access:","text":"<p>While Google Colab offers free access to GPUs and TPUs, there are limitations. For more consistent access to high-performance GPUs and TPUs, you might need to subscribe to Colab Pro or Colab Pro+ accounts. These paid plans provide priority access to better hardware, longer runtimes, and more memory.</p>"},{"location":"Task1/#using-huggingface-hub-to-share-our-datasets","title":"Using Huggingface Hub to share our Datasets","text":"<p>In this lab, we will be utilizing the Hugging Face Hub to load our custom datasets. Hugging Face provides an extensive repository of datasets that can be easily integrated into your machine learning workflows. For the purposes of this lab, we will demonstrate how to access/upload and use our custom datasets effectively.</p> <p>However, when fine-tuning models in your own work environment, especially if you are using private data, there are important considerations to keep in mind:</p> <ul> <li>Private Datastores: If you are working with proprietary or sensitive data, it is crucial to use your organization's secure datastores. Ensure that all data handling complies with your organization's data privacy policies and regulations.</li> <li>Hugging Face Datasets: If you prefer to use Hugging Face for dataset storage and management, make sure to mark your datasets as private. This setting ensures that your data cannot be accessed by anyone outside your organization, maintaining the confidentiality and integrity of your information. Please refer to Huggingface documentation for more info.</li> </ul> <p>Few more Condsideration</p> <ul> <li>Upload Dataset: When uploading your dataset to Hugging Face, choose the appropriate privacy settings. You can set your dataset to private during the upload process.</li> <li>Check Permissions: Regularly review and manage the permissions of your datasets to ensure they remain private and secure.</li> <li>Collaborator Access: If you need to share the dataset with specific team members, use the Hugging Face interface to grant access to trusted collaborators only.</li> </ul> <p>By following these guidelines, you can ensure that your data remains secure while leveraging the powerful tools and resources provided by Hugging Face. This approach not only enhances your workflow efficiency but also upholds the best practices in data security and privacy.</p>"},{"location":"Task1/#accessing-hugging-face-hub-and-creating-account","title":"Accessing Hugging Face Hub and creating account","text":"<p>Hugging Face can be accessed by browsing to huggingface</p> <p></p>"},{"location":"Task1/#signing-up","title":"Signing up","text":"<ul> <li>Browse to Hugging Face home page and click on Sign up. Follow the instructions as per below images</li> </ul> <ul> <li>Please check your email address for a confirmation link and click to verify your account </li> </ul> <ul> <li>Organization Creation (Optional): While you can upload datasets and fine-tune models directly on Hugging Face without creating an organization, you have the option to create an organization on Hugging Face. This can be particularly useful for team collaboration, as it allows you to upload all your datasets and models in one centralized location.</li> </ul> <ul> <li>Access Your Models and Datasets: The same can be accessed by clicking your profile picture on the top right corner of the Hugging Face website. This will take you to your personal dashboard where you can view and manage your models and datasets.</li> </ul> <ul> <li>At this stage, you will see no models or datasets created under your account.</li> </ul>"},{"location":"Task1/#hugging-face-api-keys","title":"Hugging Face API Keys","text":"<p>Create an API Key: As we will be uploading our datasets to the Hugging Face Hub, we need to create an API key for our account. This API key will be used to authenticate and interact with the Hugging Face services programmatically.</p> <p>you can browse to huggingface API Key</p> <p>or </p> <p>Click on your profile picture &gt; Settings &gt; Access Tokens</p> <p></p> <p></p> <p>Under the \"Access Tokens\" section, click on \"Create new token.\" You will see options to select the token type and provide a token name. For example, you might name your token \"Webexone\" and select the approperiate permissions.  </p> <ul> <li> <p>Fine-grained: tokens with this role can be used to provide fine-grained access to specific resources, such as a specific model or models in a specific organization. This type of token is useful in production environments, as you can use your own token without sharing access to all your resources.</p> </li> <li> <p>Read: tokens with this role can only be used to provide read access to repositories you could read. That includes public and private repositories that you, or an organization you\u2019re a member of, own. Use this role if you only need to read content from the Hugging Face Hub  (e.g. when downloading private models or doing inference).</p> </li> <li> <p>Write: tokens with this role additionally grant write access to the repositories you have write access to. Use this token if you need to create or push content to a repository (e.g., when training a model or modifying a model card).</p> </li> </ul> <p>As we have a lab envoirnment we will be using the \"Write\" permission. This token will have read and write access to all your resources and can make calls to inference API on your behalf, as shown in the image below.</p> <p></p> <p>Save and Secure the Token: Once the token is generated, save it securely, as it will be needed to access and manage your datasets via the API. The image below is for reference only. Make sure to use and save the token you generate.</p> <p></p>"},{"location":"Task1/#accessing-hugging-face-api-in-google-colab","title":"Accessing Hugging Face API in Google Colab","text":"<ul> <li>Open the Google Colab notebook and navigate to the new \u201cSecrets\u201d section in the sidebar by clicking the \"key icon\u201d</li> </ul> <ul> <li> <p>Click on \u201cAdd a new secret.\u201d Enter the name example: HF_TOKEN and value of the secret. Note: The name is permanent once set. </p> </li> <li> <p>The list of secrets is global across all your notebooks.</p> </li> <li> <p>Use the \u201cNotebook access\u201d toggle to grant or revoke access to a secret for each notebook.</p> </li> </ul> <p></p>"},{"location":"Task1/#using-secrets-as-environment-variables","title":"Using Secrets as Environment Variables","text":"<ul> <li>For Python modules requiring API keys as environment variables, use the below snippet:</li> </ul> <pre><code># Import Colab Secrets userdata module\n\nfrom google.colab import userdata\nimport os\n\n# Set other API keys similarly\nos.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n</code></pre>"},{"location":"Task1/#getting-started-with-langchain","title":"Getting Started with LangChain","text":"<p>LangChain is a framework that enables developers to build applications using large language models (LLMs). It connects with various AI models, data sources, and APIs, allowing the creation of complex work flows for tasks such as question answering, chatbot interactions, and document summarization. We will explore LangChain in more detail in the later section.</p> <p>Let's explore how to create a LangChain account and obtain the API key, which we will use later in our lab. </p> <p>To start building applications with LangChain, you'll need an API key. This key allows your applications to securely connect with LangChain's services, ensuring proper authentication and usage tracking.</p> <p>Lets start by visiting the official LangChain website and create an account. You'll need to enter some basic information about yourself or your organization. In this example, I will be using my Google account to sign up.</p> <p></p> <p>To create an API key head to the Settings page. Then click Create API Key.</p> <p></p> <p>Lets create a Personal Access Token</p> <p></p>"},{"location":"Task1/#setup-langchain-envoirnment-sample-code","title":"Setup LangChain envoirnment - Sample Code","text":"<ul> <li>In the code snippet below, we'll configure our environment to enable tracing of AI calls and set up the API key for interacting with LangChain's services, as we will be using these in the upcoming section.</li> </ul> Sample Code<pre><code>!pip install langchain-community\nexport LANGCHAIN_TRACING_V2=true\nexport LANGCHAIN_API_KEY=&lt;your-api-key&gt;\n</code></pre>"},{"location":"Task1/#optional-step-running-ollama-locally","title":"Optional Step - Running Ollama locally","text":"<p>Have you ever used ChatGPT and amazed at its ability to understand and respond to your queries? But did you know that you can also harness the power of open-source models using Ollama? With Ollama, you can download and interact with these models directly, getting responses that are tailored to your needs.</p> <p>In this lab, I'll show you how to install Ollama locally on your machine and start using it to generate responses. You'll learn how to download and load open-source models, and then use Ollama's intuitive interface to interact with them. More info for Ollama can be found here</p> <p> Note: The steps below are provided for informational purposes. If you are using the demo laptop in this lab, feel free to follow these instructions. However, if you are using your personal or work machine, please ensure that you have the necessary privileges and authorization from your organization's administrator to install Ollama locally. </p>"},{"location":"Task1/#prerequisites-for-installing-ollama-locally","title":"Prerequisites for Installing Ollama Locally","text":"<ul> <li>Ensure that Docker is installed and running on your machine. It is available for various operating systems, including macOS, Windows, and Linux. You can download it from the official Docker website and follow the installation instructions for your specific OS.</li> </ul>"},{"location":"Task1/#installing-ollama-locally","title":"Installing Ollama Locally","text":"<p> Note: Since we're using a Mac, I'll demonstrate how you can install it on macOS. </p> <ul> <li> <p>First, you need to install Ollama. You can download it here and clicking on the download button. Follow the instructions.</p> </li> <li> <p>Once installed, Ollama functions as a command-line application, allowing you to interact with it directly through the terminal. To get started, open the terminal and enter the following command:</p> </li> </ul> SHELL<pre><code>ollama\n</code></pre> <p>It will output the below commands</p> <p></p> <ul> <li>Ollama supports a list of models available here. Below are some example models that can be downloaded:</li> </ul> <p></p> <ul> <li>To download a model, for example, gemma:2b, which is a lighter model, we can simply type:</li> </ul> SHELL<pre><code>ollama pull gemma:2b\n</code></pre> <p></p> <ul> <li>After pulling the model we can interact with it in the terminal by typing:</li> </ul> SHELL<pre><code>ollama run gemma:2b\n</code></pre> <p></p> <ul> <li>We can now ask our question directly in the terminal</li> </ul> <p></p> <ul> <li>To remove model you can type</li> </ul> SHELL<pre><code>ollama rm gemma:2b\n</code></pre> <ul> <li>To see the models installed, just enter </li> </ul> SHELL<pre><code>ollama list\n</code></pre>"},{"location":"Task1/#web-interface","title":"Web Interface","text":"<p>Once we've installed Ollama, we can interact with it directly through the terminal, as demonstrated earlier. But what if you prefer a web interface? There are several open-source tools available, but I'll show you how to use Open WebUI, which was previously known as Ollama WebUI.</p> <p>Open WebUI is a versatile, feature-rich, and user-friendly web interface that you can host yourself and use entirely offline. It offers a ChatGPT-style interface, allowing you to interact with language models running on locally (Ollama). This tool is especially useful for those who want to run language models locally or in a self-hosted environment, ensuring both data privacy and control.</p> <ul> <li>After installing Ollama, simply run the following Docker command to set up the interface</li> </ul> SHELL<pre><code>docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main\n</code></pre> <p></p> <ul> <li>Once installed, you can access Open WebUI at localhost. </li> </ul> <p>Note: Before accessing the web interface, you will be prompted to create an account. Please follow the instructions.</p> <ul> <li>In the dropdown menu, you'll see the model you previously installed via the terminal (gemma:2b).</li> </ul> <p></p> <ul> <li>You can now Interact with the model via WebUi</li> </ul> <p></p> <ul> <li>You no longer need to use the terminal to download a model. Simply navigate to the \u201cSettings --&gt; Models\u201d section in the WebUI, and select the specific model you want to download.</li> </ul> <p></p> <ul> <li>After installing, all the installed models will be displayed in the UI.</li> </ul> <p></p> <p>Note: Using LangChain with Ollama allows you to leverage Ollama's capabilities within the LangChain ecosystem. You can find more information here </p>"},{"location":"Task2/","title":"AI/ML Revolution Unveiled","text":""},{"location":"Task2/#different-types-of-ai","title":"Different Types of AI","text":"<p>Remember: Subset of AI is ML and kind of ML is Gen-AI . LLM e.g GPT are subset of Gen-AI </p>"},{"location":"Task2/#neural-network","title":"Neural Network","text":"<p> Note: The way nodes are connected is called weights </p> <p></p> <p></p> <p></p> <p> Note: Neural network rely on training data to learn and improve their accuracy over time. We used supervised learning to train the algorithm.  There are multiple types of Neural network other than the Feed forward network that we have defined here. Example: CNN: Convolutional Neural Network - Unique architecture for identifying patterns like image recognition, or RNN: Recurrent neural network, that uses time series data to make prediction about future events like sales forecasting. </p>"},{"location":"Task20/","title":"Logging Out and Ending the Lab Session","text":"<p>Close out of the Chrome Browser Log out from HuggingFace Log out from Postman Log out from OpenAI Close the Windows VM browser tab</p> <p>Time to wrap things and with the Lab Conclusion.</p>"},{"location":"Task3/","title":"Tokenization","text":"<ul> <li>Language models have a limit on how much text they can handle at once, known as their context window. While these limits are expanding, research shows that LLM's often perform better when provided with less, but more relevant, information. However, selecting the most relevant information is straightforward for humans but challenging for computers. </li> </ul> <p>A common approach to manage large amounts of data is to break it down into smaller, more manageable parts; this is a process often referred to as Tokenization or Chunking. Tokenization is a key step in this process, where raw text is divided into smaller units, called tokens, which can then be processed by a neural network.</p> <p></p> <p>In order to do this you need to pick a chunk strategy. Just to name a few:</p> <ul> <li> <p>Word-Based: A simple and straightforward method that most of us would propose is to use word-based tokens, splitting the text by spaces.</p> </li> <li> <p>Character based tokenization: Individual words are considered as tokens . Lot of computing resources needed as now e.g for a 3 word serentence where you might need 3 tokens now 15 \u2013 20 tokens needed </p> </li> <li> <p>Sentence Based: We need a .(fullstop) for it to work </p> </li> </ul> <p> Note: We've all heard of GPT and OpenAI. They utilize a tokenization method called Byte Pair Encoding (BPE), which is a middle ground between word-based and character-based tokenization. In BPE, words are broken down into smaller character sequences that the model encountered during training, allowing it to make informed predictions. </p>"},{"location":"Task3/#why-do-we-need-tokenization-or-text-splitting","title":"Why do we need Tokenization or Text Splitting?","text":"<p>Historically, applications perform better when they are provided with your own data. However, you can't feed unlimited data to your LLMs due to two key limitations:</p> <ul> <li> <p>Context window limit: LLMs have a finite context window for processing data.</p> </li> <li> <p>Signal-to-noise ratio (SNR): LLMs perform better when the SNR is high, meaning the information provided is useful, relevant, and clear. Clear, unambiguous instructions help the model deliver more accurate and detailed results, while ambiguous or complex inputs can lead to less accurate or incomplete outputs.</p> </li> </ul> <p>As noted, chunking or splitting refers to breaking your data into smaller, manageable pieces.</p> <p></p>"},{"location":"Task3/#task-1-log-into-the-lab-environment","title":"Task 1: Log into the Lab Environment","text":"<ul> <li>Open Google Colab and create a new notebook. Click on \"File\" &gt; \"New notebook\". Please refer to the following section to create Google Colab account.</li> </ul> <ul> <li>Make sure you are connected to a runtime. For this task, you can use the CPU as the runtime environment.</li> </ul> <p>Reminder: Whenever you want to copy the code in Google Colab and run it, be sure to click on + Code to add a new code cell.</p> <p></p> <p>Reminder: Click the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\" while the cell is selected.</p> <p></p>"},{"location":"Task3/#manual-testing-for-chunking","title":"Manual testing for Chunking","text":"<p>We will create Chunks of 35 characters, so first 35 characters as chunk 1 ,next 35 characters chunk2 and so on </p> <pre><code># Manual Splitting\ntext = \"\"\"WebexOne is an annual in-person and virtual event that takes place over four days. It\u2019s an event focused on AI collaboration and customer experience.  \nIt features a range of activities such as insightful breakout sessions, technical training courses, hands-on labs, inspiring keynotes, epic entertainment, a solutions showcase and expo, customer awards, meet the experts, 1:1 executive meetings, a partner program, and more!\"\"\"\n\n# Create a list that will hold your chunks\nchunks = []\n\nchunk_size = 35 \n\n# Run through the a range with the length of your text and iterate every chunk_size you want\nfor i in range(0, len(text), chunk_size):\n    chunk = text[i:i + chunk_size]\n    chunks.append(chunk)\n\nprint(chunks)\n</code></pre> <p>Note:  The image below is provided to highlight the code and help you understand how chunking works. Not an actual output from print(chunks) command.</p> <p></p> <p>Note:  The text contained a total of 422 characters, divided into 13 chunks. Problem with the above chunking technique is that it got split at 'r'. How we know when to chunk . Before we look into that. Lets look into Langchain Splitter </p>"},{"location":"Task3/#langchain-text-splitter","title":"LangChain Text Splitter","text":"<ul> <li>Let's retrieve the langchain library from the Python Package. In the example below, we'll configure the chunk_overlap parameter, which ensures that our chunks are blended together\u2014meaning the end of chunk 1 will overlap with the beginning of chunk 2. Please copy the below command in Colab and press run to execute each cell individually.</li> </ul> <pre><code>!pip install langchain\n</code></pre> <pre><code>text = \"\"\"WebexOne is an annual in-person and virtual event that takes place over four days. It\u2019s an event focused on AI collaboration and customer experience.  \nIt features a range of activities such as insightful breakout sessions, technical training courses, hands-on labs, inspiring keynotes, epic entertainment, a solutions showcase and expo, customer awards, meet the experts, 1:1 executive meetings, a partner program, and more!\"\"\"\n</code></pre> <p><pre><code>from langchain.text_splitter import CharacterTextSplitter\n\ntext_splitter = CharacterTextSplitter(chunk_size=35, chunk_overlap=3, separator='', strip_whitespace=False) # seprate means you splt by charector\n\na = text_splitter.create_documents([text])\n\nprint(a)\n</code></pre> Note: Seprators are characters sequence you wanna split on. The image below is provided to highlight the code and help you understand how chunking works. </p> <p></p>"},{"location":"Task3/#recursive-character-text-splitting","title":"Recursive character Text Splitting","text":"<p>In the previous example, when we used character Splitting, we split the text based on a fixed number of character. Specifically, we divided the text into chunks of 35 characters each.</p> <p>However, with Recursive Text Splitting (RTS), the process is more dynamic and considers the physical structure of the text to determine the appropriate chunk size.</p> <p>Here\u2019s how RTS works:</p> <ul> <li> <p>Instead of relying on a static number of characters, RTS examines the structure of the document.</p> </li> <li> <p>It begins by identifying the largest logical divisions, such as paragraphs, and splits the text at each double newline (indicating paragraph breaks).</p> </li> <li> <p>If any of these chunks are still too large, RTS will then move to the next level of separation, which is single newlines (often indicating sentences or list items).</p> </li> <li> <p>If necessary, it will continue to break down the text further, using spaces and eventually individual characters as separators.</p> </li> </ul> <p>With RTS, you don\u2019t need to manually specify the chunk size by character count. You simply pass your text to the RTS process, and it will intelligently determine how to split the text based on its structure, resulting in chunks that are logically organized and more contextually meaningful.</p> <p></p> <pre><code>text = \"\"\"WebexOne is an annual in-person and virtual event that takes place over four days. It\u2019s an event focused on AI collaboration and customer experience.  \nIt features a range of activities such as insightful breakout sessions, technical training courses, hands-on labs, inspiring keynotes, epic entertainment, a solutions showcase and expo, customer awards, meet the experts, 1:1 executive meetings, a partner program, and more!\"\"\"\n</code></pre> <p><pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size = 35, chunk_overlap = 0)\n# how many chunks we have \nprint(len(text_splitter.create_documents([text])))\n\ntext_splitter.create_documents([text])\n</code></pre> Note: We avoid splitting in the middle of words by using spaces as one of the separators, ensuring that each chunk ends on a complete word. This is crucial because RCS (Recursive Character Splitting) helps maintain the context within sentences by keeping related words together. The image below is provided to highlight the code and help you understand how chunking works.</p> <p></p> <p>Note:If you're new to AI, I would personally recommend starting with Recursive Text Splitting (RTS). </p>"},{"location":"Task3/#document-level-splitting","title":"Document Level Splitting","text":"<p>So far, we've been working with splitting regular documents. But what if we have markdown files or  PDF or Python documentation? There's a better way to handle those cases, and that's where specialized document splitting comes into play.</p>"},{"location":"Task3/#pdf-with-table","title":"PDF with table","text":"<p>PDFs often contain tables and other structured data that can be challenging to split accurately using character-based methods. For PDFs, it's important to extract and chunk all elements, including tables, effectively. We'll accomplish this using the Unstructured library, which is specifically designed for handling such tasks. If you have a large collection of PDFs, Unstructured is an excellent tool to manage them efficiently.</p> <p></p> <ul> <li>Install relevant libraries including Unstructured</li> </ul> <pre><code>!pip install scikit-learn langchain_community unstructured[all-docs] unstructured pdfminer pdfminer.six pdf2image pillow_heif opencv-python unstructured_inference pytesseract unstructured_pytesseract python-dotenv openai\n</code></pre> <p></p> <pre><code>!apt-get install -y poppler-utils &amp;&amp; apt-get install -y tesseract-ocr\n</code></pre> <p></p> <ul> <li>We will upgrade nltk to version 3.9.1</li> </ul> <p><pre><code>!pip install --upgrade nltk\n</code></pre> </p> <ul> <li> <p>Let's load our PDF files into Google Colab. For this example, we can use the Cisco Financial Results. Please Download the file here as we will be using in the next step.</p> </li> <li> <p>In Google Colab, click on the Folder tab. Click and select New Folder to create a folder named \"data.\"</p> </li> </ul> <p></p> <ul> <li>On your newly created 'data' folder, click [...] then select Upload.</li> </ul> <p></p> <ul> <li>Choose your CiscoReport.pdf file and click Open</li> </ul> <p></p> <ul> <li>We'll use the following code from the Unstructured library to demonstrate how tables can be extracted</li> </ul> <p>Note:  In the below code filename, is the relative path where your PDF is uploaded. To get the correct file path, click on the 'data' folder, select your PDF file, click on [...] and choose Copy Path. Example: filename = \"/content/data/CiscoReport.pdf\"  <pre><code>import os\nfrom unstructured.partition.pdf import partition_pdf\nfrom unstructured.staging.base import elements_to_json\n\n# Let's load up our PDF and then partition it.\nfilename = \"&lt;Your-File-Path&gt;\"# Use relative path since the file is in the same directory.\n# Extracts the elements from the PDF\nelements = partition_pdf(\n    filename=filename,\n    extract_images_in_pdf=True,\n    strategy=\"hi_res\",\n    infer_table_structure=True,\n    hi_res_model_name=\"yolox\"\n)\n# Let's look at our elements\nprint(elements)\n</code></pre> <p></p> <ul> <li>Lets grab element 37 as its the one where our table is</li> </ul> <p><pre><code>elements[37].metadata.text_as_html\n</code></pre> OUTPUT</p> <pre><code>'&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Revenue&lt;/th&gt;&lt;th&gt;$8.51 - $8.53 Billion&lt;/th&gt;&lt;th&gt;$34.5 - $34.7 Billion&lt;/th&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;Y/Y Growth&lt;/td&gt;&lt;td&gt;~10%&lt;/td&gt;&lt;td&gt;~10%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;FX Impact)&lt;/td&gt;&lt;td&gt;no impact&lt;/td&gt;&lt;td&gt;no impact&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;GAAP Operating Margin&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;~11.4%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Non-GAAP Operating Margin\u2019)&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;~28.0%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;GAAP Earnings per Share?)&lt;/td&gt;&lt;td&gt;$0.79 - $0.80&lt;/td&gt;&lt;td&gt;$2.67 - $2.69&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Non-GAAP Earnings per Share()&lt;/td&gt;&lt;td&gt;$1.89 - $1.90&lt;/td&gt;&lt;td&gt;$7.41 - $7.43&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Operating Cash Flow Growth (Y/Y)\u00b0)&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;td&gt;16% - 17%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;Current Remaining Performance Obligation Growth (Y/Y)&lt;/td&gt;&lt;td&gt;~10%&lt;/td&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;'\n</code></pre> <ul> <li>Tables are straightforward for humans to read, but they aren't as easy for language models to interpret. Language models are typically trained on HTML tables, so when you pass HTML-formatted tables to an LLM, it will better understand the structure and content. You can paste the HTML into an HTML viewer to see how it looks.</li> </ul> <p></p> <p>Note: That's how you handle tables within a PDF.</p>"},{"location":"Task3/#multi-modal-text-images","title":"Multi-Modal (Text + images)","text":"<p>What if a PDF or other document contains images? How can you extract them? We'll use the Unstructured library again to handle this.</p>"},{"location":"Task3/#extract-images-within-pdf","title":"Extract Images within PDF","text":"<ul> <li>Install relevant libraries</li> </ul> <pre><code>from typing import Any\nfrom pydantic import BaseModel\nfrom unstructured.partition.pdf import partition_pdf  \n</code></pre> <pre><code>filepath = \"/content/data/CiscoReport.pdf\"\n# Get elements\nraw_pdf_elements = partition_pdf(\n    filename=filepath,\n\n    # Using pdf format to find embedded image blocks\n    extract_images_in_pdf=True,\n\n    # Use layout model (YOLOX) to get bounding boxes (for tables) and find titles\n    infer_table_structure=True,\n\n    # Specifies the strategy to be used for chunking the text. In our case, it will chunk the text based on titles found in the document.\n    chunking_strategy=\"by_title\",\n\n    # Sets a hard limit on the number of characters allowed in each chunk. \n    max_characters=4000,\n    # This function will attempt to create a new chunk after every 3,800 characters, allowing some flexibility while chunking.\n    new_after_n_chars=3800,\n    # If a chunk has fewer than 2,000 characters, the function will \n    # attempt to combine it with neighboring text blocks to create a larger, more meaningful chunk.\n    combine_text_under_n_chars=2000\n)\n</code></pre> <p>Note: You'll notice that a folder called 'figures' is created, where all the extracted images are stored. To make these images more useful, we can generate embeddings for them, which can later be used for semantic search. Typically, embedding models are specialized\u2014they either handle text or images, but not both. This means the vector lengths won't align, and using different models for text and image embeddings can complicate similarity searches. However, the CLIP model can generate embeddings for both images and text, making it a versatile option for this purpose. To simplify things, we can first create a text summary for each image, and then generate embeddings based on those summaries. This approach allows us to leverage text-based semantic search while still incorporating the visual information from the images. </p> <p></p> <ul> <li>You can view the image by double clicking it </li> </ul> <p>Note: After extracting images from a PDF or any other source, you can use the below method to send them to a multimodal model like GPT-4o to understand and generate insights about the images. This approach allows you to combine textual and visual data for a richer understanding of the content.</p>"},{"location":"Task3/#set-openai-token-multimodal","title":"Set OpenAI token - MultiModal","text":"<p>Note: First, create an account from the OpenAI official website.</p> <ul> <li>Create a new project API key by browsing to API Keys web page. Select Create new secret key. The API key is automatically generated. Save the API Key as we will be using it in the later steps . The below API key is for reference only . Also when creating an API key run with default settings.</li> </ul> <p></p> <p></p> <ul> <li>Within your existing Google Colab notebook navigate to the new \u201cSecrets\u201d section in the sidebar.</li> </ul> <p></p> <ul> <li> <p>Click on \u201cAdd a new secret.\u201d Enter the name example: OPENAI_API_KEY and value of the secret(the API key created above). Note: The name is permanent once set. </p> </li> <li> <p>The list of secrets is global across all your notebooks.</p> </li> <li> <p>Use the \u201cNotebook access\u201d toggle to grant or revoke access to a secret for each notebook.</p> </li> </ul> <p></p>"},{"location":"Task3/#lets-import-and-load-the-envoirnment-variables-in-the-same-notebook","title":"Let's import and load the envoirnment variables in the same notebook","text":"<pre><code>from langchain.chat_models import ChatOpenAI\nfrom langchain.schema.messages import HumanMessage\nimport os\nfrom dotenv import load_dotenv\nfrom PIL import Image\nimport base64\nimport io\nload_dotenv()\n</code></pre>"},{"location":"Task3/#incorporating-secrets-into-your-code-we-will-use-it-later-in-our-lab","title":"Incorporating Secrets into Your Code - We will use it later in our lab","text":"<pre><code>import os\nfrom google.colab import userdata\nos.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\nllm = ChatOpenAI(model=\"gpt-4o\")\n</code></pre>"},{"location":"Task3/#function-to-convert-image-to-base64","title":"Function to Convert Image to Base64:","text":"<p><pre><code># Function to convert image to base64\ndef image_to_base64(image_path):\n    with Image.open(image_path) as image:\n        buffered = io.BytesIO()\n        image.save(buffered, format=image.format)\n        img_str = base64.b64encode(buffered.getvalue())\n        return img_str.decode('utf-8')\n\nimage_str = image_to_base64(\"/content/figures/figure-1-1.jpg\")\n</code></pre> Note: This is the image that was extracted from our data and saved in the Figures folder.</p> <ul> <li>Lets see our  base64-encoded image</li> </ul> <p><pre><code>print(image_str)\n</code></pre> Output: Our base64-encoded string </p> <pre><code>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\n</code></pre>"},{"location":"Task3/#initializing-the-multimodal-and-sending-the-image-to-gpt-4o-for-analysis","title":"Initializing the Multimodal and sending the Image to GPT-4o for Analysis:","text":"<pre><code>chat = ChatOpenAI(model=\"gpt-4o\", max_tokens=1024)\n\nmsg = chat.invoke(\n    [\n        HumanMessage(\n            content=[\n                {\"type\": \"text\", \"text\": \"Please give a summary of the image provided. Be descriptive\"},\n                {\"type\": \"image_url\", \"image_url\": {\n                    \"url\": f\"data:image/jpeg;base64,{image_str}\"\n                    }\n                 },\n            ]\n        )\n    ]\n)\n</code></pre> <ul> <li>Retrieving the Response:</li> </ul> <p><pre><code>msg.content\n</code></pre> Output: The response from the model is stored in msg.content, which will contain the descriptive summary of the image. </p> <p>The image is the logo of Cisco Systems, Inc., commonly known as Cisco. The logo features the company name \"cisco\" written in lowercase letters in a distinct, modern sans-serif font. Above the text, there is a series of vertical bars of varying heights, which is stylized to resemble the Golden Gate Bridge. The logo is rendered in a light blue color. The design is clean, professional, and easily recognizable, reflecting Cisco's identity as a major tech company specializing in networking hardware, telecommunications equipment, and other high-technology services and products.</p> <p>Note: The description of your image may vary depending on the image retrieved.</p>"},{"location":"Task4/","title":"Task 4: Embeddings and Vector Database","text":"<ul> <li>Vector databases are specialized databases designed to store content like PDFs, blogs, Word documents, images, audio files, and videos as embedding vectors. This enables semantic-based retrieval, meaning the database can understand the meaning and context of the stored data, allowing for more accurate and relevant search results beyond simple keyword matching.</li> </ul> <ul> <li>In the process of a large language model, embedding is generally the first step to convert discrete tokens (like words or subwords) into dense vector representations, which will allow the rest of the network to do the math necessary to predict the next word.Embeddings and vector databases are essential if you are building any kind of AI product.</li> </ul>"},{"location":"Task4/#what-are-embeddings","title":"What are embeddings?","text":"<p>Embeddings are basically data which are converted into array of numbers called vectors that contain pattern of relationship, and can be used for similarity search.</p> <p></p> <p></p> <p>Let's explain this concept using a 2D graph. In this graph, words like \"Webex\" and \"collaboration\" are often used in similar contexts, so their embeddings\u2014essentially vector representations\u2014are positioned close to each other. These vectors are numerical arrays that computers can easily interpret.</p> <p>The advantage of using vectors is that we can find similar items by calculating the distances between their vectors, a process known as nearest neighbor search. However, simply storing these embeddings isn't enough. Performing queries across thousands of vectors can be incredibly slow. To overcome this, the vectors need to be indexed in the vector database.</p> <p> Note: In reality vectors can have hundred of dimensions </p> <p></p> <p>Similarly, images are also broken into vectors, which are arrays of numbers that machines can process. Once these embeddings (both for words and images) are generated, they are stored in a specialized database known as a vector database.</p> <p></p> <p>There are numerous embedding models available, such as Google's Word2Vec, CLIP (Contrastive Language\u2013Image Pretraining), and even those provided by OpenAI, which offer excellent capabilities for generating embeddings. However, the challenge is that these models don't include tools for storing and managing those embeddings. This is where vector databases become essential.</p> <p></p> <p> Note: In this lab session, we will be using OpenAI embeddings. More info can be found here </p>"},{"location":"Task4/#why-use-vector-databases-when-we-have-relational-databases","title":"Why use vector databases when we have relational databases?","text":"<p>Around 80% of the data we encounter is unstructured, including social media posts, images, and videos. This type of data doesn\u2019t easily fit into traditional relational databases, which is where vector databases come into play.</p> <p></p>"},{"location":"Task4/#understanding-the-workings-of-embeddings-and-vector-databases","title":"Understanding the workings of Embeddings and Vector Databases","text":""},{"location":"Task4/#practical-example-of-embedding-techniques-using-postman","title":"Practical Example of Embedding Techniques Using Postman","text":""},{"location":"Task4/#set-openai-token","title":"Set OpenAI token","text":"<p>Note: If you\u2019ve already obtained the token in previous steps or section, you can skip this section </p> <p>Note: First, create an account from the OpenAI official website.</p> <ul> <li>Create a new project API key by browsing to API Keys web page. Select Create new secret key. The API key is automatically generated. Save the APi Key as we will be using it in the later steps .</li> </ul> <p></p> <p>Note: We will use the OpenAI key in Postman to generate embeddings.</p>"},{"location":"Task4/#login-to-postman","title":"Login to Postman","text":"<p>If you haven't installed Postman on your machine yet, you can download it from the following link</p> <p>We will use embedding APIs to create an embedding vector that represents our input text, more information can be found at the following link</p> <p>The POST request we will be using:</p> <p><pre><code>https://api.openai.com/v1/embeddings\n</code></pre> * Open Postman, create a new request by pressing +. Select POST as your request and enter the request url  </p> <p></p> <ul> <li>Click on the Headers tab and enter the Following creds</li> </ul> <pre><code>Authorization: Bearer &lt;replace with your openAi API key&gt;\nContent-Type : application/json\n</code></pre> <p>Note: Ensure there is a space between \"Bearer\" and your OpenAI API key.</p> <p></p> <p>In this lab, we will be using the text-embedding-ada-002 model, but feel free to use any other embedding model of your choice.</p> <p></p> <ul> <li>In Postman, click on the \"Body\" tab, select RAW and enter the following information, and then press Send.</li> </ul> <pre><code>{\n    \"input\": \"What is WebexOne 2024? WebexOne is an annual in-person and virtual event that takes place over four days. It\u2019s an event focused on AI collaboration and customer experience, and it features a range of activities such as insightful breakout sessions, technical training courses, hands-on labs, inspiring keynotes, epic entertainment, a solutions showcase and expo, customer awards, meet the experts, 1:1 executive meetings, a partner program, and more! \",\n    \"model\": \"text-embedding-ada-002\"\n}\n</code></pre> <p></p> <ul> <li>You will receive a 200 OK message, confirming success. You'll notice that our text has been converted into embeddings (vectors). In the upcoming steps, we will learn how to manually save this information in a vector database.</li> </ul> <p></p> <p>Note:Text embedding models convert text into numerical data (embeddings) that represent the meaning of the text.</p>"},{"location":"Task4/#inserting-values-in-vector-database","title":"Inserting values in Vector Database","text":"<p>We have a variety of databases available. In the upcoming step, let's demonstrate how to use SingleStore as a vector database and save our embeddings there.</p> <p>Note: This manual step is simply to illustrate how embeddings are stored in vector databases.</p> <p></p> <p>Note: We will create a Free account on SingleStore since it provides some credits to set up a trial account.</p> <p></p> <ul> <li>I\u2019m choosing to create an account using Google, but feel free to use any other method that works for you.</li> </ul> <p></p> <ul> <li>Select Continue</li> </ul> <p></p> <p></p> <p></p> <ul> <li>Click on New Deployment</li> </ul> <p></p> <ul> <li>Give your workspace a name, keep all other settings at their default, and click \"Next.\"</li> </ul> <p></p> <ul> <li>Then, leave everything as is and click on \"Create Workspace.\"</li> </ul> <p></p> <p></p> <ul> <li>Now we can create a database</li> </ul> <p></p> <p></p> <ul> <li>Click on Database tab</li> </ul> <p></p> <p></p> <ul> <li>Return to the home screen and navigate to Develop &gt; Data Studio. Open the SQL Editor.</li> </ul> <p></p> <ul> <li>Be sure to select your workspace and database.</li> </ul> <p></p> <ul> <li>Run this SQL command to create a table - Press Run</li> </ul> <p>Note: If your lab remains inactive for 20 minutes, the SingleStore database will automatically \"pause\" or go dormant. When this happens, you\u2019ll need to return to the homepage, click \u201cresume,\u201d and the workspace will be ready again in a few minutes. </p> <p><pre><code>create table if not exists myvectortable (\ntext TEXT,\nvector BLOB\n);\n</code></pre> </p> <ul> <li>You can navigate to the Databases tab and verify that the table has been created.</li> </ul> <p></p> <ul> <li> <p>Let's copy the embeddings we generated earlier using Postman so that we can insert them into our database.</p> </li> <li> <p>To proceed, copy the input text and  all the embedding values, including the square brackets [], as shown in the image. These will be used for insertion into our database.</p> </li> </ul> <p></p> <ul> <li>Let's head back to our SQL editor and insert the values into our database. Once done press \"Run\"</li> </ul> <p>Note: You can reuse the same SQL editor block, remove the previous command, and run the following one. </p> <p><pre><code>insert into myvectortable (text ,vector) values (\"your_input_text\", JSON_ARRAY_PACK(\"your_embeddings\"))\n</code></pre> In this query:</p> <pre><code>* Replace \"your_input_text\" with the input values from Postman.\n\n* Replace your_embeddings with the embeddings you copied earlier, including the square brackets [].\n</code></pre> <pre><code>insert into myvectortable (text ,vector) values (\"What is WebexOne 2024? WebexOne is an annual in-person and virtual event that takes place over four days. It\u2019s an event focused on AI collaboration and customer experience, and it features a range of activities such as insightful breakout sessions, technical training courses, hands-on labs, inspiring keynotes, epic entertainment, a solutions showcase and expo, customer awards, meet the experts, 1:1 executive meetings, a partner program, and more!\", JSON_ARRAY_PACK(\"[\n                -0.0042485874,\n                -0.02296605,\n                0.011213377,\n                0.01114761,\n                -0.027280405,\n                0.0023528363,\n                -0.004350527,\n                0.0043768343,\n                -0.00068562734,\n                -0.011660597,\n                0.010397859,\n                0.018730614,\n                -0.0158763,\n                -0.013469206,\n                0.0026997605,\n                -0.00003768792,\n                0.014850326,\n                -0.03183152,\n                0.0026175508,\n                -0.0459321,\n                -0.0055803815,\n                -0.008247258,\n                -0.011713211,\n                0.0006169824,\n                -0.017112732,\n                -0.002885554,\n                -0.004113764,\n                -0.01348236,\n                ........\n            ]\"))\n</code></pre> <ul> <li>After running the above command, you will see that our table now contains both the input text and the corresponding embeddings.</li> </ul> <p></p>"},{"location":"Task4/#retrieving-values-from-the-vector-database","title":"Retrieving Values from the Vector Database","text":"<ul> <li>Searching the vector database is quite simple. Example: we want to find information related to WebexOne. To do this, we'll create embeddings for our query, \"Is WebexOne an annual event?\" and then search the vector database to find matches against the existing embeddings.</li> </ul> <pre><code>Is WebexOne an annual event?\n</code></pre> <ul> <li>Let's open Postman and create an embedding for our question. Be sure to click on \"Send.\"</li> </ul> <ul> <li>Let's head over to the SQL editor and run a query for our search.</li> </ul> <p><pre><code>select text,dot_product(vector,JSON_ARRAY_PACK(\"[your_embeddings]\")) as score\nfrom myvectortable\norder by score desc\nlimit 5;\n</code></pre> * In this query:     * Replace your_embeddings with the embeddings you copied earlier, including the square brackets [] for the question.</p> <pre><code>select text,dot_product(vector,JSON_ARRAY_PACK(\"[\n                -0.0015118581,\n                -0.021267666,\n                0.023597047,\n                -0.0072638104,\n                -0.02286653,\n                0.005909599,\n                -0.026119394,\n                -0.010268575,\n                0.0052169873,\n                -0.009551842,\n                0.024065679,\n                0.020178782,\n                0.015464887,\n                0.016815653,\n                -0.016333235,\n                -0.0006641838,\n                0.018069934,\n                -0.010564916,\n                0.007911626,\n                -0.026367495,\n                -0.0019417254,\n                -0.005547787,\n                -0.012604848,\n                0.0003715036,\n                -0.013962504,\n                0.011729606,\n                -0.00078737224,\n                -0.011881223,\n                0.007815143,\n                0.009992908,\n                0.020316616,\n                -0.0032580325,\n                -0.0066401153,\n                0.0097310245,\n                -0.0028652078,\n                -0.008145943,\n                -0.0074843434,\n                ........\n            ]\")) as score\nfrom myvectortable\norder by score desc\nlimit 5;\n</code></pre> <p></p> <p>Note: You\u2019ll be able to see the success of the vector database\u2014higher scores indicate better matches for the answer.</p>"},{"location":"Task4/#conclusion","title":"Conclusion","text":"<p>In summary, vector databases allow LLMs to have long-term memory. In this section, we explored how to use embeddings and vector databases by generating embeddings with the OpenAI API through Postman. After setting up a free SingleStore account, embeddings were stored in a vector database. The process included creating an embedding for a query, searching the database, and confirming that it effectively retrieved relevant information. This demonstrated how vector databases can efficiently manage and query embeddings, making it easier to find relevant information based on the data stored.</p>"},{"location":"Task5/","title":"Task 5: Context Windows and Retrieval Augmented Generation (RAG)","text":"<ul> <li> <p>The context window is the limit on how much text the model can keep in mind and process at once. It includes your messages, the model\u2019s replies, and any other text in the conversation. If the conversation gets too long and goes beyond this limit, the model might start to forget what was said earlier.</p> </li> <li> <p>Example </p> </li> <li> <p>For GPT-3.5, the context window limit is approximately 8,000 tokens (one token typically represents about 4 characters on average). If the input text along with previous interactions (if any) exceeds the limit of LLM, the model may not be able to see or remember parts of the text that fall outside this window. This can result in errors or the inability to refer back to earlier parts of the conversation or document. </p> </li> </ul> <p></p> <p> Note: A token can be as small as a piece of a word or as large as a word itself, depending on the language and complexity of the text </p> <ul> <li>Consider a model with a context window limit of 8,000 tokens. If we input 6,000 tokens and the model generates a response of 1,500 tokens, the total token count is 7,500. This is within the model's context window, so all information is processed correctly. Conversely, if the input increases to 10,000 tokens and the response remains at 1,500 tokens, the total becomes 11,500 tokens. This exceeds the model's 8,000-token limit. Consequently, the model might lose context or be unable to respond properly as the excess information falls outside its context window. This demonstrates the importance of managing input size to stay within the model's processing capabilities.</li> </ul> <p></p>"},{"location":"Task5/#how-context-window-work","title":"How Context window work","text":"<ul> <li> <p>In the below example , multiple potential predicted words exist, but the model chooses the words based on the surrounding context and their probability. However, if the context window size increases and the word \"Webex\" moves out of the window, the model may lose critical context. As a result, the next predicted word might be incorrect, as the model can no longer reference \"Webex\" to inform its predictions accurately. This highlights the importance of keeping key information within the context window to maintain the accuracy of the model's predictions.</p> </li> <li> <p>Additionally, the temperature setting in the model can influence the type of content it generates. Lower temperatures are useful for summarizing or generating more deterministic and concise outputs, while higher temperatures encourage creativity, making the model more suitable for writing stories or poems. Adjusting the temperature allows you to control the balance between predictability and creativity in the model's output.</p> </li> </ul> <p></p> <p></p> <ul> <li>Thats where RAG come into play</li> </ul>"},{"location":"Task5/#retrieval-augmented-generation-rag","title":"Retrieval Augmented Generation (RAG)","text":"<p>Let's start by understanding what RAGs are before examining how they address context window issues.</p> <p>RAG = Retrieval Augmented Generation</p> <p></p> <p>Lets focus on the Generation part. Generation basically means responding to user query. For example, you might ask the model whether DX or Navigators are compatible with supporting ThousandEyes.</p> <p></p> <p>That's an excellent question. LLMs are trained on vast amounts of historical data, so they may recognize that DX and Navigators are well-regarded Cisco products. As a result, the LLM might confidently respond that you can install ThousandEyes on those devices. However, the issue is that the response may lack a verifiable source to back up that claim or the information can be out-of-date  </p> <p></p> <p>What if, instead of just asking the question, I had first consulted a reputable source and only then informed the user that ThousandEyes can't be installed on those devices? This is where Retrieval-Augmented Generation comes into play. By integrating a knowledge base\u2014such as PDFs, CSVs, images, and other resources\u2014into the model, we ensure that any time a question is asked, the response is based on the most current and accurate information available.</p> <p></p>"},{"location":"Task5/#how-can-rag-address-the-limitations-weve-seen-with-the-context-window","title":"How can RAG address the limitations we've seen with the context window?","text":"<p>To address the context window limitation, we can use an embedding model to convert relevant text into embeddings and store them in a vector database. When a user submits a query, we transform it into an embedding and compare it to the stored embeddings to find the closest match. Once we identify the match, we retrieve that specific chunk of text to respond to the query. This approach effectively overcomes the context window limitation by ensuring that our responses are accurate and contextually relevant.</p> <p></p> <p>Now you can see how RAG can address some of the challenges with LLMs. Instead of re-training or fine-tuning the model with new information, we can augment our data sources to retrieve the most up-to-date information. Additionally, LLMs are now instructed to prioritize primary source data before generating responses, making them less likely to hallucinate and encouraging more accurate and reliable outputs.</p> <p></p>"},{"location":"Task5/#whats-happening-under-the-hood","title":"What's Happening Under the Hood?","text":"<p>We normally start by taking multiple documents and converting them into embeddings. Imagine these embeddings as points in a 3D space, where each document is projected based on its semantic meaning or content. Documents located near each other in this space contain similar semantic information, which is crucial when we search for or retrieve information.</p> <p>Now, when we receive a query, we also convert it into an embedding (numerical value). We then perform a similarity search in this 3D space, looking for the documents that are closest to our query's embedding. These nearby documents are likely to contain the most relevant information.</p> <p>Once we've identified these matching documents, we retrieve the relevant sections (or \"splits\") and feed all this information into the LLM's context window, allowing it to generate a well-informed response.</p> <p></p> <p>We've seen models like Gemini Pro with a context window size of millions, allowing them to retain more information. So, why use RAG? The answer is that RAG remains highly beneficial for sourcing real-time data,fact-checking, and accessing external knowledge that isn't contained within the context window.</p>"},{"location":"Task5/#lets-build-a-quick-rag-application","title":"Let's build a quick RAG application","text":"<p>While LLMs (Large Language Models) like GPT-4 , Llama, Gemini possess impressive general knowledge, they don't inherently have access to your data. To connect LLMs with your own data sources, we can use frameworks like LangChain. These frameworks enable us to leverage our own data by breaking down documents into smaller chunks and storing them as embeddings in vector databases.</p> <p>This approach allows us to build applications that can effectively use language models. Example: When a user asks a question, we convert it into embeddings, perform a semantic search to find the most relevant answers, and then send both the question and the retrieved information to the LLM to generate a response.</p> <p>In this section, we'll briefly introduce LangChain, but in the upcoming chapters, we'll dive deeper to gain a more comprehensive understanding of how it works.</p> <p></p> <p>Note: In this section, we will be using a version of Cisco DECT 6800 Deployment Guide.  For the purposes of this lab, I've modified the original PDF by removing a few pages and saving it as a new file.  The PDF we are using in this lab can be downloaded from here</p> <ul> <li>Open Google Colab and create a new notebook. Click on \"File\" &gt; \"New notebook\". Please refer to the following section to create Google Colab account.</li> </ul> <p></p> <ul> <li>Make sure you are connected to a runtime. For this task, you can use the CPU as the runtime environment.</li> </ul> <p></p> <ul> <li>Within your existing Google Colab notebook navigate to the new \u201cSecrets\u201d section in the sidebar.</li> </ul> <p></p> <pre><code>- If not already done, Click on \u201cAdd a new secret.\u201d Enter the name example: OPENAI_API_KEY and value of the secret(the API key created earlier). Note: The name is permanent once set.\n\n- The list of secrets is global across all your notebooks.\n\n- Use the \u201cNotebook access\u201d toggle to grant or revoke access to a secret for each notebook.\n</code></pre> <ul> <li> <p>Let's load our PDF files into Google Colab. For this example, we can use the modified DECT guide</p> </li> <li> <p>Within Google Colab, Click on Folder and create a new folder called \"data\"</p> </li> </ul> <p></p> <ul> <li>Click on [...], select Upload</li> </ul> <p></p> <ul> <li>Choose your dect.pdf file and click Open</li> </ul> <p></p> <ul> <li>First let\u2019s install the necessary Python packages</li> </ul> <pre><code>!pip install langchain langchain_community langchain-openai python-dotenv chromadb pymupdf\n</code></pre> <p></p> <ul> <li>Let's retrieve a OpenAI API key and set it as an environment variable within the Colab environment</li> </ul> <pre><code>import os\nfrom google.colab import userdata\nos.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n</code></pre> <ul> <li>Install relevant libraries for RAG</li> </ul> <pre><code>from langchain import hub\nimport os\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom dotenv import load_dotenv\nfrom langchain.document_loaders import PyMuPDFLoader\nload_dotenv()\n</code></pre> <ul> <li>Next let\u2019s load the data source. This is also known as \u201cData Ingestion\u201d.</li> </ul> <pre><code>loader = PyMuPDFLoader(\"/content/data/dect.pdf\")\ndocs = loader.load()\n</code></pre> <ul> <li>Transform, where we break data into small chunks. We can use techniques like RecursiveCharacterTextSplitter or tiktoken for tokenizing text.</li> </ul> <p><pre><code>text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\nsplits = text_splitter.split_documents(docs)\n</code></pre>  Note: RecursiveCharacterTextSplitter Covered in previous section </p> <ul> <li>Let's create our embeddings. As discussed in the Embedding section, there are multiple techniques available. In this lab, we will use OpenAI embeddings. By default, the text-embedding-ada-002 model is used, but we can also opt for the text-embedding-3-large model if desired. Once the embeddings are created, they can be stored in a Vector Database.  While there are various databases available, as covered in the Embeddings and Vector DB section, in our example, we will use the Chroma Vector DB, which can be easily deployed locally on your machine.  We will also create a retriever by using  retriever = vectorstore.as_retriever() which converts the vector store into a retriever object. This retriever can then be used to find and retrieve documents or data from the vector store that are most relevant to a given query, based on the similarity of their embeddings.</li> </ul> <pre><code>vectorstore = Chroma.from_documents(documents=splits, \n                                    embedding=OpenAIEmbeddings(model=\"text-embedding-3-large\"))\n\nretriever = vectorstore.as_retriever()\n</code></pre> <p>Note: We are using Chroma dB but you have options to use other databases as well such as Faiss.  Also we can use embeddings from Ollama, an open-source and cost-free embedding solution. Below is a code snippet for your reference only: <pre><code># from langchain_community.embeddings import OllamaEmbeddings\n# embeddings = OllamaEmbeddings()\n</code></pre> </p> <p>Note: More info on Vector Databases can be found here <ul> <li>At this stage, we've loaded the document, created splits, converted them into embeddings, and stored them in the Vector DB. Now, let's query the database to ensure we can retrieve the information.</li> </ul> <pre><code>query = \"what is the Quick Set up and Installation Process for the Dect phones\"\nresult = vectorstore.similarity_search(query)\nprint(result[0].page_content)\n</code></pre> <p></p> <ul> <li>Let's create our prompt. There are several ways to do this, you can either craft it manually or use prompts that have already been made available on the LangChain Hub. In this section, I'll demonstrate how to pull a prompt from the LangChain Hub and use it.  First browse to Langmith and create an account. In this example, I'll use Google to sign up for a Langsmith account, but feel free to choose the option that works best for you.</li> </ul> <p></p> <ul> <li>Create your API Key</li> </ul> <p></p> <ul> <li>You can either enter your key in the secret folder or paste it manually here.</li> </ul> <pre><code>os.environ[\"LANGCHAIN_API_KEY\"] = \"PASTE-YOUR-KEY-HERE\"\n</code></pre> <ul> <li>Lets pull our prompt</li> </ul> <p><pre><code>prompt = hub.pull(\"rlm/rag-prompt\")\nprint(prompt)\n</code></pre> OUTPUT \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"</p> <p>Here's an example of a manual prompt for your reference only\u2014if you prefer not to use the downloaded prompt. </p> Reference ONLY<pre><code>template = \"\"\"Answer the question based only on the following context:\n{context}\nQuestion: {question}\n\"\"\"\n</code></pre> <ul> <li>We can now connect LLMs to our data sources. In this lab, we are using OpenAI models, but feel free to use open-source models if you prefer.</li> </ul> <pre><code>llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0)\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n</code></pre> <ul> <li>Lets use the Retrieval-Augmented Generation (RAG) pipeline to put all together. We will use retriever | format_docs to retrieve relevant documents (or context) related to the input question and formats them. The RunnablePassthrough method will be used to  simply pass the question  without modifying it.</li> </ul> <pre><code>Question = \"Quick Set up and Installation Process for dectphone and summarise\"\n#  Chain that wik take our input question from below\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()  # This component parses the LLM's output, typically converting it into a string format for easy use.\n)\nreq = rag_chain.invoke(Question)\nprint(req)\n</code></pre> <p>Note: When we pass our question into the rag_chain.invoke() method, it first goes through the retriever component that we set up earlier. The retriever uses the same embedding model, text-embedding-3-large, to transform the  question into an embedding vector. This vector is then compared with the embeddings of documents stored in the Chroma vector store to identify the most relevant ones. Once these relevant documents are retrieved based on the similarity of their embeddings, they are sent along with the original question to the language model (LLM), which then generates a response.</p> <p>OUTPUT To set up and install a DECT phone system, first review the site and plan the location of the base stations, ensuring they are within 50 meters of each other for good coverage. Upgrade the base stations to the latest firmware, configure them according to the Cisco IP DECT 6800 Series Administration Guide, and unpack and prepare the handsets. Finally, mount the base stations, place the handsets in their cradles, and make a few test calls to ensure everything is working correctly.</p>"},{"location":"Task5/#measuring-vector-similarity-using-cosine-similarity","title":"Measuring Vector Similarity Using Cosine Similarity","text":"<p>Cosine similarity is a metric used to measure how similar two vectors are, regardless of their magnitude. It is particularly useful in the context of text analysis, information retrieval, and machine learning, where it is often used to compare the similarity of two text documents, sentences, or any other data that can be represented as vectors. In our code, we will use it to match the similarity between the question we asked and the answer we receive.</p> <ul> <li>Lets create a function that calculates the cosine similarity between two vectors by using dot product a scalar value. </li> </ul> <pre><code># Define the cosine similarity function\nimport numpy as np\n# Define the cosine similarity function\ndef cosine_similarity(vec1, vec2):\n    dot_product = np.dot(vec1, vec2)\n    norm_vec1 = np.linalg.norm(vec1)\n    norm_vec2 = np.linalg.norm(vec2)\n    return dot_product / (norm_vec1 * norm_vec2)\n</code></pre> <ul> <li>We will now convert the question into an embedding, retrieve the embedding of our question, and the embeddings of the documents retrieved by the retriever.</li> </ul> <pre><code># Convert the question into an embedding\nembedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\nquestion_embedding = embedding_model.embed_query(Question)\nretrieved_docs = retriever.get_relevant_documents(Question)\n</code></pre> <ul> <li>Lets calculate and print similarity between the question and each retrieved document</li> </ul> <pre><code>for doc in retrieved_docs:\n    # Assuming your retriever does not provide embeddings directly, re-embed each document\n    doc_embedding = embedding_model.embed_query(doc.page_content)\n    similarity = cosine_similarity(question_embedding, doc_embedding)\n    print(f\"Document: {doc.page_content[:100]}...\")  # Print a preview of the document\n    print(f\"Cosine Similarity: {similarity}\\n\")\n</code></pre> <p></p> <p> Note: Cosine similarity values range from -1 to 1, where 1 means the vectors are identical, 0 means they are completely different, and values closer to 1 indicate greater similarity.  In our example where one similarity score is 0.63 and another is 0.57, the document with a score of 0.63 is more similar to the query than the one with 0.57. Both scores indicate a moderate similarity to the query, but the document with 0.63 contains slightly more relevant information. </p>"},{"location":"Task6/","title":"Multimodal RAG","text":"<p>In the previous section, we explored RAG systems primarily designed for handling text. However, in real-world scenarios, many documents within organizations contain valuable information in the form of images, tables, and other non-text elements. </p> <p>When building a robust RAG system, it's crucial to have the capability to retrieve not just text-based information but also relevant images and other visual data. This multimodal approach significantly enhances the effectiveness of information retrieval. In this section, we will explore methods to retrieve both images and text in response to user queries, enabling a more comprehensive and efficient RAG based system.</p>"},{"location":"Task6/#three-different-approaches-for-multimodal-rag","title":"Three Different approaches for Multimodal RAG","text":"<p> We will be using Option 2 for our lab </p> <p>Embedding for images can be done via CLIP model.</p>"},{"location":"Task6/#contrastive-language-image-pretraining-clip-model","title":"Contrastive Language-Image Pretraining - CLIP Model","text":"<p>Unlike other embedding models that focus on a single modality, such as text or images, the CLIP (Contrastive Language-Image Pretraining), was developed by OpenAI in 2021, handles multiple modalities by integrating both text and images.</p> <p>Note: CLIP models are embedding models, not traditional Large Language Models (LLMs).</p> <p>The primary function of CLIP (Contrastive Language-Image Pretraining) is to create a shared embedding space where both text and images are represented as vectors. The key idea is to learn representations where images and their corresponding text descriptions are close to each other in this embedding space, while unrelated image-text pairs are far apart. This allows CLIP to perform tasks like image classification, zero-shot learning, and text-to-image retrieval based on the semantic similarity between text and images. So in summary CLIP models create a relationship between images and text.</p>"},{"location":"Task6/#zero-shot-learning-zsl","title":"Zero-Shot Learning (ZSL)","text":"<p>Zero-Shot Learning (ZSL): This technique enhances the ability of AI systems to classify and recognize objects they have not been explicitly trained on. Instead of relying solely on trained data, ZSL uses auxiliary information to make predictions. For example, in an image classification scenario where a model is trained to recognize dogs and cats, if it encounters an image of a zebra (which the model has never seen before), it can still classify it correctly if it has auxiliary information describing a zebra as an animal with black and white stripes.</p> <p>Using ZSL, CLIP models can perform tasks they were not specifically trained on, in real-time. Essentially, they have capabilities similar to text-based embedding models. Just as we create chunks of text and use a user query to find similar text chunks, the same concept applies to images using CLIP.</p>"},{"location":"Task6/#openclip-model","title":"OpenCLIP Model","text":"<p>It's important to note that the original CLIP model by OpenAI is not publicly available. However, people have taken the concepts from OpenAI's CLIP paper and developed open-source models like OpenCLIP.</p> <p> Note: OpenCLIP is an open-source implementation and extension of the original CLIP (Contrastive Language-Image Pretraining) model developed by OpenAI.</p> <ul> <li> <p>Different types of Multimodal Embeddings and LLM </p> </li> <li> <p>Usecases for Multimodal RAG</p> </li> </ul> <p></p> <p>Let\u2019s continue by jumping over to our lab environment!</p>"},{"location":"Task6/#deploy-multimodal-rag","title":"Deploy Multimodal RAG","text":"<p>Whenever we develop applications based on Large Language Models (LLMs), handling data is a crucial aspect. As discussed in the previous section (RAG), data injection is the initial step in this process. It's important to note that there are typically three types of data structure.</p> <p></p>"},{"location":"Task6/#task-1-log-into-the-lab-environment","title":"Task 1: Log into the Lab Environment","text":"<ul> <li>Open Google Colab and create a new notebook. Click on \"File\" &gt; \"New notebook\". Please refer to the following section to create Google Colab account.</li> </ul> <ul> <li>Change Runtime Environment: Click the \u201cRuntime\u201d dropdown menu at the top of the Colab interface.</li> </ul> <ul> <li> <p>Select \u201cChange runtime type\u201d: This will open a dialog box where you can configure the runtime environment.</p> </li> <li> <p>Select Hardware Accelerator: From the \u201cHardware accelerator\u201d dropdown menu, choose &gt;&gt; T4 GPU and enable toggle for High RAM</p> </li> </ul> <p></p> <ul> <li>Save Settings: Click \u201cSave\u201d to apply the changes.</li> </ul> <p>Reminder: Whenever you want to copy the code in Google Colab and run it, be sure to click on + Code to add a new code cell.</p> <p></p> <p>Reminder: Click the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\" while the cell is selected.</p> <p></p>"},{"location":"Task6/#set-openai-token-multimodal","title":"Set OpenAI token - MultiModal","text":"<p>Note: First, create an account from the OpenAI official website.If you have already completed this step please ignore and jump to Update Google Colab envoirnment section.</p> <ul> <li>Create a new project API key by browsing to API Keys web page. Select Create new secret key. The API key is automatically generated. Save the APi Key as we will be using it in the later steps .</li> </ul> <p></p> <ul> <li>Within your existing Google Colab notebook navigate to the new \u201cSecrets\u201d section in the sidebar.</li> </ul> <p></p> <ul> <li> <p>Click on \u201cAdd a new secret.\u201d Enter the name example: OPENAI_API_KEY and value of the secret(the API key created above). Note: The name is permanent once set. </p> </li> <li> <p>The list of secrets is global across all your notebooks.</p> </li> <li> <p>Use the \u201cNotebook access\u201d toggle to grant or revoke access to a secret for each notebook.</p> </li> </ul> <p></p>"},{"location":"Task6/#update-google-colab-envoirnment-and-install-packages","title":"Update Google Colab envoirnment and install packages","text":"<p>PDFs often contain tables and other structured data that can be challenging to split accurately using character-based embedding techniques. For PDFs, it's important to extract and chunk all elements, including tables, effectively. We'll accomplish this using the Unstructured library, which is specifically designed for handling such tasks. If you have a large collection of PDFs, Unstructured is an excellent tool to manage them efficiently.</p> <ul> <li>Install relevant libraries including Unstructured - ELT tool. Unstructured will partition PDF files by first removing all embedded image blocks. Then it will use a layout model (YOLOX) to get bounding boxes (for tables) as well as titles.</li> </ul> <p><pre><code>!pip install \"unstructured[all-docs]\" pillow pydantic lxml matplotlib langchain langchain_community chromadb langchain-experimental langchain_openai\n</code></pre> </p> <p>Note: Make sure to RESTART your notebook after installing the packages</p> <ul> <li>Since Google Colab is built on top of an Ubuntu environment, it's necessary to update the Google Colab environment to ensure we can effectively extract information from images or PDFs for analysis and processing.</li> </ul> <pre><code>!sudo apt-get update\n</code></pre> <p></p> <ul> <li>Poppler-utils will help us extracting info from our pdf</li> </ul> <pre><code>!sudo apt-get install poppler-utils\n</code></pre> <p></p> <pre><code>!sudo apt-get install libleptonica-dev tesseract-ocr libtesseract-dev python3-pil tesseract-ocr-eng tesseract-ocr-script-latn\n</code></pre> <p></p> <pre><code>!pip install unstructured-pytesseract\n!pip install tesseract-ocr\n</code></pre> <p></p> <ul> <li>In Google Colab, I noticed some challenges with the NLTK library, so let's set the path correctly to resolve these issues.</li> </ul> <pre><code>import os\n# Set the NLTK_DATA environment variable to the correct path\nos.environ['NLTK_DATA'] = '/root/nltk_data'\n</code></pre> <ul> <li>Let's make sure we have nltk version 3.9.1 installed</li> </ul> <pre><code>!pip install --upgrade nltk\n</code></pre> <p></p>"},{"location":"Task6/#pdf-with-table","title":"PDF with table","text":"<ul> <li>Let's load our PDF files into Google Colab. For this example, we can use the article titled \"Webex Customer Experience Essentials\" from the Webex Help Center. You can also download the modified article here as we will be using in the next step.</li> </ul> <p> Note: To save time and speed up processing in this lab, I have modified the CX-Essentials.pdf file by reducing it to only a few pages. You can download this version from the modified link provided above. </p> <ul> <li>Within Google Colab, Click on Folder and create a new folder called \"dat\"</li> </ul> <p></p> <ul> <li>Click on [...], select Upload</li> </ul> <p></p> <ul> <li>Choose your CX-Essentials.pdf file and click Open</li> </ul> <p></p> <ul> <li>Let's process our PDF document by splitting it into smaller, manageable chunks based on titles, extracting images, and handling text in a way that ensures the chunks are neither too large nor too small. We will store the extracted elements (text, images, etc.) in the pdf_elements variable, and images are saved in the specified path(image_path). To understand more about unstructured partition_pdf click here </li> </ul> <p>Unstructured will partition PDF files by first extracting embedded images if specified. It then processes the document's layout, dividing the content into structured elements such as titles, tables, and paragraphs based on the layout and text structure. For more detailed layout analysis, it may use advanced models like YOLOX to identify and extract bounding boxes for elements like tables and titles. This allows for the precise extraction and organization of text and images, making the content more suitable for further analysis and processing.</p> <p> NOTE: To avoid errors, overlap, or dependencies from previous tasks, open a new Google Colab notebook for this step. This will ensure a clean environment and prevent any potential issues related to previous work. </p> <pre><code>from unstructured.partition.pdf import partition_pdf\nimage_path = \"./content/images\"\npdf_elements = partition_pdf(\n    filename=\"/content/dat/CX-Essentials.pdf\",\n    chunking_strategy=\"by_title\",\n    extract_images_in_pdf=True,\n    # extract_image_block_types=[\"Image\", \"Table\"],\n    max_characters=3000, # Sets the maximum number of characters\n    new_after_n_chars=2800, # Character threshold after which a new chunk will start. Ensures  chunks are created before the max_characters limit is reached\n    combine_text_under_n_chars=2000, #If chunk of text is smaller than 2000 characters, it should be combined with the following chunk to create a more substantial piece \n    image_output_dir_path=image_path\n)\n</code></pre> <p></p> <p> Note: This process may take a few minutes to complete as we are extracting information from the PDF.</p> <ul> <li>You'll also notice that a folder has been created containing all the extracted images.</li> </ul> <p></p> <ul> <li>To see different elements from our pdf we can look into the variable</li> </ul> <pre><code>print(pdf_elements)\n</code></pre> <p></p> <ul> <li>Let's categorize extracted elements from a PDF into two types: text elements and table elements. </li> </ul> <pre><code># Categorize elements by type\ndef categorize_elements(raw_pdf_elements):\n    \"\"\"\n    raw_pdf_elements: List of unstructured.documents.elements\n    \"\"\"\n    tables = []\n    texts = []\n    for element in raw_pdf_elements:\n        if \"unstructured.documents.elements.Table\" in str(type(element)):\n            tables.append(str(element))\n        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n            texts.append(str(element))\n    return texts, tables\n\ntexts, tables = categorize_elements(pdf_elements)\n</code></pre> <pre><code>print(texts)\n</code></pre> <p></p> <pre><code>print(tables)\n</code></pre> <p></p>"},{"location":"Task6/#lets-now-create-summary-from-our-data-by-passing-the-info-to-multimodal-model","title":"Let's now create summary from our data by passing the info to Multimodal model","text":"<p>Note: We will be using GPT-4o as our Multimodal to create summary </p> <ul> <li>Let's retrieve an OpenAI API key and set it as an environment variable within the Colab environment</li> </ul> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom google.colab import userdata\nOPENAI_API_TOKEN=userdata.get('OPENAI_API_KEY')\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_TOKEN\nload_dotenv()\n</code></pre> <ul> <li>Let's create a summary for our text element</li> </ul> <pre><code>from langchain.chat_models import ChatVertexAI\nfrom langchain.llms import VertexAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema.output_parser import StrOutputParser\nfrom langchain_core.messages import AIMessage\nfrom langchain_core.runnables import RunnableLambda\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\n# Generate summaries of text elements\ndef generate_text_summaries(texts, tables, summarize_texts=False):\n    \"\"\"\n    Summarize text elements\n    texts: List of str\n    tables: List of str\n    summarize_texts: Bool to summarize texts\n    \"\"\"\n\n    # Prompt\n    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n    Give a concise summary of the table or text that is well-optimized for retrieval. Table \\\n    or text: {element} \"\"\"\n    prompt = PromptTemplate.from_template(prompt_text)\n    empty_response = RunnableLambda(\n        lambda x: AIMessage(content=\"Error processing document\")\n    )\n    # Model\n    model = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n\n    # Initialize empty summaries\n    text_summaries = []\n    table_summaries = []\n\n    # Apply to text if texts are provided and summarization is requested\n    if texts and summarize_texts:\n        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n    elif texts:\n        text_summaries = texts\n\n    # Apply to tables if tables are provided\n    if tables:\n        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 1})\n\n    return text_summaries, table_summaries\n\n\n# Get text, table summaries\ntext_summaries, table_summaries = generate_text_summaries(\n    texts, tables, summarize_texts=True\n)\n</code></pre> <ul> <li>Now Let's create summary for our images </li> </ul> <pre><code>import base64\nimport os\n\nfrom langchain_core.messages import HumanMessage\n\ndef encode_image(image_path):\n    \"\"\"Getting the base64 string\"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n\ndef image_summarize(img_base64, prompt):\n    \"\"\"Make image summary\"\"\"\n    model = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n    msg = model(\n        [\n            HumanMessage(\n                content=[\n                    {\"type\": \"text\", \"text\": prompt},\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n                    },\n                ]\n            )\n        ]\n    )\n    return msg.content\n\ndef generate_img_summaries(path):\n    \"\"\"\n    Generate summaries and base64 encoded strings for images\n    path: Path to list of .jpg files extracted by Unstructured\n    \"\"\"\n    # Store base64 encoded images\n    img_base64_list = []\n    # Store image summaries\n    image_summaries = []\n    # Prompt\n    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n    These summaries will be embedded and used to retrieve the raw image. \\\n    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n\n    # Apply to images\n    for img_file in sorted(os.listdir(path)):\n        if img_file.endswith(\".jpg\"):\n            img_path = os.path.join(path, img_file)\n            base64_image = encode_image(img_path)\n            img_base64_list.append(base64_image)\n            image_summaries.append(image_summarize(base64_image, prompt))\n\n    return img_base64_list, image_summaries\n\nfpath = \"/content/figures\"\n# Image summaries\nimg_base64_list, image_summaries = generate_img_summaries(fpath)\n</code></pre> <p><pre><code>image_summaries[0]\n</code></pre> </p> <p></p>"},{"location":"Task6/#lets-now-create-our-multivector-retriever-and-rag-system-incl-storing-summary-embeddings-in-vector-db","title":"Let's now create our MultiVector Retriever and RAG system - incl storing summary embeddings in Vector DB","text":""},{"location":"Task6/#create-our-multivector-retriever","title":"Create our MultiVector Retriever","text":"<ul> <li>Let's install packages for chroma our inmemory db and MultiVectorRetriever</li> </ul> <pre><code>import uuid\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever\nfrom langchain.storage import InMemoryStore\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_core.documents import Document\nfrom langchain_openai import OpenAIEmbeddings\n</code></pre> <ul> <li>Let's create our function to build a multi-vector retriever. We will use OpenAI embeddings and create a Chroma vector store to index our summaries. This function will allow us to index summaries of texts, tables, and images, while retrieving the raw data (texts, tables, or images) based on those summaries. By leveraging the OpenAI embeddings, our Chroma vector store will effectively handle and retrieve multimodal content.</li> </ul> <pre><code>import uuid\n\nfrom langchain.embeddings import VertexAIEmbeddings\nfrom langchain.retrievers.multi_vector import MultiVectorRetriever\nfrom langchain.schema.document import Document\nfrom langchain.storage import InMemoryStore\nfrom langchain.vectorstores import Chroma\n\n\ndef create_multi_vector_retriever(\n    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n):\n    \"\"\"\n    Create retriever that indexes summaries, but returns raw images or texts\n    \"\"\"\n\n    # Initialize the storage layer\n    store = InMemoryStore()\n    id_key = \"doc_id\"\n\n    # Create the multi-vector retriever\n    retriever = MultiVectorRetriever(\n        vectorstore=vectorstore,\n        docstore=store,\n        id_key=id_key,\n    )\n    # Helper function to add documents to the vectorstore and docstore\n    def add_documents(retriever, doc_summaries, doc_contents):\n        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n        summary_docs = [\n            Document(page_content=s, metadata={id_key: doc_ids[i]})\n            for i, s in enumerate(doc_summaries)\n        ]\n        retriever.vectorstore.add_documents(summary_docs)\n        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n\n    # Add texts, tables, and images\n    # Check that text_summaries is not empty before adding\n    if text_summaries:\n        add_documents(retriever, text_summaries, texts)\n    # Check that table_summaries is not empty before adding\n    if table_summaries:\n        add_documents(retriever, table_summaries, tables)\n    # Check that image_summaries is not empty before adding\n    if image_summaries:\n        add_documents(retriever, image_summaries, images)\n\n    return retriever\n\n# The vectorstore to use to index the summaries\nvectorstore = Chroma(\n    collection_name=\"mm_webexone_cl\",\n    embedding_function=OpenAIEmbeddings(),\n    # embedding_function=VertexAIEmbeddings(model_name=\"textembedding-gecko@latest\"),\n)\n\n# Create retriever\nretriever_multi_vector_img = create_multi_vector_retriever(\n    vectorstore,\n    text_summaries,\n    texts,\n    table_summaries,\n    tables,\n    image_summaries,\n    img_base64_list,\n)\n</code></pre> <pre><code>retriever_multi_vector_img\n</code></pre>"},{"location":"Task6/#create-our-rag-multimodal","title":"Create our RAG (Multimodal)","text":"<ul> <li>Let's create a multi_modal_rag_chain(retriever). This pipeline integrates both text and image data to enhance the quality of responses generated by an AI model</li> </ul> <pre><code>import io\nimport re\nimport base64\nfrom IPython.display import Image, display\nfrom PIL import Image as PILImage\nfrom langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n\n\ndef looks_like_base64(sb):\n    \"\"\"Check if the string looks like base64\"\"\"\n    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n\ndef is_image_data(b64data):\n    \"\"\"\n    Check if the base64 data is an image by looking at the start of the data\n    \"\"\"\n    image_signatures = {\n        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n    }\n    try:\n        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n        for sig, format in image_signatures.items():\n            if header.startswith(sig):\n                return True\n        return False\n    except Exception:\n        return False\n\n\ndef resize_base64_image(base64_string, size=(128, 128)):\n    \"\"\"\n    Resize an image encoded as a Base64 string\n    \"\"\"\n    # Decode the Base64 string\n    img_data = base64.b64decode(base64_string)\n    img = PILImage.open(io.BytesIO(img_data))\n\n    # Resize the image\n    resized_img = img.resize(size, PILImage.LANCZOS)\n\n    # Save the resized image to a bytes buffer\n    buffered = io.BytesIO()\n    resized_img.save(buffered, format=img.format)\n\n    # Encode the resized image to Base64\n    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n\ndef split_image_text_types(docs):\n    \"\"\"\n    Split base64-encoded images and texts\n    \"\"\"\n    b64_images = []\n    texts = []\n    for doc in docs:\n        # Check if the document is of type Document and extract page_content if so\n        if isinstance(doc, Document):\n            doc = doc.page_content\n        if looks_like_base64(doc) and is_image_data(doc):\n            doc = resize_base64_image(doc, size=(1300, 600))\n            b64_images.append(doc)\n        else:\n            texts.append(doc)\n    if len(b64_images) &gt; 0:\n        return {\"images\": b64_images[:1], \"texts\": []}\n    return {\"images\": b64_images, \"texts\": texts}\n\ndef img_prompt_func(data_dict):\n    \"\"\"\n    Join the context into a single string\n    \"\"\"\n    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n    messages = []\n\n    # Adding the text for analysis\n    text_message = {\n        \"type\": \"text\",\n        \"text\": (\n            \"You are an AI scientist tasking with providing factual answers.\\n\"\n            \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n            \"Use this information to provide answers related to the user question. \\n\"\n            f\"User-provided question: {data_dict['question']}\\n\\n\"\n            \"Text and / or tables:\\n\"\n            f\"{formatted_texts}\"\n        ),\n    }\n    messages.append(text_message)\n    # Adding image(s) to the messages if present\n    if data_dict[\"context\"][\"images\"]:\n        for image in data_dict[\"context\"][\"images\"]:\n            image_message = {\n                \"type\": \"image_url\",\n                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n            }\n            messages.append(image_message)\n    return [HumanMessage(content=messages)]\n\ndef multi_modal_rag_chain(retriever):\n    \"\"\"\n    Multi-modal RAG chain\n    \"\"\"\n    model = ChatOpenAI(temperature=0, model=\"gpt-4o\")\n\n    # RAG pipeline\n    chain = (\n        {\n            \"context\": retriever | RunnableLambda(split_image_text_types),\n            \"question\": RunnablePassthrough(),\n        }\n        | RunnableLambda(img_prompt_func)\n        | model\n        | StrOutputParser()\n    )\n\n    return chain\n\n# Create RAG chain\nchain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)\n</code></pre>"},{"location":"Task6/#lets-query-our-multimodal-retriever","title":"Let's Query our Multimodal Retriever","text":"<ul> <li>Open your CX-Essentials.pdf, and let's try querying some information from it.</li> </ul> <pre><code>query = \"\"\"how can I Upgrade call queue from Customer Experience Basic to Customer Experience Essentials. explain images if any there as well\"\"\"\ndocs = chain_multimodal_rag.invoke(query)\nprint(docs)\n</code></pre> <pre><code>query = \"\"\"how can I Upgrade call queue from Customer ExperienceBasic to Customer Experience Essentials. explain images if any there as well\"\"\"\ndocs = retriever_multi_vector_img.invoke(query, limit=1)\n</code></pre> <p><pre><code># To display the image\nfor doc in docs:\n    if looks_like_base64(doc) and is_image_data(doc):\n        image_data = base64.b64decode(doc)\n        display(Image(data=image_data))\n    else:\n        print(doc)\n</code></pre> * You'll notice that in our multimodal RAG system, I'm now able to retrieve images, tables, and text seamlessly.</p> <p></p>"},{"location":"Task7/","title":"Understanding Fine-Tuning - Large Language Models","text":""},{"location":"Task7/#fine-tuning-llms","title":"Fine-Tuning LLMs","text":"<p>The section centers around fine-tuning LLMs, addressing their various aspects and methodologies. As the module progresses, the focus will be given to specialized instruction tuning techniques, namely LoRA. It will examine domain-specific applications (Webex Calling), ensuring a holistic understanding of fine-tuning techniques and their real-world implications.</p> <ul> <li>Techniques for Finetuning LLMs: The lesson highlights the challenges, particularly the resource intensity of traditional approaches. We will introduce instruction tuning methods like\u00a0LoRA.</li> <li>Deep Dive into LoRA and SFT: This lesson offers an in-depth exploration of LoRA and SFT techniques. We will uncover the mechanics and underlying principles of these methods.</li> <li>Finetuning using LoRA : This lesson guides a practical application of LoRA and SFT to finetune an LLM to follow instructions, using data from the \u201cHuggingFace Dataset\u201d .</li> </ul>"},{"location":"Task7/#techniques-for-finetuning-llms","title":"Techniques for Finetuning LLMs","text":""},{"location":"Task7/#introduction","title":"Introduction","text":"<p>In this lesson, we will examine the main techniques for fine-tuning Large Language Models for superior performance on specific tasks. We explore why and how to fine-tune LLMs, the strategic importance of instruction fine-tuning, and several fine-tuning methods, such as  Low-Rank Adaptation (LoRA), Supervised Finetuning (SFT). We also touch upon the benefits of the Parameter-Efficient Fine-tuning (PEFT) approach using Hugging Face's PEFT library, promising both efficiency and performance gains in fine-tuning.</p>"},{"location":"Task7/#why-we-finetune-llms","title":"Why We Finetune LLMs","text":"<p>While pretrained Large Language Models (LLMs) provide a broad understanding of language, it doesn't equip them with the specialized knowledge needed for complex tasks. For instance, a pre-trained LLM may excel at generating text but encounter difficulties when tasked with sentiment analysis or even providing information from your own Knowledge base. This is where fine-tuning comes into play.</p> <p>Fine-tuning is the process of adapting a pretrained model to a specific task by further training it using task-specific data. For example, if we aim to make an LLM proficient in answering questions about Webex Calling or Webex CC, we would fine-tune it using a dataset comprising Webex question-answer pairs. This process enables the model to recalibrate its internal parameters and representations to align with the intended task, enhancing its capacity to address domain-specific challenges effectively.</p> <p>However, fine-tuning LLMs conventionally can be resource-intensive and costly. It involves adjusting  the parameters in the pretrained LLM models, which can number in the billions, necessitating significant computational power and time. Consequently, it's crucial to explore more efficient and cost-effective methods for fine-tuning, such as Low-Rank Adaptation (LoRA).</p>"},{"location":"Task7/#a-reminder-on-instruction-and-conversational-finetuning","title":"A Reminder On Instruction and Conversational Finetuning","text":"<p>In Conversational fine-tuning the model engages in a dialogue with the user, maintaining context over multiple turns.The interaction mimics a natural conversation, with the model responding in a way that feels like a human interlocutor.</p> <p>Instruction fine-tuning is a specific type of fine-tuning that grants precise control over a model's behavior. The objective is to train a Language Model (LLM) to interpret prompts as instructions rather than simply treating them as text to continue generating. </p>"},{"location":"Task7/#introduction-to-efficient-finetuning-with-parameter-efficient-fine-tuning-peft","title":"Introduction to Efficient Finetuning with Parameter-Efficient Fine-tuning (PEFT)","text":"<p>Parameter-Efficient Fine-tuning (PEFT) approaches address the need for computational and storage efficiency in fine-tuning LLMs. Hugging Face developed the PEFT library specifically for this purpose. PEFT leverages architectures that only fine-tune a small number of additional model parameters while freezing most parameters of the pretrained LLMs, significantly reducing computational and storage costs.</p> <p>PEFT methods offer benefits beyond just efficiency. These methods have been proven to outperform standard fine-tuning methods, particularly in low-data situations, and provide improved generalization for out-of-domain scenarios. Furthermore, they contribute to the portability of models by generating tiny model checkpoints that require substantially less storage space compared to extensive full fine-tuning checkpoints.</p> <p>The PEFT library supports popular methods such as Low-Rank Adaptation (LoRA) and Prompt Tuning. </p>"},{"location":"Task7/#a-reminder-of-the-techniques-for-finetuning-llms","title":"A Reminder of the Techniques For Finetuning LLMs","text":"<p>There are several techniques to make the finetuning process more efficient and effective:</p> <ul> <li> <p>Full Finetuning: This method involves adjusting all the parameters in the pretrained LLM models to adapt to a specific task. While effective, it is resource-intensive and requires extensive computational power, therefore it\u2019s rarely used. This is not in the scope of this lab</p> </li> <li> <p>Low-Rank Adaptation (LoRA): LoRA is a technique that aims to adapt LLMs to specific tasks and datasets while simultaneously reducing computational resources and costs. By applying low-rank approximations to the downstream layers of LLMs, LoRA significantly reduces the number of parameters to be trained, thereby lowering the GPU memory requirements and training costs. We\u2019ll also see QLoRA, a variant of LoRA that is more optimized and leverages quantization.</p> </li> </ul> <p>With a focus on the number of parameters involved in finetuning, there are multiple methods, such as:</p> <ul> <li> <p>Supervised Finetuning (SFT): SFT involves doing standard supervised finetuning with a pretrained LLM on a small amount of demonstration data. This method is less resource-intensive than full finetuning but still requires significant computational power. This is within the scope of this lab</p> </li> <li> <p>Reinforcement Learning from Human Feedback (RLHF): RLHF is a training methodology where models are trained to follow human feedback over multiple iterations. This method can be more effective than SFT, as it allows for continuous improvement based on human feedback. We\u2019ll also see some alternatives to RLHF, such as Direct Preference Optimization (DPO), and Reinforcement Learning from AI Feedback (RLAIF).This is not in the scope of this lab</p> </li> </ul>"},{"location":"Task7/#conclusion","title":"Conclusion","text":"<p>In this lesson, we've learned that while pretraining equips LLMs with a broad understanding of language, fine-tuning is necessary to specialize these models for complex tasks. We've looked into various fine-tuning techniques, including Full Finetuning, Low-Rank Adaptation (LoRA), Supervised Finetuning (SFT), and Reinforcement Learning from Human Feedback (RLHF). </p>"},{"location":"Task8/","title":"Fine Tuning - Deep Dive into Quantization , LoRA and SFT","text":""},{"location":"Task8/#deep-dive-into-quantization-lora-and-sft","title":"Deep Dive into Quantization, LoRA and SFT","text":""},{"location":"Task8/#introduction","title":"Introduction","text":"<p>In this lesson, we will dive deeper into Quantization and the mechanics of LoRA, a powerful method for optimizing the fine-tuning process of Large Language Models, its practical uses in various fine-tuning tasks, and the open-source resources that simplify its implementation. We will also introduce QLoRA, a highly efficient version of LoRA. By the end of this lesson, you will have an in-depth understanding of quantization and how LoRA and QLoRA can enhance the efficiency and accessibility of fine-tuning LLMs.</p>"},{"location":"Task8/#understanding-quantization","title":"Understanding Quantization","text":"<p>As large language models, grow more advanced, their increasing number of parameters leads to significant memory usage. This, in turn, increases the costs of hosting and deploying these tools.</p> <p>In this section, we will quickly learn about quantization. Quantization is a process that converts data that\u2019s in your memory from high bit to low bit rate.</p>"},{"location":"Task8/#quantization-in-machine-learning","title":"Quantization in Machine Learning","text":"<p>Neural Networks have millions or billions of parameters (floating point values) representing their weights and biases, a characteristic known as precision. The precision of these data types directly impacts the amount of memory required by the model. Using higher precision types, such as Float32 or Float64, offers greater accuracy but demands more memory. By default when you train your neural network  ususally the parameters are stored in FP32, which means each paraeter takes 32 bits. The idea of Quantization is to reduce parameters that takes less space example 16 bit floating points(FP16) or Int8. If we store all the parameters in INT8 , it means everything be saved between 0 - 255, which will result in saving space 4 times comapred to FP32</p> <p>Quantization can significantly reduce this memory requirement by representing parameters with fewer bits. For example, converting from 32-bit to smaller bit representations allows us to perform inference with the models more efficiently. Once models are quantized, they can be deployed on edge devices like phones, smartwatches, etc. However, using lower precision types, such as Float16 or BFloat16 or Int8, while saving memory, may result in a slight loss of accuracy. To overcome this loss in accuracy, techniques like Lora and QLoRA can be utilized.</p> <p>Benefits of Quantization:</p>"},{"location":"Task8/#fp16-quantization","title":"FP16 Quantization:","text":"<ul> <li>Reduces the model size by half.</li> <li>Maintains higher precision than INT8.</li> <li>Suitable for GPUs and TPUs that support FP16 arithmetic.</li> </ul>"},{"location":"Task8/#int8-quantization","title":"INT8 Quantization:","text":"<ul> <li>Significantly reduces model size and improves latency.</li> <li>Suitable for CPUs and hardware accelerators that support INT8 arithmetic.</li> </ul> <p>By applying quantization, we can make large models more efficient and capable of running on a variety of devices, ensuring a balance between performance and resource utilization.</p> <p>In the figure below, you can see the main floating point data types.</p> <p></p>"},{"location":"Task8/#why-quantization-uses-less-memory","title":"Why Quantization uses less memory","text":"<p>As mentioned above, the primary advantage of using quantization (4-bit) is the reduction in model size and memory usage. Here's a simple explanation:</p> <p></p> <ul> <li>A float32 number takes up 32 bits of memory.</li> <li>A 4-bit quantized number takes up only 4 bits of memory. So, theoretically, you can fit 8 times more 4-bit quantized numbers into the same memory space as float32 numbers. This allows you to load larger models into the GPU memory or use smaller GPUs that might not have been able to handle the model otherwise.</li> </ul> <p>Note: The amount of memory used by an integer in a computer system is directly related to the number of bits used to represent that integer.</p> <p>Memory Usage for 4-bit Integer A 4-bit integer uses 4 bits of memory</p> <p>Memory Usage for 32-bit Integer A 32-bit integer uses 32 bits of memory.</p> <p>Conversion to Bytes To convert these to bytes (since memory is often measured in bytes):</p> <pre><code>1 byte = 8 bits\nA 4-bit integer would use ( 4/8 = 0.5 ) bytes.\nA 16-bit integer would use ( 16/8 = 2 ) bytes.\n</code></pre> <p>Let\u2019s take Llama 2 as an example, to better understand this:</p> <p>For example, you may come across config like this in Llama 2 model:</p> <p>Note: The bitsandbytes library will be used for quantization and fine-tuning of models. It provides tools to reduce memory footprint and improve computational efficiency.</p> <pre><code>bnb_config = transformers.BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_quant_type='nf4',\nbnb_4bit_use_double_quant=True,\nbnb_4bit_compute_dtype=bfloat16\n)\nload_in_4bit=True //Enables 4-bit quantization.\nbnb_4bit_quant_type='nf4' //Specifies the type of 4-bit quantization.\nbnb_4bit_use_double_quant=True //Enables double quantization for better accuracy.\nbnb_4bit_compute_dtype=bfloat16 //Specifies the data type for computation, which is bfloat16 here.\n</code></pre> <p>By using 4-bit quantization, you can load the Llama 2 model with significantly less GPU memory, making it more accessible for devices with limited resources.</p>"},{"location":"Task8/#understanding-quantization-lab-example","title":"Understanding Quantization - Lab Example","text":"<p>We will be using and running the code in Google Colab. This code demonstrates how much memory can be saved by using 4-bit quantization compared to 32-bit floating-point precision for a hypothetical model with 1000 weights. It calculates the total memory required for both 32-bit and 4-bit representations, determines the memory savings, and converts the result to bytes for easier understanding.</p> <p>By quantizing the model weights from 32-bit to 4-bit, significant memory savings are achieved, making it more efficient to deploy models on resource-constrained devices like phones and smartwatches.</p> <p>If you have not yet created a Google Colab account, please refer to the following section</p> <ul> <li>Browse to Google Colab and create a new Notebook</li> </ul> <p></p> <ul> <li>Select your runtime envoirnment</li> </ul> <p></p> <p> NOTE:  As we will be running a hypothetical model in Google Colab, you can use the CPU to run the code below. Our goal is to understand how quantization can help us use less memory.</p> <p> Reminder:  Whenever you want to copy the below code  into Google Colab and run it on individual line, be sure to click on + Code to add a new code cell. Also make sure to press the Play button to execute the code</p> <p></p> <p></p> <ul> <li>Let's break down the code step by step and explain what it does:</li> </ul> <p> Step 1:  Define Memory Weight for Float32:</p> <pre><code>float32_memory = 32  # in bits\nnum_weights = 1000  # hypothetical number of weights\n</code></pre> <ul> <li>float32_memory is set to 32 bits, which is the memory required for each weight when using 32-bit floating-point precision (Float32).</li> <li>num_weights is set to 1000, representing a hypothetical number of weights in the model.</li> </ul> <p> Step 2:  Calculate Total Memory for Float32 Weights:</p> <pre><code>float32_total_memory = float32_memory * num_weights  # in bits\n</code></pre> <ul> <li>float32_total_memory calculates the total memory required for all the weights when using Float32. This is done by multiplying the memory per weight (float32_memory) by the number of weights (num_weights).</li> </ul> <p> Step 3:  CDefine Memory Per Weight for 4-bit Quantization:</p> <pre><code>bit4_memory = 4  # in bits\n</code></pre> <ul> <li>bit4_memory is set to 4 bits, which is the memory required for each weight when using 4-bit quantization.</li> </ul> <p> Step 4:  Calculate Total Memory for 4-bit Quantized Weights:</p> <pre><code>bit4_total_memory = bit4_memory * num_weights  # in bits\n</code></pre> <ul> <li>bit4_total_memory calculates the total memory required for all the weights when using 4-bit quantization. This is done by multiplying the memory per weight (bit4_memory) by the number of weights (num_weights).</li> </ul> <p> Step 5:  Calculate Memory Saved:</p> <pre><code>memory_saved = float32_total_memory - bit4_total_memory  # in bits\nmemory_saved_in_bytes = memory_saved / 8  # convert bits to bytes\n</code></pre> <ul> <li>memory_saved calculates the total amount of memory saved by using 4-bit quantization instead of Float32. This is done by subtracting the total memory required for 4-bit weights (bit4_total_memory) from the total memory required for Float32 weights (float32_total_memory).</li> <li>memory_saved_in_bytes converts the saved memory from bits to bytes by dividing by 8 (since there are 8 bits in a byte).</li> </ul> <p> Step 5:  Print the Result: <pre><code>print(f\"Memory saved by using 4-bit quantization: {memory_saved_in_bytes} bytes\")\n</code></pre>  OUTPUT </p> <p><code>Memory saved by using 4-bit quantization: 3500.0 bytes</code></p> <ul> <li>Will changing the model from 32-bit to 4-bit reduce the model's accuracy?  Yes </li> </ul> <p> NOTE:  Let's sets up the context of the code, indicating that reducing the bit-width from 32-bit to 4-bit quantization can introduce a loss of accuracy. Please us ethe same notebook as above to continue with your code.</p> <p> Step 1:  Import Python Library NumPY</p> <p><pre><code>import numpy as np\n</code></pre> * Import the NumPy library, which is used for numerical operations on arrays.</p> <p> Step 2:  Simulate Original float32 Weights:</p> <p><pre><code>original_weights = np.random.rand(1000).astype(np.float32)\n</code></pre> * Generate an array of 1000 random floating-point numbers between 0 and 1, simulating the original model weights with 32-bit precision (float32).</p> <p> Step 3:  Simulate 4-bit Quantized Weights:</p> <p><pre><code>quantized_weights = np.round(original_weights * 15).astype(np.uint8)\n</code></pre> * Normalize the original weights to a range of 0 to 15 because 4-bit integers can represent 16 values (0 to 15). * Use np.round() to round the normalized weights to the nearest integer. * Convert the rounded values to 8-bit unsigned integers (np.uint8), which effectively simulates the 4-bit quantized weights.</p> <p> Step 4:  De-normalize to Get the Approximated Original Weights:</p> <p><pre><code>approximated_weights = quantized_weights / 15.0\n</code></pre> * De-normalize the quantized weights back to the original range by dividing by 15.0. This gives an approximation of the original weights after quantization.</p> <p> Step 5:  Calculate the Error</p> <p><pre><code>error = np.abs(original_weights - approximated_weights).mean()\n</code></pre> * Calculate the absolute difference between the original weights and the approximated weights. * Compute the mean of these differences to get the average quantization error.</p> <p> Step 6:  Print the Result:</p> <p><pre><code>print(f\"Average Quantization Error: {error}\")\n</code></pre> * Print the average quantization error to show the potential loss of accuracy introduced by reducing the bit-width from 32-bit to 4-bit.</p> <p> OUTPUT  </p> <p><code>Average Quantization Error: 0.016423573800129817</code></p> <p> Optional Step: Let\u2019s see the original generated random floating point weights </p> <pre><code>original_weights\n</code></pre> <p><code>0.9047668 , 0.81753767, 0.15176044, 0.82469964, 0.26190883,        0.25543845, 0.91670144, 0.44082588, 0.8387138 , 0.5301147 ,        0.597587  , 0.28645265, 0.31866026, 0.8412814 , 0.87486124,        0.14635882, 0.00842665, 0.40396062, 0.7154632 , 0.92024857,        0.58492464, 0.369594  , 0.3199917 , 0.01947883, 0.5505348 ,        0.09161595, 0.75082487, 0.9818501 , 0.800069  , 0.55406606,        0.87903994, 0.6254798 , 0.8290317 , 0.73905164, 0.42614278,        0.6899402 , 0.03985518, 0.3132978 , 0.97768533, 0.42348453,        0.60850483, 0.80669063, 0.18966372, 0.79855096, 0.73046243,        0.3299927 , 0.79309106, 0.01327937, 0.01851322, 0.9269705 ,        0.8274083 , 0.45073316, 0.51967096, 0.7210426 , 0.41424063,        0.24505323, 0.90779364, 0.64724004, 0.65470797, 0.14317876,        0.91511154, 0.57397956, 0.83594376, 0.09163648, 0.3037902 ,        0.01362608, 0.92493033, 0.6721083 , 0.5777128 , 0.98029524,        0.29148692, 0.5840548 , 0.805403  , 0.17140059, 0.9880919 ,        0.01023464, 0.8627494 , 0.89710003, 0.76805186, 0.9278038 ,        0.9041322 , 0.19642776, 0.78401035, 0.16597563, 0.16709319,        0.41655192, 0.6231926 , 0.95298046, 0.6890473 , 0.79470664,        0.7721617 , 0.2800831 , 0.32485998, 0.83056533, 0.84955883,        0.9345336 , 0.20025656, 0.31240928, 0.0943019 , 0.92295516,        0.07533643, 0.2782531 , 0.34772104, 0.20622289, 0.6932999 ,        0.01451688, 0.05406734, 0.69295406, 0.56339973, 0.74457246,        0.8313878 , 0.51609147, 0.4473876 , 0.29768255, 0.9012652 ,        0.7957006 , 0.5588959 , 0.54948616, 0.41233262, 0.04758253,        0.09688636, 0.19657251, 0.60875326, 0.6305743 , 0.62236613,        0.53727114, 0.60597694, 0.27432016, 0.5396689 , 0.35570994,        0.7779527 , 0.62765694, 0.5046841 , 0.95913595, 0.37506285,        0.06490876, 0.5690066 , 0.5926366 , 0.8343261 , 0.00439525,        0.75573623, 0.89352626, 0.45979103, 0.16634902, 0.2184745,        0.39872894, 0.68869066, 0.08809475, 0.8567979 , 0.2187864,        0.21870795, 0.52296937, 0.16986693, 0.6928503 , 0.41990125,        0.5135413 , 0.45609972, 0.38769007, 0.00987561, 0.15869647,        0.80649745, 0.8347802 , 0.7420551 , 0.07340008, 0.6842359 ,        0.33549812, 0.6722021 , 0.3444218 , 0.7548876 , 0.67199737,        0.04117178, 0.24504112, 0.39629644, 0.6458071 , 0.4220033 ,        0.44719183, 0.00884627, 0.8429784 , 0.5802084 , 0.07012173,        0.2552524 , 0.23461716, 0.08043662, 0.17875864, 0.35063305,        0.99479717, 0.3229556 , 0.7217851 , 0.09103325, 0.00775268,        0.3796975 , 0.24862018, 0.32581842, 0.530216  , 0.71165705,        0.5170614 , 0.06309021, 0.6869316 , 0.9265822 , 0.04267561,        0.44075203, 0.28398433, 0.46985227, 0.5829586 , 0.23592392,        0.95805424, 0.52453846, 0.8329666 , 0.14974083, 0.48412293,        0.04571012, 0.8544567 , 0.89506125, 0.10473824, 0.29405326,        0.29569003, 0.59090966, 0.66576856, 0.35182896, 0.12008226,        0.49510583, 0.8308251 , 0.14861059, 0.5513083 , 0.45685318,        0.8333833 , 0.9543581 , 0.42451006, 0.6116371 , 0.76232755,        0.81956553, 0.89240205, 0.2516925 , 0.92773354, 0.9864879 ,        0.04117434, 0.78647643, 0.21779937, 0.3965154 , 0.6441789 ,        0.6636646 , 0.547766  , 0.8827409 , 0.24284157, 0.1969708 ,        0.5668731 , 0.78194785, 0.6832345 , 0.10209882, 0.97626406,        0.5010292 , 0.12451114, 0.32503366, 0.17754969, 0.53745407],       dtype=float32)</code></p> <p> Optional Step  Lets see the quantized_weights (4- bit)</p> <pre><code>quantized_weights\n</code></pre> <p><code>6,  1,  2,  1,  8, 14,  3,  9, 13,  1, 10,  5, 14, 12, 13,  9,  2,         4,  1,  6,  7,  5, 15,  3,  0,  9,  9, 12,  2,  6, 11,  4,  3, 13,         8,  7, 14,  4,  9, 10,  0,  8,  9,  6,  9, 10,  8,  4,  1, 13,  1,         7,  3, 13,  5,  3, 11,  6, 12,  8,  8,  2,  8, 13, 10,  6,  6,  0,         0,  3,  7, 10, 11,  4, 11, 13,  3,  8,  8,  1,  5,  5,  7,  2, 13,         5, 12,  4, 11,  6,  9,  6,  8, 13,  4, 13,  6,  6, 15, 13,  2,  0,         4, 15,  7,  7,  5,  5,  2,  4,  8,  8, 15,  6, 11,  9, 12,  1, 13,         3, 14,  8,  1,  5, 15,  1,  9,  7,  1,  1, 13, 11, 12,  4,  5,  9,        14, 11, 14,  0, 12, 12,  2,  4,  9, 10,  8,  7,  3,  7,  9,  1, 13,         7,  9,  7, 12, 14, 11,  3, 11,  9, 11,  1, 13,  9,  4,  5,  2, 15,         8, 13,  1,  9,  3, 15,  3,  3, 12,  8, 14,  7, 11, 13,  3,  0, 10,        15, 11, 14,  4, 11,  7,  6, 13,  1,  4, 10,  1, 11,  9,  2, 10, 11,         8,  1,  5,  7,  8, 11, 13,  3,  3, 14,  4, 10, 10,  1,  6, 12, 13,        12,  7,  6,  8,  9, 10,  7, 12, 11,  1,  7, 14, 13,  8, 14,  1, 13,         7,  9, 12, 14, 14,  1, 14, 14, 11, 13,  2,  9, 11,  1, 13,  1, 13,         2,  8, 12,  1,  6,  7,  1, 12,  7,  4,  6,  8,  9,  8,  4,  1,  7,        13,  7, 12,  0,  6,  7, 11, 15,  7,  6, 13, 12, 12, 10,  8,  9, 12,         0, 10,  1, 13,  9,  3,  2, 14,  9,  6,  9,  6,  4, 15,  5,  5,  5,        12,  5, 15,  4,  7,  0,  3,  3, 15, 15,  3, 10,  2, 15,  7,  8,  2,         2, 10, 11, 14,  4,  0,  8,  1,  8,  5,  1, 12,  5,  6, 14, 11, 10,         2,  6, 11, 12,  8, 12,  0, 14, 12,  4,  9, 11,  7, 14,  2,  4,  4,        10,  4, 13,  1, 14,  0, 15,  8,  6,  5, 12, 10,  6,  1,  4,  7,  7,         7, 10, 10,  5,  8, 12,  8,  6, 12,  2,  1, 14, 15, 13,  9,  3,  3,         8,  7,  3, 10,  7,  1, 10,  7,  6,  4,  8,  2, 11,  2, 15,  2, 11,        12, 10,  5,  7, 13,  2,  6, 15, 13,  6,  1, 10, 12, 10, 11,  4,  1,         8,  3,  4, 12, 15, 12,  9,  8,  1, 12,  2,  2, 14,  2,  9,  7,  4,         4,  3,  2, 11,  0,  5,  9, 12, 15, 14, 13,  1, 15,  3, 12, 10,  3,        13,  5,  8,  9, 15,  2, 14,  8,  8,  6,  7,  1,  6,  3, 15,  3,  0,        13,  9,  6, 12,  5,  5,  5,  9,  9,  2, 13, 15,  3,  9, 11,  1,  8,        13,  9, 12, 14,  5,  9,  2,  8, 14, 10,  4, 12,  8,  9, 10, 13, 14,         3,  1, 14,  9,  9, 12,  2,  1,  7,  4,  2,  0,  4, 10,  2,  6,  3,        10, 10,  9, 11,  9,  8, 15, 12,  3,  2,  5,  9,  6, 11,  1,  9,  5,        14,  7, 14,  4, 13, 14,  5,  8, 14, 10, 12,  3,  1, 12,  9, 14,  1,         2, 11, 11,  2,  2, 13, 12,  3,  4, 12,  7,  1,  9,  3,  4,  4, 14,        14,  8,  0, 12,  8,  6, 12, 13,  6, 11,  6, 10,  9,  6,  2, 14, 10,         4, 10,  2, 15,  9, 14,  0,  4,  0, 10,  1, 15,  5,  7, 10,  6, 12,        10, 11,  6,  7,  2, 14,  8, 12,  7, 14,  9, 13,  0,  5,  7,  2, 11,         9, 11, 13, 10, 14,  8,  8, 14,  4, 10,  1, 11, 15, 10,  0,  6,  0,        13, 14,  9,  4,  4, 12,  5,  9,  4,  6,  2, 10,  3,  4,  1, 14, 11,         6,  7,  1,  4, 11,  1,  7, 12,  8, 14,  2,  8,  8,  3, 11, 15, 10,         2, 14, 12,  2, 12,  4,  4, 14,  7, 13,  8,  9,  4,  5, 13, 13,  2,         0,  6, 11, 14,  9,  6,  5,  0,  8,  1, 11, 15, 12,  8, 13,  9, 12,        11,  6, 10,  1,  5, 15,  6,  9, 12,  3, 12, 11,  5, 12,  0,  0, 14,        12,  7,  8, 11,  6,  4, 14, 10, 10,  2, 14,  9, 13,  1,  5,  0, 14,        10,  9, 15,  4,  9, 12,  3, 15,  0, 13, 13, 12, 14, 14,  3, 12,  2,         3,  6,  9, 14, 10, 12, 12,  4,  5, 12, 13, 14,  3,  5,  1, 14,  1,         4,  5,  3, 10,  0,  1, 10,  8, 11, 12,  8,  7,  4, 14, 12,  8,  8,         6,  1,  1,  3,  9,  9,  9,  8,  9,  4,  8,  5, 12,  9,  8, 14,  6,         1,  9,  9, 13,  0, 11, 13,  7,  2,  3,  3, 14, 12, 11,  4, 13, 15,         4,  7, 13, 13,  0,  8, 13, 13, 15,  8, 15,  3, 11,  4,  8,  9,  2,         2, 11,  3,  7,  4,  7, 11,  6, 10,  1, 13,  3,  8,  3, 10,  6,  8,         7,  6,  0,  2, 12, 13, 11,  1, 10,  5, 10,  5, 11, 10,  1,  4,  6,        10,  6,  7,  0, 13,  9,  1,  4,  4,  1,  3,  5, 15,  5, 11,  1,  0,         6,  4,  5,  8, 11,  8,  1, 10, 14,  1,  7,  4,  7,  9,  4, 14,  8,        12,  2,  7,  1, 13, 13,  2,  4,  4,  9, 10,  5,  2,  7, 12,  2,  8,         7, 13, 14,  6,  9, 11, 12, 13,  4, 14, 15,  1, 12,  3,  6, 10, 10,         8, 13,  4,  3,  9, 12, 10,  2, 15,  8,  2,  5,  3,        dtype=uint8)</code></p> <p></p> <ul> <li> <p>After quantizing LLMs (Large Language Models) from 32-bit to 8-bit precision, we achieve significant reductions in memory usage and computational demands. However, this process can also result in some loss of information and a decrease in accuracy. This brings up an important question: why should we fine-tune a quantized model?</p> </li> <li> <p>Fine-tuning becomes crucial in this context because it helps to restore or even enhance the model's performance after quantization. By fine-tuning, we can adapt the model to specific tasks or datasets, which can mitigate the accuracy loss that occurs during quantization.</p> </li> </ul>"},{"location":"Task8/#the-functioning-of-lora-in-fine-tuning-llms","title":"The Functioning of LoRA in Fine-tuning LLMs","text":"<p>LoRA, or Low-Rank Adaptation, is a method developed by Microsoft researchers to optimize the fine-tuning of Large Language Models. This technique tackles the issues related to the fine-tuning process, such as extensive memory demands and computational inefficiency. LoRA introduces a compact set of parameters, referred to as low-rank matrices, to store the necessary changes in the model instead of altering all parameters.</p> <p>Here are the key features of how LoRA operates:</p> <ul> <li>Maintaining Pretrained Weights: LoRA adopts a unique strategy by preserving the pretrained weights of the model. This approach reduces the risk of catastrophic forgetting, ensuring the model maintains the valuable knowledge it gained during pretraining.</li> <li>Efficient Rank-Decomposition: LoRA incorporates rank-decomposition weight matrices, known as update matrices, to the existing weights. Rank decomposition is a way to simplify a large, complex matric by breaking it down into the product of smaller matrices. These update matrices have significantly fewer parameters than the original model, making them highly memory-efficient. By training only these newly added weights, LoRA achieves a faster training process with reduced memory demands. These LoRA matrices are typically integrated into the attention layers of the original model.</li> </ul> <p>By using the low-rank decomposition approach, the memory demands for training large language models are significantly reduced. This allows running fine-tuning tasks on consumer-grade GPUs, making the benefits of LoRA available to a broader range of researchers and developers.</p> <p></p>"},{"location":"Task8/#open-source-resources-for-lora","title":"Open-source Resources for LoRA","text":"<p>The following libraries offer a mix of tools that enhance the efficiency of fine-tuning large language models. They provide optimizations, compatibility with different data types, resource efficiency, and user-friendly interfaces that accommodate various tasks and hardware configurations.</p> <ul> <li>PEFT Library: Parameter-efficient fine-tuning (PEFT) methods facilitate efficient adaptation of pre-trained language models to various downstream applications without fine-tuning all the model's parameters. By fine-tuning only a portion of the model's parameters, PEFT methods like LoRA, Prefix Tuning, and P-Tuning, including QLoRA, significantly reduce computational and storage costs.</li> <li>Lit-GPT: Lit-GPT from LightningAI is an open-source resource designed to simplify the fine-tuning process, making it easier to apply LoRA's techniques without manually altering the core model architecture. Models available for this purpose include Vicuna, Pythia, and Falcon. Specific configurations can be applied to different weight matrices, and precision settings can be adjusted to manage memory consumption.</li> </ul> <p>In our lab, we\u2019ll focus on the PEFT library.</p>"},{"location":"Task8/#qlora-an-efficient-variant-of-lora","title":"QLoRA: An Efficient Variant of LoRA","text":"<p>QLoRA, or Quantized Low-Rank Adaptation, is a popular variant of LoRA that makes fine-tuning large language models even more efficient. QLoRA introduces several innovations to save memory without sacrificing performance.</p> <p>The technique involves backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low-Rank Adapters. This approach significantly reduces memory usage, enabling the fine-tuning of even larger models on consumer-grade GPUs. For instance, QLoRA can fine-tune a 65 billion parameter model on a single 48GB GPU while preserving full 16-bit fine-tuning task performance.</p> <p>QLoRA uses a new data type known as 4-bit NormalFloat (NF4), which is optimal for normally distributed weights. It also employs double quantization to reduce the average memory footprint by quantizing the quantization constants and paged optimizers to manage memory spikes.</p> <p>The practical implementation of QLoRA for fine-tuning LLMs is very accessible, thanks to open-source libraries and tools. For instance, the BitsAndBytes library offers functionalities for 4-bit quantization. In the coming sections we will see a code example showing how to use QLoRA with PEFT.</p>"},{"location":"Task8/#conclusion","title":"Conclusion","text":"<p>In this lesson, we explored LoRA and QLoRA, two powerful techniques for fine-tuning Large Language Models (LLMs). We examined how LoRA operates by preserving pretrained weights and introducing low-rank matrices, thereby enhancing the memory and computational efficiency of the fine-tuning process. We also highlighted open-source libraries like PEFT and Lit-GPT that facilitate the implementation of LoRA.</p> <p>Additionally, we discussed QLoRA, an efficient variant of LoRA that employs 4-bit NormalFloat and double quantization to further reduce memory usage. This technique allows for significant memory savings but can introduce quantization errors, impacting model accuracy.</p> <p>By quantizing weights to lower precision, we achieve reduced memory usage and potentially increased processing speed, at the cost of a possible minor loss in accuracy. The provided code example illustrates this trade-off by converting a set of 32-bit floating-point weights into 4-bit integer weights, demonstrating the balance between memory efficiency and model accuracy.</p> <p>Overall, you have successfully learned how to use quantization to optimize LLMs, understanding the benefits and potential drawbacks of this approach.</p>"},{"location":"Task8a/","title":"Task 8a - Configuring and Fine Tuning - Using llama2","text":""},{"location":"Task8a/#introduction","title":"Introduction","text":"<p>This diagram represents the workflow of training and using a LLM. Here\u2019s a step-by-step explanation:</p> <ul> <li>Build Dataset: Once we have determined that fine-tuning is the right solution, we need to create a dataset to fine-tune our model. Datasets are collections of structured or unstructued data used to train machine learning models.</li> <li>Choose Base Model: A base model is selected. This could be a pre-trained model that will be fine-tuned with the new dataset e.g Llama2 or Llama3 (in our lab)</li> <li>Setup Lora Adapter: The LoRA (Low-Rank Adaptation) adapter is set up a technique used to fine-tune the model efficiently.</li> <li>Train and Monitor: The model is trained using the dataset. During training, the process is monitored to ensure it\u2019s proceeding correctly and to make adjustments if necessary.</li> <li>Merged Model: Once the model has been tested and validated, it is considered a merged model. This model is ready to handle user queries.</li> <li>User Interaction: Users can now interact with the model by sending queries. The model processes these queries and returns appropriate responses.</li> </ul>"},{"location":"Task8a/#pre-requisites","title":"Pre-requisites","text":""},{"location":"Task8a/#usecase-1-fine-tuning-using-llama2","title":"UseCase 1 - Fine-Tuning Using Llama2","text":""},{"location":"Task8a/#datasets-creation","title":"Datasets Creation","text":"<p>There are several ways to create datasets:</p> <ul> <li>Using existing open source datasets e.g. The Pile, Common Crawl , Wiki or even HuggingFace</li> <li>Using LLM to create synthetic datasets faster and can be less expensive</li> <li>Using your own custom datasets e.t.c &lt;-- Focus for our lab</li> </ul> <p></p>"},{"location":"Task8a/#custom-datasets","title":"Custom datasets","text":"<p>As we will be using and Fine-Tuning Llam2  we need to convert them(datasets) into a uniform format compatible with training regimens More Info</p>"},{"location":"Task8a/#example-format-for-llama2","title":"Example Format for Llama2","text":"Sample Format - Dont Copy - reference ONLY<pre><code>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\n{{ system_prompt }}\n&lt;&lt;/SYS&gt;&gt;\n{{ user_message }} [/INST] Model answer &lt;/s&gt;\n</code></pre>"},{"location":"Task8a/#datasets-creation_1","title":"Datasets Creation","text":"<p>After defining the problem and confirming that LLMs are a suitable solution, the next step is to prepare a dataset for fine-tuning. If you already have a clean, high-quality dataset, that's great. However, let's assume you don't have one readily available.</p> <p>In my scenario, I was able to generate synthetic dataset from the Cisco Preferred Architecture for Webex Calling.</p>"},{"location":"Task8a/#creating-a-dataset-from-a-pdf-summary","title":"Creating a Dataset from a PDF - Summary","text":"<p>To delve into the intricacies of dataset creation, let's explore a scenario where we aim to train an AI to replicate our Preferred architecture. In this lab, we will transform a PDF document into a dataset. Specifically, we will use a PDF about Webex Calling as our source material.</p> <p>Our goal is to convert the PDF into a dynamic and engaging dialogue, similar to an interview format. This will allow us to create a more interactive and informative dataset.</p> <p>Steps Involved</p> <p>Converting the PDF to Text</p> <ul> <li>Begin by extracting the text content from the PDF document.</li> </ul> <p>Paragraph Analysis and Segmentation:</p> <ul> <li>Once the PDF is in text form, analyze and segment it into individual paragraphs. Combine short paragraphs and split longer ones to ensure each segment is coherent and self-contained, while still contributing to the overall narrative.</li> </ul> <p>Generating Interview Questions:</p> <ul> <li>For each paragraph, create an artificial interview scenario where a language model (LLM) acts as the interviewer.</li> <li>Develop questions that naturally elicit responses based on the content of the paragraph.</li> <li>The objective is to simulate an insightful dialogue, making it seem as though the PDF is responding to questions about its content and experiences.</li> </ul> <p>By following these steps, we will convert the static information in the PDF into a dynamic and interactive dataset, facilitating a deeper understanding and more engaging learning experience. You can find the complete code that worked for me here.</p> <p>Here's an example of the resulting file. Please note that for this lab demonstration, only a few data points were converted from the PDF.</p> <p>Note: We will use the following file to create and eventually train downstream tasks for our LLM, specifically the Llama2 model.</p> <p>Download example.txt</p>"},{"location":"Task8a/#sample-file-content","title":"Sample File Content","text":"example.txt<pre><code>human_text: What is the solution provided by Webex Calling? assistant_text: Webex Calling provides enterprise-grade PBX features, functionality, and performance.\nhuman_text: What options are available for PSTN access in Webex Calling? assistant_text: The options are Cisco Calling Plans, Cloud Connected PSTN, and Premises-based PSTN.\nhuman_text: What type of devices does Webex Calling support? assistant_text: Webex Calling supports Cisco MPP Series IP Phones, Webex Devices, and a variety of third-party devices.\nhuman_text: How are Webex Calling data centers distributed? assistant_text: Webex Calling data centers are globally distributed and geo-redundant, located in six regions: US, Canada, Europe, UK, Australia, and Japan.\nhuman_text: What is the function of the Local Gateway in Webex Calling? assistant_text: The Local Gateway connects to the PSTN and registers with Webex Calling to handle all calls between the PSTN and Webex Calling.\nhuman_text: What are the key benefits of Webex Calling for large enterprises? assistant_text: Webex Calling offers scalability, advanced call management, and integration with collaboration tools for large enterprises.\nhuman_text: How does Webex Calling handle call encryption? assistant_text: SRTP and TLS are used in Webex Calling to encrypt media and signaling for secure communications.\nhuman_text: What features are included in the Webex Calling analytics suite? assistant_text: Call quality, usage patterns, and performance metrics are part of the Webex Calling analytics suite.\nhuman_text: How does Webex Calling integrate with third-party devices? assistant_text: Integration with third-party SIP phones provides flexibility in device choices for Webex Calling users.\nhuman_text: What disaster recovery mechanisms are available in Webex Calling? assistant_text: Geo-redundant data centers and automatic failover mechanisms support disaster recovery in Webex Calling.\nhuman_text: How does Webex Calling manage international call routing? assistant_text: A global network of data centers manages international call routing in Webex Calling.\nhuman_text: What role does the Webex Control Hub play in Webex Calling? assistant_text: Provisioning, configuring, and managing Webex Calling services are handled through the Webex Control Hub.\nhuman_text: What are the main security protocols used in Webex Calling? assistant_text: SRTP and TLS are the main security protocols used in Webex Calling.\nhuman_text: How does Webex Calling support mobile users? assistant_text: The Webex App enables mobile users to make and receive calls on smartphones and tablets in Webex Calling.\nhuman_text: What are the benefits of Webex Calling for remote teams? assistant_text: Remote teams benefit from enterprise-grade calling features, mobility, and collaboration tool integration in Webex Calling.\nhuman_text: How does Webex Calling ensure compliance with regulatory requirements? assistant_text: Compliance with regulatory requirements in Webex Calling is ensured through call recording, monitoring, and secure data handling.\nhuman_text: What redundancy features are built into Webex Calling? assistant_text: Geo-redundant data centers and automatic failover ensure continuous service in Webex Calling.\nhuman_text: What user management capabilities are available in Webex Calling? assistant_text: User settings and permissions are managed through the Webex Control Hub in Webex Calling.\nhuman_text: How does Webex Calling handle call routing? assistant_text: Dial plans and customizable routing rules manage call routing in Webex Calling.\nhuman_text: What kind of customer support does Cisco provide for Webex Calling? assistant_text: Technical assistance, training, and deployment resources are part of Cisco's support for Webex Calling.\nhuman_text: How does Webex Calling support hybrid deployments? assistant_text: Integration with cloud and on-premises PBX systems supports hybrid deployments in Webex Calling.\nhuman_text: What are the call management features in Webex Calling? assistant_text: Call management features in Webex Calling include call forwarding, call transfer, call hold, and voicemail.\nhuman_text: How does Webex Calling support video calls? assistant_text: Video calls are supported between video-capable devices in Webex Calling.\nhuman_text: What compliance features are available in Webex Calling? assistant_text: Compliance features in Webex Calling include call recording, logging, and monitoring.\nhuman_text: What is the function of SRTP in Webex Calling? assistant_text: SRTP encrypts media streams in Webex Calling to ensure secure communication.\nhuman_text: How does Webex Calling support call analytics? assistant_text: Webex Calling provides detailed call analytics on call quality, usage patterns, and performance.\nhuman_text: What kind of redundancy features are available in Webex Calling? assistant_text: redundancy features in Webex Calling include geo-redundant data centers and automatic failover.\nhuman_text: How does Webex Calling ensure secure voice communications? assistant_text: Encryption protocols like SRTP for media and TLS for signaling ensure secure voice communications in Webex Calling.\nhuman_text: What integrations does Webex Calling offer with contact center solutions? assistant_text: Integration with Webex Contact Center offers advanced features like call routing, IVR, and analytics.\nhuman_text: How does Webex Calling handle call monitoring and recording? assistant_text: Call monitoring and recording features in Webex Calling support compliance, training, and quality assurance.\nhuman_text: What options are available for user authentication in Webex Calling? assistant_text: Webex Calling supports user authentication through secure protocols and integration with identity providers for SSO.\nhuman_text: What is the role of the Webex App in Webex Calling? assistant_text: The Webex App unifies messaging, video conferencing, and calling for a seamless communication experience.\nhuman_text: How does Webex Calling support international business operations? assistant_text: Global data centers support reliable and high-quality voice communication for international operations in Webex Calling.\nhuman_text: What management capabilities does Webex Calling provide? assistant_text: The Webex Control Hub provides centralized provisioning, configuration, and monitoring for Webex Calling services.\nhuman_text: How does Webex Calling handle voice and video integration? assistant_text: Integration with Webex Meetings and Webex Teams allows for seamless voice and video communication in Webex Calling.\nhuman_text: What are the benefits of Webex Calling for large enterprises? assistant_text: Scalability, advanced call management, and integration with collaboration tools are benefits for large enterprises using Webex Calling.\nhuman_text: What disaster recovery options are available with Webex Calling? assistant_text: Geo-redundant data centers and automatic failover provide disaster recovery options in Webex Calling.\nhuman_text: How does Webex Calling manage call quality? assistant_text: Call quality in Webex Calling is managed through network optimization, advanced codecs, and QoS policies.\nhuman_text: What user profile management features are available in Webex Calling? assistant_text: User profile management features in Webex Calling include settings and permissions configuration through the Webex Control Hub.\nhuman_text: How does Webex Calling integrate with CRM systems? assistant_text: Integration with CRM systems enhances customer interactions and business processes in Webex Calling.\nhuman_text: What compliance features does Webex Calling offer? assistant_text: Compliance features in Webex Calling include call recording, monitoring, and secure data handling.\nhuman_text: How does Webex Calling support remote teams? assistant_text: Remote teams benefit from enterprise-grade calling features, mobility, and collaboration tool integration in Webex Calling.\nhuman_text: What are the main security features of Webex Calling? assistant_text: Security features in Webex Calling include encryption, secure voice, and industry-standard compliance.\nhuman_text: How does Webex Calling handle emergency call routing? assistant_text: Predefined routing rules manage emergency call routing in Webex Calling.\nhuman_text: What kind of support does Cisco provide for Webex Calling? assistant_text: Technical assistance, training, and deployment resources are part of Cisco's support for Webex Calling.\nhuman_text: How does Webex Calling support hybrid work environments? assistant_text: Webex Calling supports hybrid work environments by integrating with cloud and on-premises PBX systems.\nhuman_text: What management tools are available in Webex Calling? assistant_text: Management tools in Webex Calling include the Webex Control Hub for provisioning, configuring, and managing services.\nhuman_text: How does Webex Calling handle call quality issues? assistant_text: Tools for monitoring and diagnosing issues ensure high-quality voice communication in Webex Calling.\nhuman_text: What kind of analytics does Webex Calling offer? assistant_text: Analytics on call quality, usage patterns, and performance metrics are available in Webex Calling.\nhuman_text: What devices are compatible with Webex Calling? assistant_text: Compatible devices for Webex Calling include Cisco IP Phones, Webex Room Devices, and third-party SIP phones.\nhuman_text: How does Webex Calling integrate with Webex Meetings? assistant_text: Integration with Webex Meetings provides a seamless experience for scheduling and joining video meetings in Webex Calling.\nhuman_text: What role does Webex Control Hub play in Webex Calling? assistant_text: Webex Control Hub provides a centralized interface for provisioning, configuring, and managing Webex Calling services.\nhuman_text: How does Webex Calling ensure security for voice communications? assistant_text: Encryption protocols like SRTP for media and TLS for signaling ensure security in Webex Calling.\nhuman_text: What features does Webex Calling offer for team collaboration? assistant_text: Group call management, conferencing, and Webex Teams integration are features of Webex Calling for team collaboration.\nhuman_text: How is call recording handled in Webex Calling? assistant_text: Built-in call recording features in Webex Calling support compliance, training, and quality assurance.\nhuman_text: What scalability options are available in Webex Calling? assistant_text: Scalability in Webex Calling supports small businesses to large enterprises, accommodating thousands of users.\nhuman_text: How does Webex Calling support international operations? assistant_text: Global data centers ensure reliable and high-quality voice communication for international operations in Webex Calling.\nhuman_text: What redundancy features are built into Webex Calling? assistant_text: redundancy features in Webex Calling include geo-redundant data centers and automatic failover for continuous service.\nhuman_text: How does Webex Calling handle user authentication? assistant_text: Secure protocols and integration with identity providers for single sign-on (SSO) manage user authentication in Webex Calling.\nhuman_text: What reporting capabilities does Webex Calling provide? assistant_text: Detailed call analytics, usage patterns, and performance metrics are part of Webex Calling's reporting capabilities.\nhuman_text: What integrations does Webex Calling support with CRM systems? assistant_text: Integrations with CRM systems in Webex Calling enhance customer interactions and streamline business processes.\nhuman_text: How does Webex Calling support compliance with regulations? assistant_text: Compliance features in Webex Calling include call recording, monitoring, and secure data handling.\nhuman_text: What is the role of SRTP in Webex Calling? assistant_text: SRTP encrypts voice media streams to ensure secure communication in Webex Calling.\nhuman_text: How does Webex Calling manage user profiles? assistant_text: Webex Control Hub manages user profiles in Webex Calling, enabling configuration of settings and permissions by administrators.\nhuman_text: How does Webex Calling support video conferencing? assistant_text: Webex Calling supports video conferencing via integration with Webex Meetings.\nhuman_text: What is the role of a Local Gateway in Webex Calling? assistant_text: The Local Gateway connects to the PSTN and handles call routing between Webex Calling and on-premises PBX systems.\nhuman_text: How does Webex Calling handle number porting? assistant_text: Webex Calling allows users to port existing phone numbers from another provider.\nhuman_text: How does Webex Calling support disaster recovery? assistant_text: Geo-redundant data centers and automatic failover mechanisms support disaster recovery in Webex Calling.\nhuman_text: What is the Webex Calling architecture? assistant_text: Webex Calling has a cloud-based architecture with distributed data centers for high availability and resilience.\nhuman_text: How does Webex Calling support call encryption? assistant_text: SRTP and TLS are used to encrypt call signaling and media in Webex Calling.\nhuman_text: What are the benefits of Webex Calling for small businesses? assistant_text: Small businesses benefit from enterprise-grade calling features, scalability, and collaboration tool integration at a lower cost with Webex Calling.\nhuman_text: How does Webex Calling integrate with existing collaboration tools? assistant_text: Integration with Webex Teams and Microsoft Teams offers a seamless collaboration experience in Webex Calling.\nhuman_text: What analytics features are available in Webex Calling? assistant_text: Analytics features in Webex Calling include call quality monitoring, usage reports, and performance dashboards.\nhuman_text: How does Webex Calling support mobile devices? assistant_text: The Webex App enables Webex Calling users to make and receive calls on mobile devices.\nhuman_text: What is the role of the Webex App in Webex Calling? assistant_text: The Webex App offers a unified interface for messaging, video conferencing, and calling in Webex Calling.\nhuman_text: How does Webex Calling handle multi-location deployments? assistant_text: Centralized management and consistent calling features support multi-location deployments in Webex Calling.\nhuman_text: What troubleshooting tools are available in Webex Calling? assistant_text: Troubleshooting tools in Webex Calling include diagnostic reports, call logs, and real-time call quality monitoring.\nhuman_text: How does Webex Calling ensure voice quality? assistant_text: Voice quality in Webex Calling is ensured by network optimization, advanced codecs, and QoS policies.\nhuman_text: What integrations does Webex Calling support? assistant_text: Integrations with CRM systems, collaboration tools, and contact center solutions are supported by Webex Calling.\nhuman_text: How does Webex Calling handle call recording? assistant_text: Call recording in Webex Calling is available for compliance, training, and quality assurance.\nhuman_text: What role do data centers play in Webex Calling? assistant_text: Data centers provide redundancy, high availability, and disaster recovery in Webex Calling.\nhuman_text: What are the benefits of Webex Calling for remote teams? assistant_text: Enterprise-grade calling features, mobility, and collaboration tool integration benefit remote teams using Webex Calling.\nhuman_text: How does Webex Calling handle call forwarding? assistant_text: Flexible call forwarding options in Webex Calling allow users to redirect calls to other numbers or devices.\nhuman_text: What is Webex Calling? assistant_text: Webex Calling is a cloud-based phone system offering enterprise-grade calling features.\nhuman_text: What types of phones are supported by Webex Calling? assistant_text: Webex Calling supports Cisco MPP Series IP Phones, Webex Devices, and various third-party devices.\nhuman_text: How does Webex Calling integrate with other Webex services? assistant_text: Webex Calling integrates with Webex Meetings, Webex Teams, and Webex Contact Center.\nhuman_text: What are the deployment options for Webex Calling? assistant_text: Webex Calling offers cloud-only and hybrid deployment options.\nhuman_text: How is call quality maintained in Webex Calling? assistant_text: Advanced codecs, network optimization, and QoS policies maintain call quality in Webex Calling.\nhuman_text: What is the purpose of the Webex Control Hub? assistant_text: The Webex Control Hub centralizes management for provisioning, configuring, and managing Webex Calling services.\nhuman_text: How does Webex Calling support remote work? assistant_text: Webex Calling supports remote work by offering enterprise-grade calling features and connectivity from any location with internet access.\nhuman_text: What security measures are implemented in Webex Calling? assistant_text: Encryption, secure voice, and industry-standard compliance are key security measures in Webex Calling.\nhuman_text: How does Webex Calling handle emergency calls? assistant_text: Emergency calls in Webex Calling are handled by comparing the dial string with defined emergency numbers.\nhuman_text: What are the benefits of Webex Calling for enterprises? assistant_text: Lower maintenance costs, scalability, remote work support, and collaboration tool integration are benefits for enterprises using Webex Calling.\nhuman_text: How does Webex Calling manage call routing? assistant_text: Dial plans and route groups manage call routing in Webex Calling.\nhuman_text: What customer support is available for Webex Calling? assistant_text: Webex Calling customer support includes technical assistance, training, and deployment resources from Cisco.\nhuman_text: How does Webex Calling ensure high availability? assistant_text: Geo-redundant data centers and a redundant global backbone network ensure high availability in Webex Calling.\nhuman_text: What are the international calling capabilities of Webex Calling? assistant_text: International calling is supported by Webex Calling's global data center network for high-quality voice communication.\nhuman_text: What is the role of SIP in Webex Calling? assistant_text: SIP manages signaling and multimedia communication sessions in Webex Calling.\nhuman_text: How does Webex Calling handle compliance with local regulations? assistant_text: Compliance with local regulations is ensured by routing calls through regional data centers and supporting lawful intercept.\nhuman_text: What integration options are available for Webex Calling? assistant_text: Webex Calling integrates with Webex Meetings, Webex Teams, and third-party tools for enhanced productivity.\nhuman_text: How does Webex Calling support contact centers? assistant_text: Advanced contact center features like call routing, IVR, and analytics are provided through Webex Contact Center integration.\nhuman_text: What is the function of SRTP in Webex Calling? assistant_text: SRTP encrypts media streams to ensure secure communication in Webex Calling.\nhuman_text: How does Webex Calling handle video calls? assistant_text: Video calls are supported between video-capable MPP phones, Webex Devices, and the Webex App in Webex Calling.\nhuman_text: What are the benefits of Webex Calling for remote workers? assistant_text: Webex Calling offers remote workers enterprise-grade calling features and connectivity from any location with internet access.\nhuman_text: What is the role of the Webex Control Hub in managing Webex Calling? assistant_text: The Webex Control Hub centralizes management for provisioning, configuring, and managing Webex Calling services.\nhuman_text: How does Webex Calling integrate with existing on-premises PBX systems? assistant_text: Webex Calling integrates with on-premises PBX systems via Local Gateways for hybrid deployment.\nhuman_text: What is the significance of SRTP in Webex Calling? assistant_text: SRTP encrypts media streams in Webex Calling to ensure secure communication.\nhuman_text: How does Webex Calling support compliance with local regulations? assistant_text: Webex Calling complies with local regulations by using regional data centers and supporting lawful intercept.\nhuman_text: What features are available for call management in Webex Calling? assistant_text: Webex Calling offers call forwarding, call transfer, call hold, and voicemail features.\nhuman_text: What options are available for integrating Webex Calling with other collaboration tools? assistant_text: Webex Calling integrates with Webex Meetings, Webex Teams, and third-party tools for enhanced productivity.\nhuman_text: How does Webex Calling handle call routing? assistant_text: Dial plans and route groups manage call routing in Webex Calling.\nhuman_text: What security features are built into Webex Calling? assistant_text: Webex Calling features encryption, secure voice, and industry-standard compliance for communication security.\nhuman_text: How does Webex Calling support mobile users? assistant_text: The Webex App enables mobile users to make and receive calls on their devices with Webex Calling.\nhuman_text: How does Webex Calling integrate with Cisco devices? assistant_text: Cisco MPP phones and Webex Room devices integrate with Webex Calling for unified communication.\nhuman_text: What are the deployment models available for Webex Calling? assistant_text: Webex Calling supports cloud-only and hybrid deployment models.\nhuman_text: How does Webex Calling handle international calling? assistant_text: International calling is supported by Webex Calling's global data center network for high-quality voice communication.\nhuman_text: What features does Webex Calling offer for contact centers? assistant_text: Advanced contact center features like call routing, IVR, and analytics are offered through Webex Contact Center integration.\nhuman_text: How does Webex Calling support scalability? assistant_text: The cloud-based architecture of Webex Calling supports scalability for businesses of all sizes.\nhuman_text: What type of customer support is available for Webex Calling? assistant_text: Webex Calling customer support includes technical assistance, training, and deployment resources from Cisco.\nhuman_text: How does Webex Calling handle voice quality? assistant_text: High voice quality in Webex Calling is ensured by advanced codecs, network optimization, and QoS policies.\nhuman_text: What are the benefits of using Webex Calling over traditional PBX systems? assistant_text: Benefits of Webex Calling include lower maintenance costs, scalability, remote work support, and collaboration tool integration.\nhuman_text: What signaling and media protocols does Webex Calling use? assistant_text: Webex Calling uses SIP for signaling and SRTP for media.\nhuman_text: What is the role of the Webex Control Hub in Webex Calling? assistant_text: The Webex Control Hub provides connection parameters and digest credentials for SIP authentication during Local Gateway registration.\nhuman_text: What group features does Webex Calling provide? assistant_text: Webex Calling provides group features like unlimited subscriptions of auto-attendants, hunt groups, and call queues.\nhuman_text: What are the regional platforms for Webex Calling? assistant_text: Webex Calling operates regional platforms in the US, Canada, UK, Europe, APJC Japan, and APJC Australia.\nhuman_text: What role do load balancers play in Webex Calling datacenters? assistant_text: Load balancers are used to build a scalable, redundant datacenter architecture.\nhuman_text: What are the capabilities of the Webex App in Webex Calling? assistant_text: The Webex App supports mid-call features, rich presence, and control of the user's Cisco MPP phone.\nhuman_text: What is the role of the Webex Control Hub in Local Gateway registration? assistant_text: The Webex Control Hub provides connection parameters and digest credentials for SIP authentication during Local Gateway registration.\nhuman_text: What deployment options are available for Webex Calling? assistant_text: Webex Calling can be deployed as a cloud-only solution or as part of a hybrid cloud.\nhuman_text: What are the data center locations for Webex Calling in the US? assistant_text: Webex Calling data centers in the US are located in Dallas, Chicago, and New York.\nhuman_text: What capabilities does the Webex App offer when integrated with Webex Calling? assistant_text: The Webex App offers messaging, screen sharing, audio and video conferencing, and integrated calling with mid-call features or control of a user\u2019s desk phone.\nhuman_text: What are the considerations for video calls in Webex Calling? assistant_text: Video calls can be made within a single Webex Calling org between video-capable devices or Webex App, while PSTN only supports voice calls.\nhuman_text: What are the main functions hosted in each Webex Calling datacenter? assistant_text: Webex Calling datacenters host call routing functions, provide provisioning interface access, and host access and peering SBCs.\nhuman_text: How does Webex Calling ensure firewall traversal for calls? assistant_text: Webex Calling ensures firewall traversal by using TLS connections initiated by phones and Local Gateways, and sending traffic back through the same connection.\nhuman_text: How are trunks used in Webex Calling? assistant_text: Trunks connect Webex Calling with Local Gateways or Dedicated Instances.\nhuman_text: What is the Private Network Connect (PNC) solution? assistant_text: The Private Network Connect (PNC) solution extends private networks to the cloud for high quality of service and low latency.\nhuman_text: What type of subscription is Webex Calling based on? assistant_text: Webex Calling uses a subscription-based licensing model managed with the Cisco Collaboration Flex Plan.\nhuman_text: Where are Webex Calling data centers located? assistant_text: Webex Calling data centers are located in the US, Canada, Europe, UK, Australia, and Japan.\nhuman_text: How does Webex Calling handle emergency calls? assistant_text: Emergency calls are handled by comparing the dial string with emergency numbers defined in the national numbering plan.\nhuman_text: How are unknown numbers handled in Webex Calling? assistant_text: Unknown numbers are handled based on the 'Unknown Number Handling' and 'Calls to On-Premises Extension' settings.\nhuman_text: What is the purpose of dial plans in Webex Calling? assistant_text: Dial plans enable call routing to premises-based call control instances based on dial patterns.\nhuman_text: What are the benefits of using route groups in Webex Calling? assistant_text: Route groups provide redundancy and increased capacity by grouping multiple trunks together.\nhuman_text: What happens if ICE negotiation fails in Webex Calling? assistant_text: If ICE negotiation fails, media is anchored on the Webex Calling Access SBC, resulting in media flowing through the customer\u2019s Internet edge to the SBC and back to the destination endpoint.\nhuman_text: What are route groups in Webex Calling? assistant_text: Route groups provide redundancy or increased capacity by grouping multiple trunks together.\nhuman_text: What is required for Webex Calling endpoints to connect to the datacenters? assistant_text: Webex Calling endpoints use the public Internet to connect to datacenters and establish over-the-top TLS connections.\nhuman_text: What features does the Dedicated Instance option provide in Webex Calling? assistant_text: The Dedicated Instance option provides a Cisco Unified Communications Manager based stack of applications in a private cloud dedicated to a single customer.\nhuman_text: How does Webex Calling handle PSTN access? assistant_text: Webex Calling handles PSTN access through Cisco Calling Plans, Cloud Connected PSTN, and Premises-based PSTN.\nhuman_text: What are the benefits of the Webex Calling global backbone? assistant_text: The global backbone optimizes media round-trip times and ensures high availability with a multi-gigabit, fully redundant network.\nhuman_text: What is the purpose of a Local Gateway in Webex Calling? assistant_text: The Local Gateway connects to the PSTN and registers with Webex Calling to handle all calls between the PSTN and Webex Calling.\nhuman_text: What is the significance of media path optimization in Webex Calling? assistant_text: Media path optimization establishes a direct media path between entities to reduce bandwidth usage and improve call quality.\nhuman_text: What is Webex Edge Connect? assistant_text: Webex Edge Connect peers Webex meetings and Webex Calling traffic with an Equinix Cloud Exchange (ECX) location to improve user experience with guaranteed bandwidth and QoS.\nhuman_text: What is the Webex Calling solution overview? assistant_text: Webex Calling provides enterprise-grade PBX features, functionality, and performance.\nhuman_text: What connectivity options are available for Webex Calling? assistant_text: Connectivity options for Webex Calling include Over-the-top (OTT) Internet, Webex Edge Connect, and Private Network Connect.\nhuman_text: What type of patterns can be included in a Webex Calling dial plan? assistant_text: Dial plans can include numeric patterns and domain patterns for routing SIP URIs.\nhuman_text: What is the role of Local Gateways in Webex Calling? assistant_text: Local Gateways provide PSTN access and connect Webex Calling to existing on-premises call control services.\nhuman_text: How does Webex Calling handle calls between different customers? assistant_text: Calls between different Webex Calling customers are routed through the PSTN.\nhuman_text: What types of phones are supported by Webex Calling? assistant_text: Webex Calling supports all models of Cisco Multiplatform Phones (MPP).\nhuman_text: What is Webex Edge Connect? assistant_text: Webex Edge Connect peers Webex meetings and Webex Calling traffic with an Equinix Cloud Exchange location, improving user experience with guaranteed bandwidth and QoS.\nhuman_text: How can video calls be made in Webex Calling? assistant_text: Video calls in Webex Calling can be made between video capable MPP phones, Webex Devices, and Webex App.\nhuman_text: What is the Dedicated Instance option in Webex Calling? assistant_text: The Dedicated Instance option provides a Cisco Unified Communications Manager based stack of applications in a private cloud dedicated to a single customer.\nhuman_text: How many participants can Webex Meetings support when added to Webex Calling? assistant_text: Webex Meetings can support up to 1000 meeting participants.\n</code></pre> <p>Save the file as example.txt as we will be using it in the next step</p> <p>To finalize, we again convert the above dataset (example.txt) into Llama2 format. Lets look into those steps</p>"},{"location":"Task8a/#convert-dataset-into-llama2-format-and-upload-on-hugging-face","title":"Convert dataset into Llama2 format and upload on Hugging Face","text":"<ul> <li>Open Google Colab and create a new notebook. Click on \"File\" &gt; \"New notebook\" .Go to the \"Secrets\" section in the sidebar and ensure the Hugging Face toggle is enabled. Please refer to the following section to create Google Colab account.</li> </ul> <ul> <li>Make sure you are connected to a runtime. For this task, you can use the CPU as the runtime environment.</li> </ul> <ul> <li>Click on Folder and create a new folder called \"data\"</li> </ul> <ul> <li>Click on [...], select Upload</li> </ul> <ul> <li>Choose your example.txt file and click Open</li> </ul> <p>Note\u00a0 Ensure that your files are saved elsewhere. This runtime's files will be deleted when it is terminated.</p> <p>Summary: So far, we have our raw dataset as example.txt, enabled Hugging Face in our Colab notebook, and uploaded the data into our folder. we will be convertig the dataset (example.txt) into Llama2 format and uploading on Hugging Face so it can be used for our Fine-Tuning in the upcoming steps</p> <p> We will start by installing specific Python packages. </p> <pre><code>!pip install datasets huggingface_hub google-colab\n</code></pre> <p></p> <ul> <li> <p>The ! at the beginning is used in Jupyter notebooks or Google Colab to run shell commands. Using pip install we will install Python packages  </p> </li> <li> <p>datasets: This package is part of the Hugging Face ecosystem and provides tools for working with large datasets. It allows users to easily download, preprocess, and manage datasets, especially those used in machine learning and natural language processing (NLP).</p> </li> <li> <p>huggingface_hub: This package provides tools to interact with the Hugging Face Hub. </p> </li> <li> <p>google-colab: This package includes utilities specifically designed for Google Colab, a  Jupyter notebook environment that runs in the cloud.</p> </li> </ul> <p> Step 1: Import the required modules </p> <pre><code># Import required modules\nfrom datasets import Dataset\nfrom huggingface_hub import login\nimport os\nfrom google.colab import userdata\n</code></pre> <ul> <li> <p>from datasets import Dataset: imports the Dataset class from the datasets library. The Dataset class is used to create and manipulate datasets. This library is part of the Hugging Face ecosystem and is especially useful for handling datasets for machine learning and NLP tasks.</p> </li> <li> <p>from huggingface_hub import login: This imports the login function from the huggingface_hub library. The login function is used to authenticate with the Hugging Face Hub, allowing the user to upload and manage models and datasets on the platform.</p> </li> <li> <p>import os: This imports the os module, which provides a way of using operating system-dependent functionality like reading or writing to the file system, environment variables, and more. </p> </li> <li> <p>from google.colab import userdata: imports the userdata module from the google.colab library. The google.colab library contains utilities specifically designed for use with Google Colab. </p> </li> </ul> <p>Step 2:  Retrieve Hugging Face token from Colab secrets </p> <pre><code># Retrieve Hugging Face token from Colab secrets\nos.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n# Login to Hugging Face\nlogin(token=os.environ[\"HF_TOKEN\"])\n</code></pre> <p></p> <ul> <li> <p>This part of the code retrieves a previously stored Hugging Face token from Colab secrets. This token is essential for authenticating with the Hugging Face platform. Storing tokens in Colab secrets is a secure way to handle sensitive information without hardcoding it in your script.</p> </li> <li> <p>Login to Hugging Face: This line logs into Hugging Face using the retrieved token. Logging in allows our code to interact with the Hugging Face Hub, and allow us to upload datasets and models.</p> </li> </ul> <p> Step 3: Define a fuction that will help us convert our example.txt into Llama2 format </p> <pre><code># Define the generator function\ndef data_generator(file_path):\n    with open(file_path, 'r') as file:\n        for line in file:\n            if \"assistant_text: \" in line:\n                parts = line.split(\"assistant_text: \")\n                human_text = parts[0].replace(\"human_text: \", \"\").strip()\n                assistant_text = parts[1].strip()\n                reformatted_segment = f'&lt;s&gt;[INST] {human_text} [/INST] {assistant_text} &lt;/s&gt;'\n                yield {\"reformatted_segment\": reformatted_segment}\n</code></pre> <ul> <li> <p>def data_generator(file_path): defines a function named data_generator that takes a single argument, file_path.</p> </li> <li> <p>with open(file_path, 'r') as file:   opens the file specified by file_path in read mode ('r'). </p> </li> <li> <p>for line in file: iterates over each line in the file. </p> </li> <li> <p>if \"assistant_text: \" in line: checks if the string \"assistant_text: \" is present in the current line (example.txt) file </p> </li> <li> <p>parts = line.split(\"assistant_text: \"): splits the current line into two parts using \"assistant_text: \" as the delimiter.</p> </li> <li> <p>human_text = parts[0].replace(\"human_text: \", \"\").strip(): takes the first part of the split line (parts[0]), replaces \"human_text: \" with an empty string, and then removes any leading or trailing whitespace using the strip() method.</p> </li> <li> <p>assistant_text = parts[1].strip(): takes the second part of the split line (parts[1]) and removes any leading or trailing whitespace using the strip() method. </p> </li> <li> <p>reformatted_segment =  Creates a formatted string. The human_text and assistant_text variables are inserted into the string at the specified locations, to replicate Llama2 format.</p> </li> <li> <p>yield {\"reformatted_segment\": reformatted_segment}: the yield statement  return a dictionary containing the reformatted_segment. The yield statement makes this function a generator, allowing it to produce a sequence of values over time, rather than returning them all at once.</p> </li> </ul> <p> Step 4: Define the path for the file that we want to convert </p> <pre><code># Path to your data file\nfile_path = '/content/data/example.txt'\n</code></pre> <p> Note: How to get path for your file in Google COlab </p> <ul> <li>Right click on your file (example.txt) and select Copy Path</li> </ul> <p></p> <p> Step 5: Calling data_generator function </p> <pre><code># Create the dataset from the generator\nds = Dataset.from_generator(data_generator, gen_kwargs={\"file_path\": file_path})\n</code></pre> <p>Note: Make sure to add quotations around the \"file_path\" </p> <ul> <li>Creating a Dataset object by passing the file (example.txt) to the data_generator function</li> </ul> <p> Step 6: Verify if our data formatted </p> <pre><code># Access the dataset\nprint(ds[2])\n</code></pre> <p></p> <p> Step 7: Verify values in our varaiable ds </p> <pre><code># Access the dataset\nprint(ds)\n</code></pre> <p></p> <p> Step 8: Consolidates all the data and create new Dataset </p> <pre><code>reformatted_segments_list = []\n# Iterate through the dataset and collect reformatted segments\nfor example in ds:\n    reformatted_segment = example[\"reformatted_segment\"]\n    reformatted_segments_list.append(reformatted_segment)\n# Now you have all reformatted segments in reformatted_segments_list\nprint(\"Total reformatted segments:\", len(reformatted_segments_list))\nprint(\"First reformatted segment example:\", reformatted_segments_list[0])\n# Create a new Dataset object with these reformatted segments\nreformatted_ds = Dataset.from_dict({\"text\": reformatted_segments_list})  # Assuming downstream processes expect 'text'\n</code></pre> <ul> <li> <p>The  above code snippet iterates over the original dataset (ds), extracts the reformatted segments, and collects them into a list (reformatted_segments_list). This process consolidates all the relevant data into a single list for further processing.</p> </li> <li> <p>After collecting all the reformatted segments, we will create a new Dataset object (reformatted_ds) from the list. This new dataset is structured in a way that is required by downstream processes when Fine-Tuning starts.</p> </li> </ul> <p></p> <p> Step 9: Push our new Dataset to Hugging Face Hub so it can be processed and used for Fine-Tuning </p> <pre><code># Push the dataset to the Hub\nreformatted_ds.push_to_hub(\"WebexOne/test\")\n</code></pre> <p></p> <ul> <li> <p>The push_to_hub method is used to upload the dataset reformatted_ds to the Hugging Face Hub. This makes the dataset publicly available (or private, depending on the repository settings) for others to access and use.</p> </li> <li> <p>Repository Naming: The string \"WebexOne/test\" specifies the target repository on the Hugging Face Hub. </p> </li> </ul> <p>Let's login to Hugging Face and view our uploaded model</p> <p></p> <p>Our uploaded Dataset</p> <p></p> <p>Complete Code - FOR REFERENCE ONLY</p> <pre><code>!pip install datasets huggingface_hub google-colab\n# Import required modules\nfrom datasets import Dataset\nfrom huggingface_hub import login\nimport os\nfrom google.colab import userdata\n# Retrieve Hugging Face token from Colab secrets\nos.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n# Login to Hugging Face\nlogin(token=os.environ[\"HF_TOKEN\"])\n# Define the generator function\ndef data_generator(file_path):\n    with open(file_path, 'r') as file:\n        for line in file:\n            if \"assistant_text: \" in line:\n                parts = line.split(\"assistant_text: \")\n                human_text = parts[0].replace(\"human_text: \", \"\").strip()\n                assistant_text = parts[1].strip()\n                reformatted_segment = f'&lt;s&gt;[INST] {human_text} [/INST] {assistant_text} &lt;/s&gt;'\n                yield {\"reformatted_segment\": reformatted_segment}\n# Path to your data file\nfile_path = '/content/data/example.txt'\n# Create the dataset from the generator\nds = Dataset.from_generator(data_generator, gen_kwargs={\"file_path\": file_path})\n\nreformatted_segments_list = []\n# Iterate through the dataset and collect reformatted segments\nfor example in ds:\n    reformatted_segment = example[\"reformatted_segment\"]\n    reformatted_segments_list.append(reformatted_segment)\n\n# Now you have all reformatted segments in reformatted_segments_list\nprint(\"Total reformatted segments:\", len(reformatted_segments_list))\nprint(\"First reformatted segment example:\", reformatted_segments_list[0])\n\n# Create a new Dataset object with these reformatted segments\nreformatted_ds = Dataset.from_dict({\"text\": reformatted_segments_list})  # Assuming downstream processes expect 'text'\n\n# Save the dataset locally - OPTIONAL STEP\nreformatted_ds.save_to_disk('/content/data/reformatted_dataset')\n\n# Push the dataset to the Hub\nreformatted_ds.push_to_hub(\"WebexOne/test\")\n</code></pre>"},{"location":"Task8a/#fine-tuning-llama2-7b-model","title":"Fine-tuning Llama2-7B Model","text":"<p> NOTE: If you\u2019re serious about fine-tuning models, using a script instead of a notebook is recommended. You can easily rent GPUs on Lambda Labs, Runpod, Vast.ai e.t.c </p>"},{"location":"Task8a/#background-on-fine-tuning","title":"Background on Fine-Tuning","text":"<p>Language models (LLMs) are pretrained on extensive text corpus. For example, Llama 2 was trained with 2 trillion tokens. As mentioned before pretraining is resource-intensive and often faces hardware challenges.</p> <p>After pretraining, models like Llama 2, can predict the next word in a sequence but don't naturally follow instructions. To make them better assistants, we use instruction tuning, which involves two primary methods:</p> <ul> <li> <p>Supervised Fine-Tuning (SFT): Models are trained on datasets of instructions and responses to minimize the difference between their answers and the correct ones.</p> </li> <li> <p>Reinforcement Learning from Human Feedback (RLHF): Models learn through interaction and feedback to optimize a reward signal based on human evaluations.</p> </li> </ul> <p>RLHF can capture nuanced human preferences but is complex to implement, requiring a well-designed reward system and consistent feedback. Not a focus for this lab</p> <p>In this lab, we will use Supervised Fine-Tuning (SFT). Fine-tuning works well because it builds on the knowledge gained during pretraining. If a model has seen similar data, fine-tuning can significantly enhance performance. For instance, fine-tuning a LLaMA model with 65 billion parameters on 1,000 high-quality samples can outperform larger models like GPT-3.</p> <p>Note: As mentioned earlier, the choice of prompt template is crucial for fine-tuning. In our dataset section, we have converted our data into the following template:</p> <p><code>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt; {{ system_prompt }} &lt;&lt;/SYS&gt;&gt; {{ user_message }} [/INST] Model answer &lt;/s&gt;</code></p> <p>Note: For this tutorial, we will use a preformatted dataset (WebexOne/test) that was uploaded earlier on Hugging Face. We will apply Supervised Fine-Tuning (SFT) to a base model.</p>"},{"location":"Task8a/#guide-to-fine-tuning-llama-2","title":"Guide to Fine-Tuning Llama 2","text":"<p>In this section, we will learn about all the steps required to fine-tune the Llama 2 model with 7 billion parameters on a T4 GPU with high RAM using Google Colab.</p> <p>The Colab T4 GPU has a limited 16 GB of VRAM. That is barely enough to store Llama 2\u20137b's weights (7b \u00d7 2 bytes = 14 GB in FP16), which means full fine-tuning is not possible, and we need to use parameter-efficient fine-tuning techniques like LoRA and QLoRA.</p> <p>We will use the QLoRA technique to fine-tune the model in 4-bit precision and optimize VRAM usage. For that, we will use the Hugging Face ecosystem of LLM libraries:   transformers, accelerate, peft, trl, and bitsandbytes.</p>"},{"location":"Task8a/#base-models-for-fine-tuning","title":"Base models for Fine-Tuning","text":"<ul> <li>We have the option to fine-tune the Llama2 Base model (meta-llama/Llama-2-7b-hf), which can be found in the Hugging Face repository</li> </ul> <ul> <li>Since it is a gated repository, you'll need to provide the required information to access and use the model. Please scroll down, fill in the necessary details, and submit the form.</li> </ul> <ul> <li>Once submitted, you can check the status of your request by navigating to Settings and clicking on Gated Repositories.</li> </ul> <p> NOTE: It may take some time for the status to update from Pending to Accepted.</p> <p> NOTE: In this lab environment, we can use the Llama2 base model from another repository that is not gated.  More Info at:  Repository</p> <ul> <li>Lets continue</li> </ul>"},{"location":"Task8a/#logging-into-google-collab","title":"Logging into Google Collab","text":"<ul> <li>You can either use the existing notebook from above and add a new code cell to start fine-tuning, or create a new Jupyter Notebook.</li> </ul> <ul> <li>Change Runtime Environment: Click the \u201cRuntime\u201d dropdown menu at the top of the Colab interface.</li> </ul> <ul> <li> <p>Select \u201cChange runtime type\u201d: This will open a dialog box where you can configure the runtime environment.</p> </li> <li> <p>Select Hardware Accelerator: From the \u201cHardware accelerator\u201d dropdown menu, choose &gt;&gt; T4 GPU and enable toggle for High RAM</p> </li> </ul> <p></p> <ul> <li>Save Settings: Click \u201cSave\u201d to apply the changes.</li> </ul> <p>Reminder: Whenever you want to copy the code in Google Colab and run it, be sure to click on + Code to add a new code cell.</p> <p></p> <p>Reminder: Click the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\" while the cell is selected.</p> <p></p>"},{"location":"Task8a/#set-up-envoirnment","title":"Set up Envoirnment","text":"<p>Step 1: Check the status of Nvidia chipset. OPTIONAL STEP </p> <pre><code>!nvidia-smi \n</code></pre> <p>Note: Remember to execute each cell individually </p> <p></p> <p>Note:  nvidia-smi stands for NVIDIA System Management Interface. It is a command-line utility that provides information about NVIDIA GPUs installed on the system. This tool is part of the NVIDIA GPU driver package. By using !nvidia-smi, you can quickly check the status and health of your NVIDIA GPUs from within a Jupyter Notebook, making it a useful tool for machine learning and data science workflows.</p> <p>Step 2: Install the required libraries. </p> <pre><code>!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7\n</code></pre> <p>Note: Remember to execute each cell individually </p> <p></p> <ul> <li> <p>-q: flag stands for \"quiet\" mode, which minimizes the output during the installation process. OPTIONAL FLAG</p> </li> <li> <p>accelerate==0.21.0: accelerate is a library from Hugging Face for easily running models on different devices (e.g., CPU, GPU). </p> </li> <li> <p>peft stands for Parameter-Efficient Fine-Tuning. It is a library that provides tools for fine-tuning large models efficiently. </p> </li> <li> <p>bitsandbytes==0.40.2: is a library for efficient model quantization </p> </li> <li> <p>transformers==4.31.0: library by Hugging Face that provides pre-trained models and tools for natural language processing (NLP) tasks.</p> </li> <li> <p>trl==0.4.7:  Transformer Reinforcement Learning. It is a library for applying reinforcement learning techniques to transformer models. </p> </li> </ul> <p>Step 3: Load the necessary modules </p> <pre><code>import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer\n</code></pre> <p>Note: Remember to execute each cell individually </p> <ul> <li> <p>datasets:  library, is part of the Hugging Face ecosystem, that provides tools to load and process datasets.</p> </li> <li> <p>transformers: Hugging Face Transformers library, which provides tools for working with transformer-based models.</p> </li> <li> <p>AutoModelForCausalLM: automatically selects the appropriate model architecture for causal language modeling (e.g., Llama or GPT based models).</p> </li> <li> <p>AutoTokenizer: A class that automatically selects the appropriate tokenizer for a given model.</p> </li> <li> <p>BitsAndBytesConfig: Configuration class for model quantization</p> </li> <li> <p>HfArgumentParser: A helper class for parsing command-line arguments, designed for Hugging Face libraries.</p> </li> <li> <p>TrainingArguments: A class that defines the training configuration, such as learning rate, batch size, number of epochs, etc.</p> </li> <li> <p>pipeline: The pipelines are a great and easy way to use models for inference. More info</p> </li> <li> <p>peft: Stands for Parameter-Efficient Fine-Tuning (PEFT), which includes methods like LoRA (Low-Rank Adaptation) and Qlora</p> </li> <li> <p>trl: Stands for Transformer Reinforcement Learning, a library that includes tools for fine-tuning and training models</p> </li> </ul> <p>Step 4: Retrieve Hugging Face token and set as an environment variable. </p> <p><pre><code>from google.colab import userdata\nos.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n</code></pre> Note: Remember to execute each cell individually </p> <pre><code>!huggingface-cli whoami\n</code></pre> <p>Note: It will output your custom HuggingFace org </p> <p></p> <ul> <li>Verify to confirm that the Hugging Face CLI is correctly authenticated with your account.</li> </ul>"},{"location":"Task8a/#load-and-prepare-dataset-and-model","title":"Load and prepare dataset and model","text":"<p>Step 5: Before we start processing the data we prepared earlier, we need to load the model, tokenizer and datasets.  </p> <pre><code># model_name = \"meta-llama/Llama-2-7b-hf\" as its a gated repo\nmodel_name = \"NousResearch/Llama-2-7b-chat-hf\"\nnew_model = \"WebexOneDemo-Llama-2-7b-chat-finetune\"\n</code></pre> <p>Note: You can name the new_model variable whatever you prefer. </p> <ul> <li>We can use the gated repo or the one mentioned earlier</li> </ul> <pre><code># The instruction dataset to use that we created earlier \ndataset_name = \"WebexOne/test\"\n\n# Load dataset (you can process it here)\ndataset = load_dataset(dataset_name, split=\"train\")\n</code></pre> <pre><code>print(dataset)\n</code></pre> <p></p> <pre><code>print(dataset['text'][2])\n</code></pre> <ul> <li> <p>shows the total number of rows in our dataset</p> </li> <li> <p>Loading the previously created dataset from the Hugging Face Hub</p> </li> </ul>"},{"location":"Task8a/#training-parameters-and-4-bit-quantization-parameters-qlora","title":"Training parameters and 4 bit Quantization Parameters (Qlora)","text":"<p>Step 6: Define hyperparameters for training a machine learning model using QLoRA (Quantized Low-Rank Adaptation) and bitsandbytes, along with the TrainingArguments for the training process. In the below we will use QLoRA with a rank of 64 and a scaling parameter of 16. We\u2019ll load the Llama 2 model directly in 4-bit using the NF4 type and train it for 10 epoch. </p> <pre><code>################################################################################\n# QLoRA parameters - That we will use\n################################################################################\n\n# LoRA attention dimension . A higher value increases the model's capacity to learn but also its computational cost. The higher the rank the more parameters you train and the bigger your adapter files will be.\nlora_r = 64\n\n# Alpha parameter for LoRA scaling. It controls the trade-off between model capacity and stability.\nlora_alpha = 16\n\n# Dropout probability for LoRA layers. The dropout probability applied to the LoRA layers to prevent overfitting.\nlora_dropout = 0.1\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model. Enables loading the base model with 4-bit precision, which reduces memory usage.\nuse_4bit = True\n\n# Compute dtype for 4-bit base models. Specifies the data type for computations in 4-bit precision models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4) we\u2019ll load the Llama 2 model directly in 4-bit precision using the NF4 type\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate or Deactivate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Number of training epochs. Epochs efers to one complete pass through the entire training dataset. During an epoch, the model processes each example in the training set once and updates its weights accordingly\n# In machine learning, particularly in training neural networks, the term \"number of training epochs\" refers to the number of times the entire training dataset is passed forward and backward through the neural network. Each pass through the entire dataset is counted as one epoch.\n\nnum_train_epochs = 10\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of steps to accumulate gradients before updating model weights.\ngradient_accumulation_steps = 1\n\n#  Enables saving memory during training by checkpointing gradients.\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule (constant a bit better than cosine)\nlr_scheduler_type = \"constant\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 25\n\n# Log every X updates steps\nlogging_steps = 25\n\n################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}\n</code></pre>"},{"location":"Task8a/#load-tokenizer","title":"Load tokenizer","text":"<p>Step 7: Load Tokenizer </p> <pre><code>compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major &gt;= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n\n# Set the padding token and EOS token to the same value\n# tokenizer.pad_token = \"&lt;/s&gt;\"\n# tokenizer.add_special_tokens({'eos_token': '&lt;/s&gt;'})\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n</code></pre> <p></p>"},{"location":"Task8a/#load-lora-configuration","title":"Load LoRA configuration","text":"<p>Step 8: Load Lora Config </p> <pre><code>peft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n</code></pre>"},{"location":"Task8a/#setting-peft-parameters","title":"Setting Peft Parameters","text":"<p>Step 9: Setting Peft Config </p> <pre><code># Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n</code></pre> <ul> <li>Supervised fine-tuning (SFT) is a key step in reinforcement learning. We will provide SFT Trainer the model, dataset, Lora configuration, tokenizer, and training parameters.</li> </ul> <p>Step 10: We will use .train() to fine-tune the Llama 2 model on our new dataset.</p> <pre><code>trainer.train()\n</code></pre> <p></p> <p>Note: Given our minimal dataset, the training process may take up to 10 minutes. However, with larger datasets, the training duration can range from an hour to several hours, depending on the size and complexity of the data. </p> <p>Step 11: After training the model, we will save the model adopter and tokenizers.</p> <pre><code>trainer.model.save_pretrained(new_model)\ntrainer.tokenizer.save_pretrained(new_model)\n</code></pre> <p></p> <p></p> <p>Step 12: To ensure our model is functioning correctly, let's ask a question from our dataset. While a more comprehensive evaluation is necessary for production, we can perform a basic check in our lab using the text generation pipeline. For instance, we can ask, \"What capabilities does the Webex App offer when integrated with Webex Calling?\" Note that the input is formatted to match Llama 2's prompt template. </p> <ul> <li>Our dataset on HF</li> </ul> <p></p> <pre><code># Run text generation pipeline with our next model\nprompt = \"What are the considerations for video calls in Webex Calling?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"&lt;s&gt;[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])\n</code></pre> <p></p>"},{"location":"Task8a/#merge-lora-adapters-with-base-model-and-saving-our-model","title":"Merge LoRA adapters with base model and saving our model","text":"<p> Step 13:  Merge LoRA adapters </p> <ul> <li>To store our newly fine-tuned  WebexOneDemo-Llama-2-7b-chat-finetune  model, we need to merge the LoRA weights with the base model. This involves reloading the base model with FP16 precision and using the PEFT library to combine all components.</li> </ul> <pre><code>base_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n# tokenizer.pad_token = \"&lt;/s&gt;\"\n# tokenizer.add_special_tokens({'eos_token': '&lt;/s&gt;'})\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n</code></pre> <p> Optional STEP: if you run out of GPU memory - Create a function to clear GPU cache on google colab </p> <pre><code># Function to clear GPU cache\ndef clear_gpu_cache():\n    torch.cuda.empty_cache()\n\nclear_gpu_cache()\n</code></pre> <p> Step 14:  Push our model and tokenizer to Hugging Face</p> <ul> <li>We can now push everything to the Hugging Face Hub to save our model.</li> </ul> <pre><code>model.push_to_hub(new_model, use_temp_dir=False)\n# tokenizer.push_to_hub(new_model, use_temp_dir=False) # Get push automatically\n</code></pre> <p>Or we can push it to the org that we created earlier in my case I called it - WebexOne</p> <pre><code>org_name = \"WebexOne\" \nmodel.push_to_hub(f\"{org_name}/{new_model}\", use_temp_dir=False)\ntokenizer.push_to_hub(f\"{org_name}/{new_model}\", use_temp_dir=False)\n</code></pre> <p></p> <p></p>"},{"location":"Task8a/#load-the-pretrained-model-from-hugging-face-and-use-for-inference","title":"Load the  pretrained model from Hugging Face and use for inference","text":"<ul> <li>The Model Hub simplifies the process of selecting the appropriate model. Since we've just uploaded the model to HuggingFace, there may be a slight delay before it becomes accessible. In the meantime, you can use the model I\u2019ve already uploaded, WebexOne/WebexOneDemo-Llama-2-7b-chat-finetune for inference. You can load it like any other Llama 2 model from the Hub. Let's dive into how to use this pre-uploaded model due to time constraints.</li> </ul> <p> Step 15:  login into Hugging Face hub </p> <p></p> <ul> <li>We select the WebexOne/WebexOneDemo-Llama-2-7b-chat-finetune model. Click Use this model.</li> </ul> <p></p> <p>Note: The inference api requires that the model repo have a certain amount of user activity before it can be used directly on api-inference or \"Use the model\" tab appears. If you having issues please speak with the lab proctor</p> <ul> <li>Lets click on Transformer so we can instantiate it using the pipeline() function:</li> </ul> <p></p> <p> Step 16: Copy the code for transformers </p> <pre><code>from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n# Load the model and tokenizer\nmodel_name = \"WebexOne/WebexOneDemo-Llama-2-7b-chat-finetune\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n</code></pre> <p></p> <p>The process involves downloading the necessary files for the model and tokenizer from the Hub to ensure the model and tokenizer work correctly.  Followed by loading them into memory so they can be used for generating text. When downloading a model, the following components are typically fetched:</p> <ul> <li> <p>Model Configuration: Defines the architecture and hyperparameters (config.json).</p> </li> <li> <p>Weight Shards: Large binary files containing the model's trained parameters (pytorch_model-00001-of-00002.bin, pytorch_model-00002-of-00002.bin).</p> </li> <li> <p>Index Files: Help manage and load weight shards (pytorch_model.bin.index.json).</p> </li> <li> <p>Generation Configuration: Specifies settings for text generation tasks (generation_config.json).</p> </li> <li> <p>Tokenizer Files: Include configuration, the tokenizer model, and mappings for converting text to tokens (tokenizer_config.json, tokenizer.model, tokenizer.json).</p> </li> <li> <p>Special Tokens: Ensures special tokens are correctly handled (added_tokens.json, special_tokens_map.json).</p> </li> </ul> <p> Step 17: Instantiate using the pipeline() function </p> <p><pre><code># Set up the text generation pipeline\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_length=200,\n    pad_token_id=tokenizer.eos_token_id  # Ensure the padding token is correctly set\n)\n</code></pre>  Step 18: Example 1: </p> <p> Note :  Lets take one question from our dataset.</p> <pre><code># Define the prompt\nprompt = \"What is the Private Network Connect (PNC) solution?\"\n\n# Run the text generation pipeline with the prompt\nresult = pipe(f\"&lt;s&gt;[INST] {prompt} [/INST]\")\n\n# Print the generated text\nprint(result[0]['generated_text'])\n</code></pre> <p> Output: </p> <p></p> <p> Step 19: Example 2: </p> <p> Note :  Lets take one question from our dataset.</p> <pre><code># Define the prompt\nprompt = \"What happens if ICE negotiation fails in Webex Calling?\"\n\n# Run the text generation pipeline with the prompt\nresult = pipe(f\"&lt;s&gt;[INST] {prompt} [/INST]\")\n\n# Print the generated text\nprint(result[0]['generated_text'])\n</code></pre> <p> Output: </p> <p></p>"},{"location":"Task8a/#fine-tuning-example-2","title":"Fine-Tuning - Example 2","text":"<p>In this example, I will show you how to fine-tune using the Cisco IP DECT Phone 6800 Series Deployment Guide . I have already created the dataset from the PDF, which can be found at WebexOne/dect1</p> <p> Step 1: Import the Necessary Libraries </p> <pre><code>import os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer\nfrom dotenv import load_dotenv\n</code></pre> <p> Step 2: Load your HuggingFace tokens </p> <pre><code>from google.colab import userdata\nos.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n</code></pre> <p>Note: Optional Step - If using Jupyter notebook on your machine. Create a .env folder, add your HuggingFace token. Then, import the token.  Optional STEP<pre><code>load_dotenv()\nhf_token = os.getenv(\"HF_TOKEN\")\n</code></pre> </p> <p> Step 3: Load your Datasets and models </p> <pre><code># The instruction dataset to use that we created earlier\ndataset_name = \"WebexOne/dect1\"\n# Load dataset (you can process it here)\ndataset = load_dataset(dataset_name, split=\"train\")\n# model_name = \"meta-llama/Llama-2-7b-hf\" as its a gated repo\nmodel_name = \"NousResearch/Llama-2-7b-chat-hf\"\nnew_model = \"dect-phone-Llama-2-7b-Finale\"\n</code></pre> <p>Note: You can name the new_model variable whatever you prefer. </p> <p> Step 4: Load pre-trained model with 4-bit quantization</p> <pre><code>###############################################################################\n# QLoRA parameters - That we will use\n################################################################################\n\n# LoRA attention dimension . A higher value increases the model's capacity to learn but also its computational cost. The higher the rank the more parameters you train and the bigger your adapter files will be.\nlora_r = 64\n\n# Alpha parameter for LoRA scaling. It controls the trade-off between model capacity and stability.\nlora_alpha = 16\n\n# Dropout probability for LoRA layers. The dropout probability applied to the LoRA layers to prevent overfitting.\nlora_dropout = 0.1\n\n################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model. Enables loading the base model with 4-bit precision, which reduces memory usage.\nuse_4bit = True\n\n# Compute dtype for 4-bit base models. Specifies the data type for computations in 4-bit precision models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4) we\u2019ll load the Llama 2 model directly in 4-bit precision using the NF4 type\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate or Deactivate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False\n\n################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./dectresults1\"\n\n# Number of training epochs. Epochs efers to one complete pass through the entire training dataset. During an epoch, the model processes each example in the training set once and updates its weights accordingly\nnum_train_epochs = 10\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of steps to accumulate gradients before updating model weights.\ngradient_accumulation_steps = 1\n\n#  Enables saving memory during training by checkpointing gradients.\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule (constant a bit better than cosine)\nlr_scheduler_type = \"constant\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 25\n\n# Log every X updates steps\nlogging_steps = 25\n\n################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = {\"\": 0}\n</code></pre> <p> Step 5: Configure PEFT and Training Parameters</p> <pre><code>peft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)\n</code></pre> <p> Step 6: Train the Model Using the Trainer API and Save the model</p> <pre><code>trainer.train()\ntrainer.model.save_pretrained(new_model)\n</code></pre> <p> Testing the model Locally</p> <pre><code># Run text generation pipeline with our next model\nprompt = \"What is a key consideration when planning the DECT system for different regions?\"\nsys1 = \"You are an interviewer AI tasked with asking questions about a deployment guide for the Cisco IP DECT Phone 6800 Series. Your goal is to elicit detailed responses from the text as if the pdf itself were answering. Be polite and address the user's query directly. Aim to offer clear and accurate answers. If you dont find the answer in pdf just say no info available at this time please contact your Cisco TME\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt; {sys1} &lt;&lt;/SYS&gt;&gt; {prompt} [/INST]\")\nprint(result[0]['generated_text'])\n</code></pre> <p> Step 7: Push the Model to Hub <pre><code>model.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)\n</code></pre></p> <p> Step 8: Verify the Model: </p> <ul> <li>Log in to the Hugging Face Hub and navigate to your model's page.</li> <li>Click on \"Use this model\" and then \"Transformers\".</li> <li>Copy the provided code snippet for using the model.</li> </ul> <p></p> <p> Step 9: Configure Inferencing </p> <pre><code># Load model directly\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\ntokenizer = AutoTokenizer.from_pretrained(\"compile2011/dect-phone-Llama-2-7b-Finale\")\nmodel = AutoModelForCausalLM.from_pretrained(\"compile2011/dect-phone-Llama-2-7b-Finale\")\nfrom transformers import pipeline\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_length=200,\n    pad_token_id=tokenizer.eos_token_id  # Ensure the padding token is correctly set\n)\n</code></pre> <p> Step 10: Inferencing with downloaded Model to test </p> <pre><code># Run text generation pipeline with our next model\nprompt = \"What is a key consideration when planning the DECT system for different regions?\"\nsys1 = \"You are an interviewer AI tasked with asking questions about a deployment guide for the Cisco IP DECT Phone 6800 Series. Your goal is to elicit detailed responses from the text as if the pdf itself were answering. Be polite and address the user's query directly. Aim to offer clear and accurate answers. If you dont find the answer in pdf just say no info available at this time please contact your Cisco TME\"\n# pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt; {sys1} &lt;&lt;/SYS&gt;&gt; {prompt} [/INST]\")\nprint(result[0]['generated_text'])\n</code></pre> <p>  OUTPUT  <pre><code>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt; You are an interviewer AI tasked with asking questions about a deployment guide for the Cisco IP DECT Phone 6800 Series. Your goal is to elicit detailed responses from the text as if the pdf itself were answering. Be polite and address the user's query directly. Aim to offer clear and accurate answers. If you dont find the answer in pdf just say no info available at this time please contact your Cisco TME &lt;&lt;/SYS&gt;&gt; What is a key consideration when planning the DECT system for different regions? [/INST]\nConsider the regulatory requirements for DECT frequencies in different regions. Cisco offers units set up correctly for each region, such as 1880-1900 MHz for Australia and New Zealand, 1880-1900 MHz for E.U. and APAC. \n</code></pre></p>"},{"location":"Task8a/#troubleshooting-best-practices","title":"Troubleshooting - Best Practices","text":"<ul> <li>Challenges are an inherent part of model training. Let's discuss some common issues and their resolutions.</li> </ul>"},{"location":"Task8a/#out-of-memory-oom-errors","title":"Out of Memory (OOM) Errors","text":"<ul> <li> <p>If you encounter an Out of Memory (OOM) error:</p> <ul> <li>reduce Batch Size: Lowering the batch size can help fit the model into memory.</li> <li>Shorten Training Samples: Decrease the context length (e.g., max_length in tokenize()).</li> </ul> </li> </ul>"},{"location":"Task8a/#slow-training","title":"Slow Training","text":"<ul> <li> <p>If training seems sluggish:</p> <ul> <li> <p>Increase Batch Size: A larger batch size can speed up training.</p> </li> <li> <p>Use Multiple GPUs: Consider using multiple GPUs, either by purchasing or renting (e.g., on platforms like Runpod). The provided code is compatible with accelerate for multi-GPU settings. Simply launch it with  accelerate launch your_file.py  instead of  python your_file.py </p> </li> </ul> </li> </ul>"},{"location":"Task8a/#poor-model-quality","title":"Poor Model Quality","text":"<p>The quality of your model reflects the quality of your dataset. To improve model quality ensure your dataset is rich and relevant.</p>"},{"location":"Task8a/#metadata-info-on-huggingface-optional","title":"Metadata - Info on HuggingFace - Optional","text":"<pre><code>---\ninference: true\nlanguage:\n- en\npipeline_tag: text-generation\ntags:\n- facebook\n- meta\n- pytorch\n- llama\n- llama-2\nlicense: llama2\nlibrary_name: transformers\n---\n</code></pre>"},{"location":"Task8a/#conclusions","title":"Conclusions","text":"<p>So, we have explored the process of fine-tuning our model and performing inference with it. Now, let\u2019s say you have successfully trained or fine-tuned your model and are ready to deploy it for widespread use. However, you notice that the latency is too slow, and you want to speed up the model. There are four key methods you can employ to enhance your model's performance. While these steps are not covered in this lab, they serve as a good starting point for further investigation:</p> <ul> <li>Quantization</li> <li>Pruning</li> <li>Model or Knowledge Distillation</li> <li>Engineering Optimizations</li> </ul>"},{"location":"Task8b/","title":"Task 8b - Configuring and Fine Tuning - Using llama3","text":""},{"location":"Task8b/#introduction","title":"Introduction","text":"<p>This diagram represents the workflow of training and using a LLM. </p> <p>Note: This diagram is referenced from Task8a</p>"},{"location":"Task8b/#pre-requisites","title":"Pre-requisites","text":"<p>Note: Referenced from Task8a</p>"},{"location":"Task8b/#usecase-1-fine-tuning-using-llama2","title":"UseCase 1 - Fine Tuning Using Llama2","text":""},{"location":"Task8b/#datasets-creation","title":"Datasets Creation","text":"<p>As we will be using and fine tuning Llama3 base model we need to convert datasets into a uniform format compatible with Llama3. More Info at</p>"},{"location":"Task8b/#example-format-for-llama3-instruct","title":"Example Format for Llama3 - Instruct","text":"Sample Format - Dont Copy - reference ONLY<pre><code>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt; You are a helpful AI assistant for  Collaboration&lt;|eot_id|&gt;\n\n                 &lt;|start_header_id|&gt;user&lt;|end_header_id|&gt; What can you help me with?&lt;|eot_id|&gt;\n\n                 &lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n</code></pre> <ul> <li>&lt;|begin_of_text|&gt;: Specifies the start of the prompt</li> <li>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;: Specifies the role for the following message, i.e. \u201csystem\u201d You are a helpful AI assistant for Collaboration</li> <li>&lt;|eot_id|&gt;: Specifies the end of the input message</li> <li>&lt;|start_header_id|&gt;user&lt;\\end_header_id|&gt;: Specifies the role for the following message i.e. \u201cuser\u201d What can you help me with?: The user message</li> <li>&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;: Ends with the assistant header, to prompt the model to start generation.</li> </ul>"},{"location":"Task8b/#datasets-creation_1","title":"Datasets Creation","text":"<p>The next step is to prepare a dataset for fine-tuning. If you already have a clean, high-quality dataset, that's great. However, let's assume you don't have one readily available.</p> <p>In my scenario, I was able to generate synthetic dataset from the Cisco Preferred Architecture for Webex Calling.</p>"},{"location":"Task8b/#creating-a-dataset-from-a-pdf-summary","title":"Creating a Dataset from a PDF - Summary","text":"<p>To delve into the intricacies of dataset creation, let's explore a scenario where we aim to train an AI to replicate our preferred architecture. In this lab, we will transform a PDF document into a dataset. Specifically, we will use a PDF about Webex Calling as our source material.</p> <p>Steps Involved</p> <p>Converting the PDF to Text</p> <ul> <li>Begin by extracting the text content from the PDF document.</li> </ul> <p>Paragraph Analysis and Segmentation:</p> <ul> <li>Once the PDF is in text form, analyze and segment it into individual paragraphs. Combine short paragraphs and split longer ones to ensure each segment is coherent and self-contained, while still contributing to the overall narrative.</li> </ul> <p>Generating Interview Questions:</p> <ul> <li>For each paragraph, create an artificial interview scenario where a language model (LLM) acts as the interviewer.</li> <li>Develop questions that naturally elicit responses based on the content of the paragraph.</li> <li>The objective is to simulate an insightful dialogue, making it seem as though the PDF is responding to questions about its content and experiences.</li> </ul> <p>By following these steps, we will convert the static information in the PDF into a dynamic and interactive dataset, facilitating a deeper understanding and more engaging learning experience.</p> <p>Here's an example of the resulting file. Please note that for this lab demonstration, only a few data points were converted from the PDF.</p> <p>Note: We will use the following file to create and eventually train downstream tasks for our LLM, specifically the Llama3 model.</p>"},{"location":"Task8b/#sample-file","title":"Sample File","text":"example_llama3.json<pre><code>[\n    {\n        \"question\":\"What is the solution provided by Webex Calling?\",\n        \"context\":\"Webex Calling is part of an integrated, intelligent, and modular team collaboration suite. It provides enterprise-grade PBX features, functionality, and performance previously only possible with an on-premises PBX network.\",\n        \"answer\":\"Webex Calling provides enterprise-grade PBX features, functionality, and performance.\"\n    },\n    {\n        \"question\":\"What options are available for PSTN access in Webex Calling?\",\n        \"context\":\"Webex Calling can access the PSTN in three ways: Cisco Calling Plans, Cloud Connected PSTN, and Premises-based PSTN.\",\n        \"answer\":\"The options are Cisco Calling Plans, Cloud Connected PSTN, and Premises-based PSTN.\"\n    },\n    {\n        \"question\":\"What type of devices does Webex Calling support?\",\n        \"context\":\"Webex Calling supports a wide range of Cisco MPP Series IP Phones (6800, 7800, and 8800 Series), Webex Devices, and a variety of third-party devices.\",\n        \"answer\":\"Webex Calling supports Cisco MPP Series IP Phones, Webex Devices, and a variety of third-party devices.\"\n    },\n    {\n        \"question\":\"How are Webex Calling data centers distributed?\",\n        \"context\":\"Webex Calling data centers are globally distributed and geo-redundant. The data centers are located in six regions: US, Canada, Europe, UK, Australia, and Japan.\",\n        \"answer\":\"Webex Calling data centers are globally distributed and geo-redundant, located in six regions: US, Canada, Europe, UK, Australia, and Japan.\"\n    },\n    {\n        \"question\":\"What is the function of the Local Gateway in Webex Calling?\",\n        \"context\":\"A Local Gateway connects to the PSTN either directly by terminating a PSTN trunk or by connecting to an existing PSTN gateway via a SIP trunk. It also registers with Webex Calling and handles all calls between the PSTN and Webex Calling.\",\n        \"answer\":\"The Local Gateway connects to the PSTN and registers with Webex Calling to handle all calls between the PSTN and Webex Calling.\"\n    },\n    {\n        \"question\":\"What are the key benefits of Webex Calling for large enterprises?\",\n        \"context\":\"Webex Calling provides large enterprises with scalability, advanced call management features, and integration with other collaboration tools to streamline communication.\",\n        \"answer\":\"Webex Calling offers scalability, advanced call management, and integration with collaboration tools for large enterprises.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle call encryption?\",\n        \"context\":\"Webex Calling uses SRTP for media encryption and TLS for signaling encryption to ensure secure communications.\",\n        \"answer\":\"SRTP and TLS are used in Webex Calling to encrypt media and signaling for secure communications.\"\n    },\n    {\n        \"question\":\"What features are included in the Webex Calling analytics suite?\",\n        \"context\":\"Webex Calling offers analytics on call quality, usage patterns, and performance metrics to help optimize communication systems.\",\n        \"answer\":\"Call quality, usage patterns, and performance metrics are part of the Webex Calling analytics suite.\"\n    },\n    {\n        \"question\":\"How does Webex Calling integrate with third-party devices?\",\n        \"context\":\"Webex Calling supports integration with third-party SIP phones, enabling flexibility in device choices.\",\n        \"answer\":\"Integration with third-party SIP phones provides flexibility in device choices for Webex Calling users.\"\n    },\n    {\n        \"question\":\"What disaster recovery mechanisms are available in Webex Calling?\",\n        \"context\":\"Webex Calling ensures disaster recovery through geo-redundant data centers and automatic failover mechanisms.\",\n        \"answer\":\"Geo-redundant data centers and automatic failover mechanisms support disaster recovery in Webex Calling.\"\n    },\n    {\n        \"question\":\"How does Webex Calling manage international call routing?\",\n        \"context\":\"International call routing in Webex Calling is managed through a global network of data centers to ensure high-quality communication.\",\n        \"answer\":\"A global network of data centers manages international call routing in Webex Calling.\"\n    },\n    {\n        \"question\":\"What role does the Webex Control Hub play in Webex Calling?\",\n        \"context\":\"The Webex Control Hub is used for provisioning, configuring, and managing Webex Calling services from a centralized interface.\",\n        \"answer\":\"Provisioning, configuring, and managing Webex Calling services are handled through the Webex Control Hub.\"\n    },\n    {\n        \"question\":\"What are the main security protocols used in Webex Calling?\",\n        \"context\":\"Webex Calling uses SRTP for media encryption and TLS for signaling encryption to protect communications.\",\n        \"answer\":\"SRTP and TLS are the main security protocols used in Webex Calling.\"\n    },\n    {\n        \"question\":\"How does Webex Calling support mobile users?\",\n        \"context\":\"Webex Calling supports mobile users through the Webex App, which allows calls to be made and received on smartphones and tablets.\",\n        \"answer\":\"The Webex App enables mobile users to make and receive calls on smartphones and tablets in Webex Calling.\"\n    },\n    {\n        \"question\":\"What are the benefits of Webex Calling for remote teams?\",\n        \"context\":\"Webex Calling provides remote teams with enterprise-grade calling features, mobility, and integration with collaboration tools.\",\n        \"answer\":\"Remote teams benefit from enterprise-grade calling features, mobility, and collaboration tool integration in Webex Calling.\"\n    },\n    {\n        \"question\":\"How does Webex Calling ensure compliance with regulatory requirements?\",\n        \"context\":\"Webex Calling includes features such as call recording, monitoring, and secure data handling to ensure compliance with regulatory requirements.\",\n        \"answer\":\"Compliance with regulatory requirements in Webex Calling is ensured through call recording, monitoring, and secure data handling.\"\n    },\n    {\n        \"question\":\"What redundancy features are built into Webex Calling?\",\n        \"context\":\"Webex Calling includes redundancy features like geo-redundant data centers and automatic failover to ensure continuous service.\",\n        \"answer\":\"Geo-redundant data centers and automatic failover ensure continuous service in Webex Calling.\"\n    },\n    {\n        \"question\":\"What user management capabilities are available in Webex Calling?\",\n        \"context\":\"Webex Calling provides user management capabilities through the Webex Control Hub, allowing administrators to configure user settings and permissions.\",\n        \"answer\":\"User settings and permissions are managed through the Webex Control Hub in Webex Calling.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle call routing?\",\n        \"context\":\"Call routing in Webex Calling is managed through dial plans and routing rules that can be customized to meet business needs.\",\n        \"answer\":\"Dial plans and customizable routing rules manage call routing in Webex Calling.\"\n    },\n    {\n        \"question\":\"What kind of customer support does Cisco provide for Webex Calling?\",\n        \"context\":\"Cisco provides comprehensive customer support for Webex Calling, including technical assistance, training, and resources for deployment and management.\",\n        \"answer\":\"Technical assistance, training, and deployment resources are part of Cisco's support for Webex Calling.\"\n    },\n    {\n        \"question\":\"How does Webex Calling support hybrid deployments?\",\n        \"context\":\"Webex Calling supports hybrid deployments by integrating with both cloud and on-premises PBX systems, allowing for seamless communication.\",\n        \"answer\":\"Integration with cloud and on-premises PBX systems supports hybrid deployments in Webex Calling.\"\n    },\n    {\n        \"question\":\"What are the call management features in Webex Calling?\",\n        \"context\":\"Webex Calling provides call management features such as call forwarding, call transfer, call hold, and voicemail.\",\n        \"answer\":\"Call management features in Webex Calling include call forwarding, call transfer, call hold, and voicemail.\"\n    },\n    {\n        \"question\":\"How does Webex Calling support video calls?\",\n        \"context\":\"Webex Calling supports video calls between video-capable devices, allowing users to make and receive video calls seamlessly.\",\n        \"answer\":\"Video calls are supported between video-capable devices in Webex Calling.\"\n    },\n    {\n        \"question\":\"What compliance features are available in Webex Calling?\",\n        \"context\":\"Webex Calling includes compliance features such as call recording, logging, and monitoring to meet regulatory requirements.\",\n        \"answer\":\"Compliance features in Webex Calling include call recording, logging, and monitoring.\"\n    },\n    {\n        \"question\":\"What is the function of SRTP in Webex Calling?\",\n        \"context\":\"Secure Real-Time Transport Protocol (SRTP) is used to encrypt media streams in Webex Calling, ensuring secure communication.\",\n        \"answer\":\"SRTP encrypts media streams in Webex Calling to ensure secure communication.\"\n    },\n    {\n        \"question\":\"How does Webex Calling support call analytics?\",\n        \"context\":\"Webex Calling offers detailed call analytics, including metrics on call quality, usage patterns, and performance to help optimize communication systems.\",\n        \"answer\":\"Webex Calling provides detailed call analytics on call quality, usage patterns, and performance.\"\n    },\n    {\n        \"question\":\"What kind of redundancy features are available in Webex Calling?\",\n        \"context\":\"Webex Calling includes geo-redundant data centers and automatic failover mechanisms to ensure continuous service and high availability.\",\n        \"answer\":\"Redundancy features in Webex Calling include geo-redundant data centers and automatic failover.\"\n    },\n    {\n        \"question\":\"How does Webex Calling ensure secure voice communications?\",\n        \"context\":\"Webex Calling uses encryption protocols such as SRTP for media and TLS for signaling to protect voice communications.\",\n        \"answer\":\"Encryption protocols like SRTP for media and TLS for signaling ensure secure voice communications in Webex Calling.\"\n    },\n    {\n        \"question\":\"What integrations does Webex Calling offer with contact center solutions?\",\n        \"context\":\"Webex Calling integrates with Webex Contact Center, providing advanced features like call routing, IVR, and analytics to enhance customer service operations.\",\n        \"answer\":\"Integration with Webex Contact Center offers advanced features like call routing, IVR, and analytics.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle call monitoring and recording?\",\n        \"context\":\"Webex Calling includes features for call monitoring and recording, which can be used for compliance, training, and quality assurance purposes.\",\n        \"answer\":\"Call monitoring and recording features in Webex Calling support compliance, training, and quality assurance.\"\n    },\n    {\n        \"question\":\"What options are available for user authentication in Webex Calling?\",\n        \"context\":\"User authentication in Webex Calling is managed through secure protocols and can integrate with identity providers for single sign-on (SSO).\",\n        \"answer\":\"Webex Calling supports user authentication through secure protocols and integration with identity providers for SSO.\"\n    },\n    {\n        \"question\":\"What is the role of the Webex App in Webex Calling?\",\n        \"context\":\"The Webex App provides a unified interface for messaging, video conferencing, and calling, allowing users to manage all communication needs from a single platform.\",\n        \"answer\":\"The Webex App unifies messaging, video conferencing, and calling for a seamless communication experience.\"\n    },\n    {\n        \"question\":\"How does Webex Calling support international business operations?\",\n        \"context\":\"Webex Calling supports international operations with global data centers, ensuring reliable and high-quality voice communication around the world.\",\n        \"answer\":\"Global data centers support reliable and high-quality voice communication for international operations in Webex Calling.\"\n    },\n    {\n        \"question\":\"What management capabilities does Webex Calling provide?\",\n        \"context\":\"Webex Calling offers comprehensive management tools through the Webex Control Hub, allowing for centralized provisioning, configuration, and monitoring of services.\",\n        \"answer\":\"The Webex Control Hub provides centralized provisioning, configuration, and monitoring for Webex Calling services.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle voice and video integration?\",\n        \"context\":\"Webex Calling integrates with Webex Meetings and Webex Teams to provide seamless voice and video communication, enhancing collaboration.\",\n        \"answer\":\"Integration with Webex Meetings and Webex Teams allows for seamless voice and video communication in Webex Calling.\"\n    },\n    {\n        \"question\":\"What are the benefits of Webex Calling for large enterprises?\",\n        \"context\":\"Webex Calling offers large enterprises scalability, advanced call management features, and integration with other collaboration tools to streamline communication.\",\n        \"answer\":\"Scalability, advanced call management, and integration with collaboration tools are benefits for large enterprises using Webex Calling.\"\n    },\n    {\n        \"question\":\"What disaster recovery options are available with Webex Calling?\",\n        \"context\":\"Webex Calling ensures disaster recovery through geo-redundant data centers and automatic failover to maintain continuous service.\",\n        \"answer\":\"Geo-redundant data centers and automatic failover provide disaster recovery options in Webex Calling.\"\n    },\n    {\n        \"question\":\"How does Webex Calling manage call quality?\",\n        \"context\":\"Webex Calling manages call quality with network optimization, advanced codecs, and quality of service (QoS) policies to ensure clear voice communication.\",\n        \"answer\":\"Call quality in Webex Calling is managed through network optimization, advanced codecs, and QoS policies.\"\n    },\n    {\n        \"question\":\"What user profile management features are available in Webex Calling?\",\n        \"context\":\"User profiles in Webex Calling are managed through the Webex Control Hub, allowing administrators to configure settings and permissions.\",\n        \"answer\":\"User profile management features in Webex Calling include settings and permissions configuration through the Webex Control Hub.\"\n    },\n    {\n        \"question\":\"How does Webex Calling integrate with CRM systems?\",\n        \"context\":\"Webex Calling supports integration with CRM systems, enhancing customer interactions and improving business processes.\",\n        \"answer\":\"Integration with CRM systems enhances customer interactions and business processes in Webex Calling.\"\n    },\n    {\n        \"question\":\"What compliance features does Webex Calling offer?\",\n        \"context\":\"Webex Calling includes compliance features such as call recording, monitoring, and secure data handling to meet regulatory requirements.\",\n        \"answer\":\"Compliance features in Webex Calling include call recording, monitoring, and secure data handling.\"\n    },\n    {\n        \"question\":\"How does Webex Calling support remote teams?\",\n        \"context\":\"Webex Calling supports remote teams by providing enterprise-grade calling features, mobility, and integration with collaboration tools.\",\n        \"answer\":\"Remote teams benefit from enterprise-grade calling features, mobility, and collaboration tool integration in Webex Calling.\"\n    },\n    {\n        \"question\":\"What are the main security features of Webex Calling?\",\n        \"context\":\"Webex Calling includes security features such as encryption, secure voice, and compliance with industry standards to protect communications.\",\n        \"answer\":\"Security features in Webex Calling include encryption, secure voice, and industry-standard compliance.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle emergency call routing?\",\n        \"context\":\"Emergency call routing in Webex Calling is managed through predefined routing rules that ensure calls are directed to the appropriate emergency services.\",\n        \"answer\":\"Predefined routing rules manage emergency call routing in Webex Calling.\"\n    },\n    {\n        \"question\":\"What kind of support does Cisco provide for Webex Calling?\",\n        \"context\":\"Cisco provides comprehensive support for Webex Calling, including technical assistance, training, and resources for deployment and management.\",\n        \"answer\":\"Technical assistance, training, and deployment resources are part of Cisco's support for Webex Calling.\"\n    },\n    {\n        \"question\":\"How does Webex Calling support hybrid work environments?\",\n        \"context\":\"Webex Calling supports hybrid work environments by integrating with both cloud and on-premises PBX systems, allowing for seamless communication.\",\n        \"answer\":\"Webex Calling supports hybrid work environments by integrating with cloud and on-premises PBX systems.\"\n    },\n    {\n        \"question\":\"What management tools are available in Webex Calling?\",\n        \"context\":\"Webex Calling provides management tools such as the Webex Control Hub for provisioning, configuring, and managing services.\",\n        \"answer\":\"Management tools in Webex Calling include the Webex Control Hub for provisioning, configuring, and managing services.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle call quality issues?\",\n        \"context\":\"Webex Calling includes tools for monitoring call quality and diagnosing issues, ensuring high-quality voice communication.\",\n        \"answer\":\"Tools for monitoring and diagnosing issues ensure high-quality voice communication in Webex Calling.\"\n    },\n    {\n        \"question\":\"What kind of analytics does Webex Calling offer?\",\n        \"context\":\"Webex Calling offers analytics on call quality, usage patterns, and performance metrics to help optimize communication.\",\n        \"answer\":\"Analytics on call quality, usage patterns, and performance metrics are available in Webex Calling.\"\n    },\n    {\n        \"question\":\"What devices are compatible with Webex Calling?\",\n        \"context\":\"Webex Calling is compatible with a range of devices including Cisco IP Phones, Webex Room Devices, and third-party SIP phones.\",\n        \"answer\":\"Compatible devices for Webex Calling include Cisco IP Phones, Webex Room Devices, and third-party SIP phones.\"\n    },\n    {\n        \"question\":\"How does Webex Calling integrate with Webex Meetings?\",\n        \"context\":\"Webex Calling integrates with Webex Meetings to provide a seamless experience for scheduling and joining video meetings.\",\n        \"answer\":\"Integration with Webex Meetings provides a seamless experience for scheduling and joining video meetings in Webex Calling.\"\n    },\n    {\n        \"question\":\"What role does Webex Control Hub play in Webex Calling?\",\n        \"context\":\"Webex Control Hub is used for provisioning, configuring, and managing Webex Calling services, providing a centralized management interface.\",\n        \"answer\":\"Webex Control Hub provides a centralized interface for provisioning, configuring, and managing Webex Calling services.\"\n    },\n    {\n        \"question\":\"How does Webex Calling ensure security for voice communications?\",\n        \"context\":\"Webex Calling ensures security through encryption protocols like SRTP for media and TLS for signaling, protecting voice communications.\",\n        \"answer\":\"Encryption protocols like SRTP for media and TLS for signaling ensure security in Webex Calling.\"\n    },\n    {\n        \"question\":\"What features does Webex Calling offer for team collaboration?\",\n        \"context\":\"Webex Calling offers features such as group call management, conferencing, and integration with Webex Teams for enhanced collaboration.\",\n        \"answer\":\"Group call management, conferencing, and Webex Teams integration are features of Webex Calling for team collaboration.\"\n    },\n    {\n        \"question\":\"How is call recording handled in Webex Calling?\",\n        \"context\":\"Call recording in Webex Calling is managed through built-in features that allow recording for compliance, training, and quality assurance.\",\n        \"answer\":\"Built-in call recording features in Webex Calling support compliance, training, and quality assurance.\"\n    },\n    {\n        \"question\":\"What scalability options are available in Webex Calling?\",\n        \"context\":\"Webex Calling is designed to scale from small businesses to large enterprises, supporting thousands of users across multiple locations.\",\n        \"answer\":\"Scalability in Webex Calling supports small businesses to large enterprises, accommodating thousands of users.\"\n    },\n    {\n        \"question\":\"How does Webex Calling support international operations?\",\n        \"context\":\"Webex Calling supports international operations with global data centers, ensuring reliable and high-quality voice communication worldwide.\",\n        \"answer\":\"Global data centers ensure reliable and high-quality voice communication for international operations in Webex Calling.\"\n    },\n    {\n        \"question\":\"What redundancy features are built into Webex Calling?\",\n        \"context\":\"Webex Calling includes redundancy features like geo-redundant data centers and automatic failover to ensure continuous service.\",\n        \"answer\":\"Redundancy features in Webex Calling include geo-redundant data centers and automatic failover for continuous service.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle user authentication?\",\n        \"context\":\"User authentication in Webex Calling is managed through secure protocols and integration with identity providers for single sign-on (SSO).\",\n        \"answer\":\"Secure protocols and integration with identity providers for single sign-on (SSO) manage user authentication in Webex Calling.\"\n    },\n    {\n        \"question\":\"What reporting capabilities does Webex Calling provide?\",\n        \"context\":\"Webex Calling provides reporting capabilities that include detailed call analytics, usage patterns, and performance metrics.\",\n        \"answer\":\"Detailed call analytics, usage patterns, and performance metrics are part of Webex Calling's reporting capabilities.\"\n    },\n    {\n        \"question\":\"What integrations does Webex Calling support with CRM systems?\",\n        \"context\":\"Webex Calling supports integrations with CRM systems to enhance customer interactions and streamline business processes.\",\n        \"answer\":\"Integrations with CRM systems in Webex Calling enhance customer interactions and streamline business processes.\"\n    },\n    {\n        \"question\":\"How does Webex Calling support compliance with regulations?\",\n        \"context\":\"Webex Calling supports compliance with regulations through features like call recording, monitoring, and secure data handling.\",\n        \"answer\":\"Compliance features in Webex Calling include call recording, monitoring, and secure data handling.\"\n    },\n    {\n        \"question\":\"What is the role of SRTP in Webex Calling?\",\n        \"context\":\"Secure Real-Time Transport Protocol (SRTP) is used in Webex Calling to encrypt voice media streams, ensuring secure communication.\",\n        \"answer\":\"SRTP encrypts voice media streams to ensure secure communication in Webex Calling.\"\n    },\n    {\n        \"question\":\"How does Webex Calling manage user profiles?\",\n        \"context\":\"User profiles in Webex Calling are managed through the Webex Control Hub, allowing administrators to configure settings and permissions.\",\n        \"answer\":\"Webex Control Hub manages user profiles in Webex Calling, enabling configuration of settings and permissions by administrators.\"\n    },\n    {\n        \"question\":\"How does Webex Calling support video conferencing?\",\n        \"context\":\"Webex Calling supports video conferencing through integration with Webex Meetings, allowing users to join video meetings from their phones or Webex devices.\",\n        \"answer\":\"Webex Calling supports video conferencing via integration with Webex Meetings.\"\n    },\n    {\n        \"question\":\"What is the role of a Local Gateway in Webex Calling?\",\n        \"context\":\"A Local Gateway connects to the PSTN and handles call routing between Webex Calling and on-premises PBX systems.\",\n        \"answer\":\"The Local Gateway connects to the PSTN and handles call routing between Webex Calling and on-premises PBX systems.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle number porting?\",\n        \"context\":\"Webex Calling supports number porting, allowing users to transfer existing phone numbers from another provider to Webex Calling.\",\n        \"answer\":\"Webex Calling allows users to port existing phone numbers from another provider.\"\n    },\n    {\n        \"question\":\"How does Webex Calling support disaster recovery?\",\n        \"context\":\"Webex Calling ensures disaster recovery through geo-redundant data centers and automatic failover mechanisms.\",\n        \"answer\":\"Geo-redundant data centers and automatic failover mechanisms support disaster recovery in Webex Calling.\"\n    },\n    {\n        \"question\":\"What is the Webex Calling architecture?\",\n        \"context\":\"Webex Calling is built on a cloud-based architecture with distributed data centers to ensure high availability and resilience.\",\n        \"answer\":\"Webex Calling has a cloud-based architecture with distributed data centers for high availability and resilience.\"\n    },\n    {\n        \"question\":\"How does Webex Calling support call encryption?\",\n        \"context\":\"Webex Calling uses Secure Real-Time Transport Protocol (SRTP) and Transport Layer Security (TLS) to encrypt call signaling and media.\",\n        \"answer\":\"SRTP and TLS are used to encrypt call signaling and media in Webex Calling.\"\n    },\n    {\n        \"question\":\"What are the benefits of Webex Calling for small businesses?\",\n        \"context\":\"Webex Calling offers small businesses enterprise-grade calling features, scalability, and integration with collaboration tools at a lower cost.\",\n        \"answer\":\"Small businesses benefit from enterprise-grade calling features, scalability, and collaboration tool integration at a lower cost with Webex Calling.\"\n    },\n    {\n        \"question\":\"How does Webex Calling integrate with existing collaboration tools?\",\n        \"context\":\"Webex Calling integrates with tools like Webex Teams and Microsoft Teams, providing a seamless collaboration experience.\",\n        \"answer\":\"Integration with Webex Teams and Microsoft Teams offers a seamless collaboration experience in Webex Calling.\"\n    },\n    {\n        \"question\":\"What analytics features are available in Webex Calling?\",\n        \"context\":\"Webex Calling provides analytics features such as call quality monitoring, usage reports, and performance dashboards.\",\n        \"answer\":\"Analytics features in Webex Calling include call quality monitoring, usage reports, and performance dashboards.\"\n    },\n    {\n        \"question\":\"How does Webex Calling support mobile devices?\",\n        \"context\":\"Webex Calling supports mobile devices through the Webex App, allowing users to make and receive calls on their smartphones.\",\n        \"answer\":\"The Webex App enables Webex Calling users to make and receive calls on mobile devices.\"\n    },\n    {\n        \"question\":\"What is the role of the Webex App in Webex Calling?\",\n        \"context\":\"The Webex App provides a unified interface for messaging, video conferencing, and calling within Webex Calling.\",\n        \"answer\":\"The Webex App offers a unified interface for messaging, video conferencing, and calling in Webex Calling.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle multi-location deployments?\",\n        \"context\":\"Webex Calling supports multi-location deployments by allowing centralized management and consistent calling features across all locations.\",\n        \"answer\":\"Centralized management and consistent calling features support multi-location deployments in Webex Calling.\"\n    },\n    {\n        \"question\":\"What troubleshooting tools are available in Webex Calling?\",\n        \"context\":\"Webex Calling provides troubleshooting tools such as diagnostic reports, call logs, and real-time call quality monitoring.\",\n        \"answer\":\"Troubleshooting tools in Webex Calling include diagnostic reports, call logs, and real-time call quality monitoring.\"\n    },\n    {\n        \"question\":\"How does Webex Calling ensure voice quality?\",\n        \"context\":\"Webex Calling ensures voice quality through network optimization, advanced codecs, and quality of service (QoS) policies.\",\n        \"answer\":\"Voice quality in Webex Calling is ensured by network optimization, advanced codecs, and QoS policies.\"\n    },\n    {\n        \"question\":\"What integrations does Webex Calling support?\",\n        \"context\":\"Webex Calling supports integrations with CRM systems, collaboration tools, and contact center solutions.\",\n        \"answer\":\"Integrations with CRM systems, collaboration tools, and contact center solutions are supported by Webex Calling.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle call recording?\",\n        \"context\":\"Webex Calling includes call recording features for compliance, training, and quality assurance purposes.\",\n        \"answer\":\"Call recording in Webex Calling is available for compliance, training, and quality assurance.\"\n    },\n    {\n        \"question\":\"What role do data centers play in Webex Calling?\",\n        \"context\":\"Data centers in Webex Calling provide redundancy, high availability, and disaster recovery capabilities.\",\n        \"answer\":\"Data centers provide redundancy, high availability, and disaster recovery in Webex Calling.\"\n    },\n    {\n        \"question\":\"What are the benefits of Webex Calling for remote teams?\",\n        \"context\":\"Webex Calling offers remote teams enterprise-grade calling features, mobility, and integration with collaboration tools.\",\n        \"answer\":\"Enterprise-grade calling features, mobility, and collaboration tool integration benefit remote teams using Webex Calling.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle call forwarding?\",\n        \"context\":\"Webex Calling provides flexible call forwarding options, allowing users to redirect calls to other numbers or devices.\",\n        \"answer\":\"Flexible call forwarding options in Webex Calling allow users to redirect calls to other numbers or devices.\"\n    },\n    {\n        \"question\":\"What is Webex Calling?\",\n        \"context\":\"Webex Calling is a cloud-based phone system designed to provide enterprise-grade calling features.\",\n        \"answer\":\"Webex Calling is a cloud-based phone system offering enterprise-grade calling features.\"\n    },\n    {\n        \"question\":\"What types of phones are supported by Webex Calling?\",\n        \"context\":\"Webex Calling supports Cisco MPP Series IP Phones, Webex Devices, and a variety of third-party devices.\",\n        \"answer\":\"Webex Calling supports Cisco MPP Series IP Phones, Webex Devices, and various third-party devices.\"\n    },\n    {\n        \"question\":\"How does Webex Calling integrate with other Webex services?\",\n        \"context\":\"Webex Calling integrates seamlessly with Webex Meetings, Webex Teams, and Webex Contact Center.\",\n        \"answer\":\"Webex Calling integrates with Webex Meetings, Webex Teams, and Webex Contact Center.\"\n    },\n    {\n        \"question\":\"What are the deployment options for Webex Calling?\",\n        \"context\":\"Webex Calling can be deployed as a cloud-only solution or in a hybrid model with existing on-premises PBX systems.\",\n        \"answer\":\"Webex Calling offers cloud-only and hybrid deployment options.\"\n    },\n    {\n        \"question\":\"How is call quality maintained in Webex Calling?\",\n        \"context\":\"Call quality in Webex Calling is maintained through the use of advanced codecs, network optimization, and quality of service (QoS) policies.\",\n        \"answer\":\"Advanced codecs, network optimization, and QoS policies maintain call quality in Webex Calling.\"\n    },\n    {\n        \"question\":\"What is the purpose of the Webex Control Hub?\",\n        \"context\":\"The Webex Control Hub provides a centralized management interface for provisioning, configuring, and managing Webex Calling services.\",\n        \"answer\":\"The Webex Control Hub centralizes management for provisioning, configuring, and managing Webex Calling services.\"\n    },\n    {\n        \"question\":\"How does Webex Calling support remote work?\",\n        \"context\":\"Webex Calling provides remote workers with enterprise-grade calling features and connectivity from any location with internet access.\",\n        \"answer\":\"Webex Calling supports remote work by offering enterprise-grade calling features and connectivity from any location with internet access.\"\n    },\n    {\n        \"question\":\"What security measures are implemented in Webex Calling?\",\n        \"context\":\"Webex Calling includes security measures such as encryption, secure voice, and compliance with industry standards.\",\n        \"answer\":\"Encryption, secure voice, and industry-standard compliance are key security measures in Webex Calling.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle emergency calls?\",\n        \"context\":\"Webex Calling compares the dial string with emergency numbers defined in the national numbering plan for the calling user.\",\n        \"answer\":\"Emergency calls in Webex Calling are handled by comparing the dial string with defined emergency numbers.\"\n    },\n    {\n        \"question\":\"What are the benefits of Webex Calling for enterprises?\",\n        \"context\":\"Webex Calling provides enterprises with benefits such as lower maintenance costs, scalability, remote work support, and integration with collaboration tools.\",\n        \"answer\":\"Lower maintenance costs, scalability, remote work support, and collaboration tool integration are benefits for enterprises using Webex Calling.\"\n    },\n    {\n        \"question\":\"How does Webex Calling manage call routing?\",\n        \"context\":\"Call routing in Webex Calling is managed through dial plans and route groups for efficient and flexible call management.\",\n        \"answer\":\"Dial plans and route groups manage call routing in Webex Calling.\"\n    },\n    {\n        \"question\":\"What customer support is available for Webex Calling?\",\n        \"context\":\"Cisco provides comprehensive customer support for Webex Calling, including technical assistance, training, and deployment resources.\",\n        \"answer\":\"Webex Calling customer support includes technical assistance, training, and deployment resources from Cisco.\"\n    },\n    {\n        \"question\":\"How does Webex Calling ensure high availability?\",\n        \"context\":\"High availability is ensured through geo-redundant data centers and a fully redundant global backbone network.\",\n        \"answer\":\"Geo-redundant data centers and a redundant global backbone network ensure high availability in Webex Calling.\"\n    },\n    {\n        \"question\":\"What are the international calling capabilities of Webex Calling?\",\n        \"context\":\"Webex Calling supports international calling through its global network of data centers, ensuring high-quality voice communication.\",\n        \"answer\":\"International calling is supported by Webex Calling's global data center network for high-quality voice communication.\"\n    },\n    {\n        \"question\":\"What is the role of SIP in Webex Calling?\",\n        \"context\":\"Session Initiation Protocol (SIP) is used for signaling and managing multimedia communication sessions in Webex Calling.\",\n        \"answer\":\"SIP manages signaling and multimedia communication sessions in Webex Calling.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle compliance with local regulations?\",\n        \"context\":\"Webex Calling supports compliance by routing calls through regional data centers and supporting lawful intercept.\",\n        \"answer\":\"Compliance with local regulations is ensured by routing calls through regional data centers and supporting lawful intercept.\"\n    },\n    {\n        \"question\":\"What integration options are available for Webex Calling?\",\n        \"context\":\"Webex Calling integrates seamlessly with Webex Meetings, Webex Teams, and third-party collaboration tools to enhance productivity.\",\n        \"answer\":\"Webex Calling integrates with Webex Meetings, Webex Teams, and third-party tools for enhanced productivity.\"\n    },\n    {\n        \"question\":\"How does Webex Calling support contact centers?\",\n        \"context\":\"Webex Calling integrates with Webex Contact Center to provide advanced features such as call routing, IVR, and analytics.\",\n        \"answer\":\"Advanced contact center features like call routing, IVR, and analytics are provided through Webex Contact Center integration.\"\n    },\n    {\n        \"question\":\"What is the function of SRTP in Webex Calling?\",\n        \"context\":\"Secure Real-Time Transport Protocol (SRTP) is used to encrypt media streams, ensuring secure communication in Webex Calling.\",\n        \"answer\":\"SRTP encrypts media streams to ensure secure communication in Webex Calling.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle video calls?\",\n        \"context\":\"Webex Calling supports video calls between video-capable MPP phones, Webex Devices, and the Webex App.\",\n        \"answer\":\"Video calls are supported between video-capable MPP phones, Webex Devices, and the Webex App in Webex Calling.\"\n    },\n    {\n        \"question\":\"What are the benefits of Webex Calling for remote workers?\",\n        \"context\":\"Webex Calling provides remote workers with enterprise-grade calling features and the ability to stay connected from anywhere with an internet connection.\",\n        \"answer\":\"Webex Calling offers remote workers enterprise-grade calling features and connectivity from any location with internet access.\"\n    },\n    {\n        \"question\":\"What is the role of the Webex Control Hub in managing Webex Calling?\",\n        \"context\":\"The Webex Control Hub provides a centralized management interface for provisioning, configuring, and managing Webex Calling services.\",\n        \"answer\":\"The Webex Control Hub centralizes management for provisioning, configuring, and managing Webex Calling services.\"\n    },\n    {\n        \"question\":\"How does Webex Calling integrate with existing on-premises PBX systems?\",\n        \"context\":\"Webex Calling can integrate with existing on-premises PBX systems through Local Gateways, allowing for a hybrid deployment model.\",\n        \"answer\":\"Webex Calling integrates with on-premises PBX systems via Local Gateways for hybrid deployment.\"\n    },\n    {\n        \"question\":\"What is the significance of SRTP in Webex Calling?\",\n        \"context\":\"Secure Real-Time Transport Protocol (SRTP) is used in Webex Calling to encrypt media streams and ensure secure communication.\",\n        \"answer\":\"SRTP encrypts media streams in Webex Calling to ensure secure communication.\"\n    },\n    {\n        \"question\":\"How does Webex Calling support compliance with local regulations?\",\n        \"context\":\"Webex Calling supports compliance with local regulations by routing calls through regional data centers and supporting lawful intercept.\",\n        \"answer\":\"Webex Calling complies with local regulations by using regional data centers and supporting lawful intercept.\"\n    },\n    {\n        \"question\":\"What features are available for call management in Webex Calling?\",\n        \"context\":\"Webex Calling provides features such as call forwarding, call transfer, call hold, and voicemail for efficient call management.\",\n        \"answer\":\"Webex Calling offers call forwarding, call transfer, call hold, and voicemail features.\"\n    },\n    {\n        \"question\":\"What options are available for integrating Webex Calling with other collaboration tools?\",\n        \"context\":\"Webex Calling integrates seamlessly with Webex Meetings, Webex Teams, and third-party collaboration tools to enhance productivity.\",\n        \"answer\":\"Webex Calling integrates with Webex Meetings, Webex Teams, and third-party tools for enhanced productivity.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle call routing?\",\n        \"context\":\"Call routing in Webex Calling is managed through dial plans and route groups, allowing for efficient and flexible call management.\",\n        \"answer\":\"Dial plans and route groups manage call routing in Webex Calling.\"\n    },\n    {\n        \"question\":\"What security features are built into Webex Calling?\",\n        \"context\":\"Webex Calling includes security features such as encryption, secure voice, and compliance with industry standards to protect communications.\",\n        \"answer\":\"Webex Calling features encryption, secure voice, and industry-standard compliance for communication security.\"\n    },\n    {\n        \"question\":\"How does Webex Calling support mobile users?\",\n        \"context\":\"Webex Calling supports mobile users through the Webex App, allowing them to make and receive calls from their mobile devices.\",\n        \"answer\":\"The Webex App enables mobile users to make and receive calls on their devices with Webex Calling.\"\n    },\n    {\n        \"question\":\"How does Webex Calling integrate with Cisco devices?\",\n        \"context\":\"Webex Calling integrates with Cisco devices such as MPP phones and Webex Room devices for a unified communication experience.\",\n        \"answer\":\"Cisco MPP phones and Webex Room devices integrate with Webex Calling for unified communication.\"\n    },\n    {\n        \"question\":\"What are the deployment models available for Webex Calling?\",\n        \"context\":\"Webex Calling can be deployed as a cloud-only solution or in a hybrid model with existing on-premises PBX systems.\",\n        \"answer\":\"Webex Calling supports cloud-only and hybrid deployment models.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle international calling?\",\n        \"context\":\"Webex Calling supports international calling through its global network of data centers, ensuring high-quality voice communication.\",\n        \"answer\":\"International calling is supported by Webex Calling's global data center network for high-quality voice communication.\"\n    },\n    {\n        \"question\":\"What features does Webex Calling offer for contact centers?\",\n        \"context\":\"Webex Calling integrates with Webex Contact Center to provide advanced contact center features such as call routing, IVR, and analytics.\",\n        \"answer\":\"Advanced contact center features like call routing, IVR, and analytics are offered through Webex Contact Center integration.\"\n    },\n    {\n        \"question\":\"How does Webex Calling support scalability?\",\n        \"context\":\"Webex Calling is designed to scale with the needs of businesses, supporting small to large enterprises with its cloud-based architecture.\",\n        \"answer\":\"The cloud-based architecture of Webex Calling supports scalability for businesses of all sizes.\"\n    },\n    {\n        \"question\":\"What type of customer support is available for Webex Calling?\",\n        \"context\":\"Cisco provides comprehensive customer support for Webex Calling, including technical assistance, training, and resources for deployment.\",\n        \"answer\":\"Webex Calling customer support includes technical assistance, training, and deployment resources from Cisco.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle voice quality?\",\n        \"context\":\"Webex Calling ensures high voice quality through the use of advanced codecs, network optimization, and quality of service (QoS) policies.\",\n        \"answer\":\"High voice quality in Webex Calling is ensured by advanced codecs, network optimization, and QoS policies.\"\n    },\n    {\n        \"question\":\"What are the benefits of using Webex Calling over traditional PBX systems?\",\n        \"context\":\"Webex Calling offers benefits such as lower maintenance costs, scalability, remote work support, and integration with other collaboration tools.\",\n        \"answer\":\"Benefits of Webex Calling include lower maintenance costs, scalability, remote work support, and collaboration tool integration.\"\n    },\n    {\n        \"question\":\"What signaling and media protocols does Webex Calling use?\",\n        \"context\":\"Webex Calling uses SIP for signaling and SRTP for media. Dynamic NAT can be used for IP Phone and Local Gateway IP addresses.\",\n        \"answer\":\"Webex Calling uses SIP for signaling and SRTP for media.\"\n    },\n    {\n        \"question\":\"What is the role of the Webex Control Hub in Webex Calling?\",\n        \"context\":\"The Webex Control Hub, as part of the provisioning process of a trunk, provides connection parameters and digest credentials for SIP authentication.\",\n        \"answer\":\"The Webex Control Hub provides connection parameters and digest credentials for SIP authentication during Local Gateway registration.\"\n    },\n    {\n        \"question\":\"What group features does Webex Calling provide?\",\n        \"context\":\"In addition to enterprise-grade PBX features, Webex Calling also provides group features, including unlimited subscriptions of auto-attendants, hunt groups, and call queues.\",\n        \"answer\":\"Webex Calling provides group features like unlimited subscriptions of auto-attendants, hunt groups, and call queues.\"\n    },\n    {\n        \"question\":\"What are the regional platforms for Webex Calling?\",\n        \"context\":\"Webex Calling operates six regional platforms: US, Canada, UK, Europe, APJC Japan, and APJC Australia.\",\n        \"answer\":\"Webex Calling operates regional platforms in the US, Canada, UK, Europe, APJC Japan, and APJC Australia.\"\n    },\n    {\n        \"question\":\"What role do load balancers play in Webex Calling datacenters?\",\n        \"context\":\"Load balancers and other network functions are required to build a scalable, redundant datacenter architecture in each Webex Calling datacenter.\",\n        \"answer\":\"Load balancers are used to build a scalable, redundant datacenter architecture.\"\n    },\n    {\n        \"question\":\"What are the capabilities of the Webex App in Webex Calling?\",\n        \"context\":\"The Webex App supports commonly used mid-call features, rich presence with MPP and a single line for each user. The Webex App also allows control of the user\\u2019s Cisco MPP phone.\",\n        \"answer\":\"The Webex App supports mid-call features, rich presence, and control of the user's Cisco MPP phone.\"\n    },\n    {\n        \"question\":\"What is the role of the Webex Control Hub in Local Gateway registration?\",\n        \"context\":\"The Webex Control Hub, as part of the provisioning process of a trunk, provides connection parameters and digest credentials for SIP authentication.\",\n        \"answer\":\"The Webex Control Hub provides connection parameters and digest credentials for SIP authentication during Local Gateway registration.\"\n    },\n    {\n        \"question\":\"What deployment options are available for Webex Calling?\",\n        \"context\":\"Webex Calling is deployed as a cloud-only solution, or if you require a mixed network of both cloud and on-premises PBXs, it is deployed as part of a hybrid cloud.\",\n        \"answer\":\"Webex Calling can be deployed as a cloud-only solution or as part of a hybrid cloud.\"\n    },\n    {\n        \"question\":\"What are the data center locations for Webex Calling in the US?\",\n        \"context\":\"Webex Calling operates from redundant data centers in Dallas, Chicago, and New York within the US.\",\n        \"answer\":\"Webex Calling data centers in the US are located in Dallas, Chicago, and New York.\"\n    },\n    {\n        \"question\":\"What capabilities does the Webex App offer when integrated with Webex Calling?\",\n        \"context\":\"The Webex App offers messaging, screen sharing, audio and video conferencing, and integrated calling with mid-call features or control of a user\\u2019s desk phone.\",\n        \"answer\":\"The Webex App offers messaging, screen sharing, audio and video conferencing, and integrated calling with mid-call features or control of a user\\u2019s desk phone.\"\n    },\n    {\n        \"question\":\"What are the considerations for video calls in Webex Calling?\",\n        \"context\":\"Video calls are possible within a single Webex Calling org between video-capable devices or Webex App by dialing a work number or extension. PSTN only carries voice.\",\n        \"answer\":\"Video calls can be made within a single Webex Calling org between video-capable devices or Webex App, while PSTN only supports voice calls.\"\n    },\n    {\n        \"question\":\"What are the main functions hosted in each Webex Calling datacenter?\",\n        \"context\":\"Each Webex Calling datacenter hosts call routing functions, provides provisioning interface access, and hosts access and peering Session Border Controllers (SBCs).\",\n        \"answer\":\"Webex Calling datacenters host call routing functions, provide provisioning interface access, and host access and peering SBCs.\"\n    },\n    {\n        \"question\":\"How does Webex Calling ensure firewall traversal for calls?\",\n        \"context\":\"Phones and Local Gateways initiate a TLS connection to Webex Calling and are authenticated by Webex Calling, which then uses the same connection to send traffic back, providing firewall traversal.\",\n        \"answer\":\"Webex Calling ensures firewall traversal by using TLS connections initiated by phones and Local Gateways, and sending traffic back through the same connection.\"\n    },\n    {\n        \"question\":\"How are trunks used in Webex Calling?\",\n        \"context\":\"Trunks connect Webex Calling with Local Gateways or to Dedicated Instance, if included in the solution. Each trunk represents a connection to one Local Gateway instance configured on a CUBE or Cisco voice gateway.\",\n        \"answer\":\"Trunks connect Webex Calling with Local Gateways or Dedicated Instances.\"\n    },\n    {\n        \"question\":\"What is the Private Network Connect (PNC) solution?\",\n        \"context\":\"The Private Network Connect (PNC) solution allows Webex Calling customers to extend their private network to the cloud, ensuring high quality of service and low latency for voice calls.\",\n        \"answer\":\"The Private Network Connect (PNC) solution extends private networks to the cloud for high quality of service and low latency.\"\n    },\n    {\n        \"question\":\"What type of subscription is Webex Calling based on?\",\n        \"context\":\"Webex Calling is part of an integrated, intelligent, and modular team collaboration suite. Licensing is subscription-based and managed with the Cisco Collaboration Flex Plan.\",\n        \"answer\":\"Webex Calling uses a subscription-based licensing model managed with the Cisco Collaboration Flex Plan.\"\n    },\n    {\n        \"question\":\"Where are Webex Calling data centers located?\",\n        \"context\":\"Webex Calling is available globally and is delivered from redundant data centers in six regions: US, Canada, Europe, UK, Australia, and Japan.\",\n        \"answer\":\"Webex Calling data centers are located in the US, Canada, Europe, UK, Australia, and Japan.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle emergency calls?\",\n        \"context\":\"For calls originating from Webex Calling users, the dial string is first compared with the emergency numbers defined in the national numbering plan for the calling user.\",\n        \"answer\":\"Emergency calls are handled by comparing the dial string with emergency numbers defined in the national numbering plan.\"\n    },\n    {\n        \"question\":\"How are unknown numbers handled in Webex Calling?\",\n        \"context\":\"The routing behavior for unknown numbers is controlled by two settings: the enterprise-wide 'Unknown Number Handling' setting and the 'Calls to On-Premises Extension' setting at the location level.\",\n        \"answer\":\"Unknown numbers are handled based on the 'Unknown Number Handling' and 'Calls to On-Premises Extension' settings.\"\n    },\n    {\n        \"question\":\"What is the purpose of dial plans in Webex Calling?\",\n        \"context\":\"Dial plans add the ability to route calls to premises-based call control instances or between multiple premises-based call control instances based on matches against dial patterns within a dial plan.\",\n        \"answer\":\"Dial plans enable call routing to premises-based call control instances based on dial patterns.\"\n    },\n    {\n        \"question\":\"What are the benefits of using route groups in Webex Calling?\",\n        \"context\":\"Using route groups allows for redundancy and increased capacity. Multiple trunks can be grouped together in a route group to provide these benefits.\",\n        \"answer\":\"Route groups provide redundancy and increased capacity by grouping multiple trunks together.\"\n    },\n    {\n        \"question\":\"What happens if ICE negotiation fails in Webex Calling?\",\n        \"context\":\"If ICE negotiation fails, media is anchored on the Webex Calling Access SBC, causing media to flow from the originating phone via the customer\\u2019s Internet edge to the SBC and then back to the destination endpoint.\",\n        \"answer\":\"If ICE negotiation fails, media is anchored on the Webex Calling Access SBC, resulting in media flowing through the customer\\u2019s Internet edge to the SBC and back to the destination endpoint.\"\n    },\n    {\n        \"question\":\"What are route groups in Webex Calling?\",\n        \"context\":\"Route groups are used to group multiple trunks together to provide redundancy or increased capacity.\",\n        \"answer\":\"Route groups provide redundancy or increased capacity by grouping multiple trunks together.\"\n    },\n    {\n        \"question\":\"What is required for Webex Calling endpoints to connect to the datacenters?\",\n        \"context\":\"Webex Calling endpoints on the customer\\u2019s network use the public Internet as the access network to connect to the Webex Calling datacenters and establish over-the-top TLS connections.\",\n        \"answer\":\"Webex Calling endpoints use the public Internet to connect to datacenters and establish over-the-top TLS connections.\"\n    },\n    {\n        \"question\":\"What features does the Dedicated Instance option provide in Webex Calling?\",\n        \"context\":\"The Dedicated Instance option is an entitlement within Webex Calling that provides a Cisco Unified Communications Manager based stack of applications, in a private cloud, dedicated to a single customer.\",\n        \"answer\":\"The Dedicated Instance option provides a Cisco Unified Communications Manager based stack of applications in a private cloud dedicated to a single customer.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle PSTN access?\",\n        \"context\":\"Webex Calling can access the PSTN via Cisco Calling Plans, Cloud Connected PSTN, and Premises-based PSTN.\",\n        \"answer\":\"Webex Calling handles PSTN access through Cisco Calling Plans, Cloud Connected PSTN, and Premises-based PSTN.\"\n    },\n    {\n        \"question\":\"What are the benefits of the Webex Calling global backbone?\",\n        \"context\":\"The global backbone interconnects the datacenters with a multi-gigabit and fully redundant network, optimizing media round-trip times and ensuring high availability.\",\n        \"answer\":\"The global backbone optimizes media round-trip times and ensures high availability with a multi-gigabit, fully redundant network.\"\n    },\n    {\n        \"question\":\"What is the purpose of a Local Gateway in Webex Calling?\",\n        \"context\":\"A Local Gateway connects to the PSTN either directly by terminating a PSTN trunk or by connecting to an existing PSTN gateway via a SIP trunk. It also registers with Webex Calling and handles all calls between the PSTN and Webex Calling.\",\n        \"answer\":\"The Local Gateway connects to the PSTN and registers with Webex Calling to handle all calls between the PSTN and Webex Calling.\"\n    },\n    {\n        \"question\":\"What is the significance of media path optimization in Webex Calling?\",\n        \"context\":\"Media path optimization using ICE tries to establish a direct media path between the involved entities to reduce bandwidth usage and improve call quality.\",\n        \"answer\":\"Media path optimization establishes a direct media path between entities to reduce bandwidth usage and improve call quality.\"\n    },\n    {\n        \"question\":\"What is Webex Edge Connect?\",\n        \"context\":\"Webex Edge Connect is a solution that peers your Webex meetings and Webex Calling traffic with an Equinix Cloud Exchange (ECX) location. This peering improves user experience by providing guaranteed bandwidth and quality of service (QoS).\",\n        \"answer\":\"Webex Edge Connect peers Webex meetings and Webex Calling traffic with an Equinix Cloud Exchange (ECX) location to improve user experience with guaranteed bandwidth and QoS.\"\n    },\n    {\n        \"question\":\"What is the Webex Calling solution overview?\",\n        \"context\":\"Webex Calling is part of an integrated, intelligent, and modular team collaboration suite. It provides enterprise-grade PBX features, functionality, and performance previously only possible with an on-premises PBX network.\",\n        \"answer\":\"Webex Calling provides enterprise-grade PBX features, functionality, and performance.\"\n    },\n    {\n        \"question\":\"What connectivity options are available for Webex Calling?\",\n        \"context\":\"Customers and partners have access connectivity options beyond Over-the-top (OTT) Internet that can optimize the connection to Webex Calling and these include Webex Edge Connect or Private Network Connect.\",\n        \"answer\":\"Connectivity options for Webex Calling include Over-the-top (OTT) Internet, Webex Edge Connect, and Private Network Connect.\"\n    },\n    {\n        \"question\":\"What type of patterns can be included in a Webex Calling dial plan?\",\n        \"context\":\"Dial plans can include numeric patterns (E.164 numbers, enterprise numbers, extensions) and domain patterns for routing alphanumeric SIP URIs.\",\n        \"answer\":\"Dial plans can include numeric patterns and domain patterns for routing SIP URIs.\"\n    },\n    {\n        \"question\":\"What is the role of Local Gateways in Webex Calling?\",\n        \"context\":\"Local Gateways provide PSTN access for Webex Calling and can also connect Webex Calling to existing on-premises call control services.\",\n        \"answer\":\"Local Gateways provide PSTN access and connect Webex Calling to existing on-premises call control services.\"\n    },\n    {\n        \"question\":\"How does Webex Calling handle calls between different customers?\",\n        \"context\":\"Calls between two Webex Calling customers must be routed through the PSTN to meet legal requirements, such as lawful intercept.\",\n        \"answer\":\"Calls between different Webex Calling customers are routed through the PSTN.\"\n    },\n    {\n        \"question\":\"What types of phones are supported by Webex Calling?\",\n        \"context\":\"Webex Calling supports all models of Cisco Multiplatform Phones (MPP).\",\n        \"answer\":\"Webex Calling supports all models of Cisco Multiplatform Phones (MPP).\"\n    },\n    {\n        \"question\":\"What is Webex Edge Connect?\",\n        \"context\":\"Webex Edge Connect is a solution that peers your Webex meetings and Webex Calling traffic with an Equinix Cloud Exchange (ECX) location. This peering improves the calling and in-meeting user experience by providing guaranteed bandwidth and quality of service (QoS).\",\n        \"answer\":\"Webex Edge Connect peers Webex meetings and Webex Calling traffic with an Equinix Cloud Exchange location, improving user experience with guaranteed bandwidth and QoS.\"\n    },\n    {\n        \"question\":\"How can video calls be made in Webex Calling?\",\n        \"context\":\"Webex Calling supports video calls between video capable MPP phones, Webex Devices, and Webex App (desktop and mobile). Users can use speed dial with a SIP address to call into Webex meetings.\",\n        \"answer\":\"Video calls in Webex Calling can be made between video capable MPP phones, Webex Devices, and Webex App.\"\n    },\n    {\n        \"question\":\"What is the Dedicated Instance option in Webex Calling?\",\n        \"context\":\"The Dedicated Instance option is an entitlement within Webex Calling that provides a Cisco Unified Communications Manager based stack of applications, in a private cloud, dedicated to a single customer.\",\n        \"answer\":\"The Dedicated Instance option provides a Cisco Unified Communications Manager based stack of applications in a private cloud dedicated to a single customer.\"\n    },\n    {\n        \"question\":\"How many participants can Webex Meetings support when added to Webex Calling?\",\n        \"context\":\"Webex Meetings supports advanced meeting experiences including meeting room recording, meeting room locking, remote dial-in access over PSTN, and supporting up to 1000 meeting participants.\",\n        \"answer\":\"Webex Meetings can support up to 1000 meeting participants.\"\n    }\n]\n</code></pre> <p>Download example_llama3 file</p> <p>Save the file as example_llama3.json as we will be using it in the next step to push dataset on Hugging Face Hub</p> <p>To finalize, we again convert the above dataset (example_llama3.json) into Llama3 format. Lets look into those steps</p>"},{"location":"Task8b/#convert-dataset-into-llama3-format-and-upload-on-hugging-face","title":"Convert dataset into Llama3 format and upload on Hugging Face","text":"<ul> <li>Open Google Colab and create a new notebook. Click on \"File\" &gt; \"New notebook\" .Go to the \"Secrets\" section in the sidebar and ensure the Hugging Face toggle is enabled. Please refer to the following section to create Google Colab account.</li> </ul> <ul> <li>Make sure you are connected to a runtime. For this task, you can use the CPU as the runtime environment.</li> </ul> <ul> <li>Click on Folder and create a new folder called \"data\"</li> </ul> <ul> <li>Click on [...], select Upload</li> </ul> <ul> <li>Choose your example_llama3.json file and click Open</li> </ul> <p>Note\u00a0 Ensure that your files are saved elsewhere. This runtime's files will be deleted when its is terminated.</p> <p>Summary: So far, we have our raw dataset as example_llama3.json, enabled Hugging Face in our Colab notebook, and uploaded the data into our folder. we will be convertig the dataset (example_llama3.json) into Llama3 format and uploading on Hugging Face so it can be used for our fine tuning in the upcoming steps</p> <p> We will start by installing specific Python packages. </p> <pre><code>!pip install datasets huggingface_hub google-colab\n</code></pre> <p></p> <p>Note\u00a0 You don't need to install the packages if you are using the same notebook as in Task 8a when the datasets were created for Llama2.</p> <ul> <li> <p>The ! at the beginning is used in Jupyter notebooks or Google Colab to run shell commands. Using pip install we will install Python packages  </p> </li> <li> <p>datasets: This package is part of the Hugging Face ecosystem and provides tools for working with large datasets. It allows users to easily download, preprocess, and manage datasets, especially those used in machine learning and natural language processing (NLP).</p> </li> <li> <p>huggingface_hub: This package provides tools to interact with the Hugging Face Hub. </p> </li> <li> <p>google-colab: This package includes utilities specifically designed for Google Colab, a  Jupyter notebook environment that runs in the cloud.</p> </li> </ul> <p> Step 1: Import the required modules </p> <pre><code># Import required modules\nfrom datasets import Dataset\nfrom huggingface_hub import login\nimport os\nfrom google.colab import userdata\nimport json\nimport pandas as pd\n</code></pre> <ul> <li> <p>from datasets import Dataset: imports the Dataset class from the datasets library. The Dataset class is used to create and manipulate datasets. This library is part of the Hugging Face ecosystem and is especially useful for handling datasets for machine learning and NLP tasks.</p> </li> <li> <p>from huggingface_hub import login: This imports the login function from the huggingface_hub library. The login function is used to authenticate with the Hugging Face Hub, allowing the user to upload and manage models and datasets on the platform.</p> </li> <li> <p>import os: This imports the os module, which provides a way of using operating system-dependent functionality like reading or writing to the file system, environment variables, and more. </p> </li> <li> <p>from google.colab import userdata: imports the userdata module from the google.colab library. The google.colab library contains utilities specifically designed for use with Google Colab. </p> </li> <li> <p>import Json: will be used to Parsed JSON data</p> </li> <li> <p>import pandas as pd: used for data manipulation and analysis </p> </li> </ul> <p>Step 2:  Retrieve Hugging Face token from Colab secrets </p> <pre><code># Retrieve Hugging Face token from Colab secrets\nos.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n# Login to Hugging Face\nlogin(token=os.environ[\"HF_TOKEN\"])\n</code></pre> <p></p> <ul> <li> <p>This part of the code retrieves a previously stored Hugging Face token from Colab secrets. This token is essential for authenticating with the Hugging Face platform. Storing tokens in Colab secrets is a secure way to handle sensitive information without hardcoding it in your script.</p> </li> <li> <p>Login to Hugging Face: This line logs into Hugging Face using the retrieved token. Logging in allows our code to interact with the Hugging Face Hub, and allow us to upload datasets and models.</p> </li> </ul> <pre><code>!huggingface-cli whoami\n</code></pre> <p>Note: Confirm you are connected to your HuggingFace account </p> <p> Step 3: Standardizing and load data from example_llama3.json </p> <p><pre><code># Load JSON data from a file\nwith open('/content/data/example_llama3.json', 'r') as f:\n# with open('new_data_records.json', 'r') as f:\n    data = json.load(f)\n\n# Transform the data to match the desired format\ntransformed_data = []\nfor item in data:\n    transformed_data.append({\n        \"question\": item[\"question\"],\n        \"context\": item[\"context\"],\n        \"answer\": item[\"answer\"]\n    })\n</code></pre>  Note: How to get path for your file in GoogleColab </p> <ul> <li>Right click on your file (example_llama3.json) and select Copy Path</li> </ul> <p></p> <p> Step 3a: List and confirm data from  transformed_data. </p> <p><pre><code>print(transformed_data[0])\n</code></pre>  Output: </p> <p> Step 4: Saves the transformed data to a new JSON file, ensuring it is stored in a structured and readable format for future use.</p> <pre><code># Save the transformed data to a new JSON file\nwith open('/content/data/transformed_data.json', 'w') as f:\n    json.dump(transformed_data, f, indent=2)\n</code></pre> <p> Step 5: Converts the transformed JSON data into a pandas DataFrame and then into a Dataset object.  </p> <pre><code># Create a Dataset from the transformed data\ndf = pd.DataFrame(transformed_data)\ndataset = Dataset.from_pandas(df)\n</code></pre> <p> Step 6: Push our new Dataset to Hugging Face Hub so it can be processed and used for fine tuning </p> <p><pre><code># Push the dataset to Hugging Face Hub\ndataset.push_to_hub(\"WebexOne/llama3Data\")\n</code></pre> Complete Code - FOR REFERENCE ONLY</p> <pre><code>from datasets import Dataset\nfrom huggingface_hub import login\nimport os\nfrom google.colab import userdata\nimport json\nimport pandas as pd\n\n# Retrieve Hugging Face token from Colab secrets\nos.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n# Login to Hugging Face\nlogin(token=os.environ[\"HF_TOKEN\"])\n\n# Load JSON data from a file\nwith open('/content/data/example_llama3.json', 'r') as f:\n# with open('new_data_records.json', 'r') as f:\n    data = json.load(f)\n\n# Transform the data to match the desired format\ntransformed_data = []\nfor item in data:\n    transformed_data.append({\n        \"question\": item[\"question\"],\n        \"context\": item[\"context\"],\n        \"answer\": item[\"answer\"]\n    })\n# Save the transformed data to a new JSON file\nwith open('/content/data/transformed_data.json', 'w') as f:\n    json.dump(transformed_data, f, indent=2)\n\n# Create a Dataset from the transformed data\ndf = pd.DataFrame(transformed_data)\ndataset = Dataset.from_pandas(df)\n\n# Push the dataset to Hugging Face Hub\ndataset.push_to_hub(\"WebexOne/llama3Data\")\n</code></pre>"},{"location":"Task8b/#fine-tuning-llama3-model","title":"Fine-tuning Llama3 Model","text":"<p> NOTE: If you\u2019re serious about fine-tuning models, using a script instead of a notebook is recommended. You can easily rent GPUs on Lambda Labs, Runpod, Vast.ai e.t.c </p>"},{"location":"Task8b/#background-on-fine-tuning","title":"Background on Fine-Tuning","text":"<p>Meta has released a new series of large language models (LLMs) called Llama 3, a collection of pre-trained and instruction-tuned text-to-text models. </p> <p>Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. Both pre-trained and instruction-tuned models come with 8B and 70B parameters with a context length of 8K tokens.</p> <p>As mentioned earlier Language models (LLMs) are pretrained on extensive text corpus. The 8B model has a knowledge cutoff of March 2023, while the 70B model has a cutoff of December 2023. The models use Grouped-Query Attention (GQA), which reduces memory bandwidth and improves efficiency.</p> <p>In this lab, we will FineTune Llama3-8b parameter model with our dataset that was created earlier.</p> <p>Note: As mentioned earlier, the choice of prompt template is crucial for fine-tuning. The following prompt template will be used to fine tune Llama3-8b</p> Sample Format - Dont Copy - reference ONLY<pre><code>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt; You are a helpful AI assistant for  Collaboration&lt;|eot_id|&gt;\n\n                 &lt;|start_header_id|&gt;user&lt;|end_header_id|&gt; What can you help me with?&lt;|eot_id|&gt;\n\n                 &lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n</code></pre> <p>Note: For this tutorial, we will use a preformatted dataset (WebexOne/llama3Data) that was uploaded earlier on Hugging Face. We will apply Supervised Fine-Tuning (SFT) to a base model.</p>"},{"location":"Task8b/#guide-to-fine-tuning-llama-3","title":"Guide to Fine-Tuning Llama 3","text":"<p>In this section, we will learn about all the steps required to fine-tune the Llama 3 model with 8billion parameters on a T4 GPU with high RAM using Google Colab.</p> <p>The Colab T4 GPU has a limited 16 GB of VRAM. </p>"},{"location":"Task8b/#base-models-for-fine-tuning","title":"Base models for Fine Tuning","text":"<ul> <li>We have the option to fine-tune the Llama3 8b model (meta-llama/Meta-Llama-3-8B-Instruct), which can be found in the Hugging Face repository</li> </ul> <ul> <li>Since it is a gated repository, you'll need to provide the required information to access and use the model. Please scroll down, fill in the necessary details, and submit the form.</li> </ul> <ul> <li>Once submitted, you can check the status of your request by navigating to Settings and clicking on Gated Repositories.</li> </ul> <p> NOTE: It may take some time for the status to update from Pending to Accepted.</p> <ul> <li>Lets continue</li> </ul>"},{"location":"Task8b/#logging-into-google-collab","title":"Logging into Google Collab","text":"<ul> <li>You can either use the existing notebook from above and add a new code cell to start fine-tuning, or create a new Colab Notebook.</li> </ul> <ul> <li>Change Runtime Environment: Click the \u201cRuntime\u201d dropdown menu at the top of the Colab interface.</li> </ul> <ul> <li> <p>Select \u201cChange runtime type\u201d: This will open a dialog box where you can configure the runtime environment.</p> </li> <li> <p>Select Hardware Accelerator: From the \u201cHardware accelerator\u201d dropdown menu, choose &gt;&gt; T4 GPU and enable toggle for High RAM</p> </li> </ul> <p></p> <ul> <li>Save Settings: Click \u201cSave\u201d to apply the changes.</li> </ul> <p>Reminder: Whenever you want to copy the code in Google Colab and run it, be sure to click on + Code to add a new code cell.</p> <p></p> <p>Reminder: Click the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\" while the cell is selected.</p> <p></p>"},{"location":"Task8b/#set-up-envoirnment","title":"Set up Envoirnment","text":"<p>Step 1: Check the status of Nvidia chipset. OPTIONAL STEP </p> <pre><code>!nvidia-smi \n</code></pre> <p>Note: Remember to execute each cell individually </p> <p></p> <p>Note:  nvidia-smi stands for NVIDIA System Management Interface. It is a command-line utility that provides information about NVIDIA GPUs installed on the system. This tool is part of the NVIDIA GPU driver package. By using !nvidia-smi, you can quickly check the status and health of your NVIDIA GPUs from within a Jupyter Notebook, making it a useful tool for machine learning and data science workflows.</p> <p>Step 2: Install the required libraries. </p> <pre><code>!pip install transformers\n!pip install datasets\n!pip install accelerate\n!pip install bitsandbytes\n!pip install peft\n!pip install trl\n!pip install --upgrade jinja2\n!pip install tensorboard\n!pip install scikit-learn\n</code></pre> <p>Note: Remember to execute each cell individually </p> <p></p> <ul> <li> <p>datasets: accessing and using various datasets for NLP </p> </li> <li> <p>bitsandbytes: is a library for efficient model quantization </p> </li> <li> <p>transformers library by Hugging Face that provides pre-trained models and tools for natural language processing (NLP) tasks.</p> </li> <li> <p>trl:  Transformer Reinforcement Learning. It is a library for applying reinforcement learning techniques to transformer models. </p> </li> <li> <p>peft: package for fine tuning LLM's</p> </li> </ul> <p>Step 3: Load the necessary modules </p> <pre><code>from textwrap import dedent\nfrom typing import Dict, List\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom datasets import Dataset, load_dataset\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    TaskType,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n)\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    pipeline,\n)\nfrom dotenv import load_dotenv\nimport os\n</code></pre> <p>Note: Remember to execute each cell individually </p> <ul> <li> <p>datasets:  library, is part of the Hugging Face ecosystem, that provides tools to load and process datasets.</p> </li> <li> <p>transformers: Hugging Face Transformers library, which provides tools for working with transformer-based models.</p> </li> <li> <p>AutoModelForCausalLM: automatically selects the appropriate model architecture for causal language modeling (e.g., Llama or GPT based models).</p> </li> <li> <p>AutoTokenizer: A class that automatically selects the appropriate tokenizer for a given model.</p> </li> <li> <p>BitsAndBytesConfig: Configuration class for model quantization</p> </li> <li> <p>pipeline: The pipelines are a great and easy way to use models for inference. More info</p> </li> <li> <p>peft: Stands for Parameter-Efficient Fine-Tuning (PEFT), which includes methods like LoRA (Low-Rank Adaptation) and Qlora</p> </li> <li> <p>trl: Stands for Transformer Reinforcement Learning, a library that includes tools for fine-tuning and training models</p> </li> </ul> <p>Step 4: Retrieve Hugging Face token and set as an environment variable. </p> <p><pre><code>from google.colab import userdata\nos.environ[\"HF_TOKEN\"] = userdata.get(\"HF_TOKEN\")\n</code></pre> Note: Remember to execute each cell individually </p> <pre><code>!huggingface-cli whoami\n</code></pre> <p></p> <ul> <li>Verify to confirm that the Hugging Face CLI is correctly authenticated with your account.</li> </ul>"},{"location":"Task8b/#load-model","title":"Load model","text":"<p>Step 5: Before we start processing the data let's load our base model.  </p> <pre><code>PAD_TOKEN = \"&lt;|pad|&gt;\"\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nNEW_MODEL = \"WebexOne-lllama-3\"\n</code></pre> <ul> <li> <p>PAD_TOKEN: is set to help model not to repeat text.</p> </li> <li> <p>MODEL_NAME: specifies the name of a pre-trained model from Hugging Face that we will be using</p> </li> <li> <p>NEW_MODEL: gives a custom name for our fine-tuned version of the model.</p> </li> </ul>"},{"location":"Task8b/#load-quantization","title":"Load Quantization","text":"<p>Step 6: Loading a Quantized Language Model  </p> <pre><code>quantization_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\ntokenizer.add_special_tokens({\"pad_token\": PAD_TOKEN}) #help modles not to repeat text\ntokenizer.padding_side = \"right\"  #help modles not to repeat text\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n)\nmodel.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8) #help models not to repeat text\n</code></pre> <ul> <li> <p>Creating a configuration for the model with 4-bit precision and bfloat16 compute type. Specified the quantization type as nf4</p> </li> <li> <p>Downloading the original tokenizer from the Meta repository and adding special tokens. Used the fast tokenizer implementation for faster processing</p> </li> <li> <p>Loading the quantized model and automatically mapping it to available devices.</p> </li> <li> <p>Resizing the model's token embeddings to match the tokenizer's vocabulary and setting the padding side to 'right' to prevent the model from generating text automatically.</p> </li> <li> <p>Adding a padding token to help prevent text repetition.</p> </li> </ul> <p></p> <p>Step 7: Retrieves the configuration associated with our loaded model - OPTIONAL STEP</p> <pre><code>model.config\n</code></pre> <p></p> <ul> <li>You will be able to see the config here for Quantization</li> </ul>"},{"location":"Task8b/#load-dataset","title":"Load Dataset","text":"<p>Step 7: Loading Dataset  </p> <pre><code>dataset = load_dataset(\"WebexOne/llama3Data\")\ndataset\n</code></pre> <p></p> <p>Step 8: Extracting and converting Dataset into a pandas DataFrame  </p> <pre><code>rows = []\nfor item in dataset[\"train\"]:\n    rows.append(\n        {\n            \"question\": item[\"question\"],\n            \"context\": item[\"context\"],\n            \"answer\": item[\"answer\"],\n        }\n    )\ndf = pd.DataFrame(rows)\n</code></pre> <ul> <li> <p>Initialize an empty list called rows.  Iterate over each item in the training split of the dataset and extracted the \"question\", \"context\", and \"answer\" fields from each item.</p> </li> <li> <p>Converted our list (rows) into a pandas DataFrame (df variable).</p> </li> </ul> <p><pre><code>df\n</code></pre> </p> <p>Step 9: Lets create a function which will format each row into a list of dictionaries (messages), followed by a conversation-like structure that is suitable for training Llama3. Each message includes a role (system, user, assistant) and corresponding content.  </p> <pre><code>def format_example(row: dict):\n    prompt = dedent(\n        f\"\"\"\n        {row[\"question\"]}\n\n        Information:\n        ```\n        {row[\"context\"]}\n        ```\n        \"\"\"\n    )\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"Use only the information to answer the question. If you dont know the answer just say it.\",\n        },\n        {\"role\": \"user\", \"content\": prompt},\n        {\"role\": \"assistant\", \"content\": row[\"answer\"]},\n    ]\n    print(messages)\n    return tokenizer.apply_chat_template(messages, tokenize=False)\n</code></pre> <p>Note: tokenizer.apply_chat_template(messages, tokenize=False): Applies the chat template to the messages list using the tokenizer. The tokenize=False argument indicates that the text should not be tokenized at this stage. This step prepares the text in a format that can be used by Llama3 for training and inference. </p> <p>Step 10: Apply the Function to our DataFrame </p> <pre><code>df[\"text\"] = df.apply(format_example, axis=1)\n</code></pre> <p>Note: axis = 1 , apply the function to each row </p> <p></p> <pre><code>df.text[0]\n</code></pre> OUTPUT - For Reference ONLY<pre><code>'&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\n\\nUse only the information to answer the question. If you dont know the answer just say it.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWhat is the solution provided by Webex Calling?\\n\\nInformation:\\n```\\nWebex Calling is part of an integrated, intelligent, and modular team collaboration suite. It provides enterprise-grade PBX features, functionality, and performance previously only possible with an on-premises PBX network.\\n```&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\nWebex Calling provides enterprise-grade PBX features, functionality, and performance.&lt;|eot_id|&gt;'\n</code></pre> <p>Step 11: Counting Tokens for Each Text in a DataFrame </p> <ul> <li>Lets define a function to calculate the number of tokens in each  entry within our DataFrame. Note: Optional Step </li> </ul> <pre><code>def count_tokens(row: Dict) -&gt; int:\n    return len(\n        tokenizer(\n            row[\"text\"],\n            add_special_tokens=True,\n            return_attention_mask=False,\n        )[\"input_ids\"]\n    )\n</code></pre> <ul> <li>Lets convert text into tokens and then counts how many tokens each piece of text contains. The result is stored in a new column named token_count in the DataFrame, providing an easy reference for the length of each text in terms of tokens. </li> </ul> <pre><code>df[\"token_count\"] = df.apply(count_tokens, axis=1)\n</code></pre> <p></p>"},{"location":"Task8b/#splitting-a-dataset-into-training-validation-and-test-sets","title":"Splitting a Dataset into Training, Validation, and Test Sets","text":"<p>Step 12: We will split our dataset into three distinct subsets: a training set, a validation set, and a test set. This is a common practice in machine learning to ensure that the model is trained, validated, and tested on separate data, which helps to evaluate its performance more reliably.</p> <p>Note: For our fine-tuning, we will only use the training dataset. However, I'll demonstrate how to split and create separate datasets.</p> <pre><code>train, temp = train_test_split(df, test_size=0.2)\nval, test = train_test_split(temp, test_size=0.2)\n</code></pre> <ul> <li> <p>We performed a two-step data splitting process to divide a dataset into training, validation, and test :</p> <ul> <li> <p>First Split: The original dataset  is split into a training set (80%) and a temporary set (temp, 20%).</p> </li> <li> <p>Second Split: The temporary set is further split into a validation set (val, 16%) and a test set (test, 4%).</p> </li> </ul> </li> </ul> <p>The final result is:</p> <ul> <li>train: 80% of the data for training the model.</li> <li>val: 16% of the data for validating the model during training.</li> <li>test: 4% of the data for testing the model's performance.</li> </ul> <p>This method ensures that the model is trained, validated, and tested on different subsets of the data, which helps in preventing overfitting and provides a more accurate assessment of the model's generalization capability.</p> <p>Step 13: Calculating Dataset Proportions and Sizes </p> <p><pre><code>len(train) / len(df), len(val) / len(df), len(test) / len(df)\n\nlen(train), len(val), len(test)\n</code></pre>  Output </p> <p>(128, 26, 7)</p> <ul> <li>We calculate both the proportions and the absolute sizes of the training, validation, and test datasets relative to the original dataset   This helps in confirming that the dataset was split correctly and understanding how the data is distributed among the training, validation, and test sets.</li> </ul> <p>Step 14: Sampling and Exporting Subsets to JSON Files </p> <pre><code>train.sample(n=128).to_json(\"train.json\", orient=\"records\", lines=True)\nval.sample(n=26).to_json(\"val.json\", orient=\"records\", lines=True)\n</code></pre> <ul> <li> <p>Performs 2 functions:</p> <ul> <li>Random Sampling: It randomly selects 128 samples from the training dataset (train) (absoulte size calculated above) and 26 samples from the validation dataset (val).</li> <li>Export to JSON: The sampled data is then exported to JSON files (train.json and val.json) in a format where each row is a separate JSON object. orient=\"records\": allows the JSON output so that each object is written on a new line.</li> </ul> </li> <li> <p>This approach is very useful for creating smaller, representative datasets for testing or sharing, ensuring that the data remains in a structured and easily readable format.</p> </li> </ul> <p>Step 15: Loading a Dataset from JSON Files </p> <ul> <li>Load dataset from JSON files using the Hugging Face datasets library:</li> </ul> <pre><code>dataset = load_dataset(\n    \"json\",\n    data_files={\"train\": \"train.json\", \"validation\": \"val.json\"}\n)\n</code></pre> <p></p> <ul> <li>To view JSON data</li> </ul> <pre><code>dataset['train'][0][\"text\"]\n</code></pre> <p>Step 16: Testing Before Fine-Tuning our model</p> <ul> <li>Setting Up the Text Generation Pipeline</li> </ul> <pre><code>pipe = pipeline(\n    task=\"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=128,\n    return_full_text=False,\n)\n</code></pre> <ul> <li>We'll create a test prompt by removing the original answer from the prompt. By setting add_generation_prompt=True, the following be added at the end of prompt &lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;</li> </ul> <pre><code>def create_test_prompt(data_row):\n    prompt = dedent(\n        f\"\"\"\n        {data_row[\"question\"]}\n\n        Information:\n        ```\n        {data_row[\"context\"]}\n        ```\n        \"\"\"\n    )\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"Use only the information to answer the question. If you dont know the answer just say it.\",\n        },\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    return tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n</code></pre> <ul> <li>We will test the Model with a Sample Row from our training dataset  </li> </ul> <p><pre><code>row = dataset[\"train\"][0]\nprompt = create_test_prompt(row)\nprint(prompt)\n</code></pre> </p> <ul> <li>Generating and Comparing Output</li> </ul> <p><pre><code>%%time\noutputs = pipe(prompt)\nresponse = f\"\"\"\nanswer: {row[\"answer\"]}\nprediction: {outputs[0][\"generated_text\"]}\n\"\"\"\nprint(response)\n</code></pre> </p>"},{"location":"Task8b/#configuring-and-preparing-a-model-for-low-rank-adaptation-lora-with-quantization","title":"Configuring and Preparing a Model for Low-Rank Adaptation (LoRA) with Quantization","text":"<p>Step 17: Testing Before Fine-Tuning our model</p> <ul> <li> <p>K-bit quantization reduces the precision of the model's weights to k-bits (e.g., 4-bit or 8-bit)</p> </li> <li> <p>The LoRA configuration is applied to the model, adding low-rank matrices to the specified layers, enabling efficient fine-tuning with minimal additional parameters.</p> </li> <li> <p>We will target all the linear layers to train models like query, key , value pretty much every linear layer in the model mlp = multilayer preception.  Its like creating a small model on top of orignal model, we will only finetune teh weights of the small model.</p> </li> </ul> <pre><code>lora_config = LoraConfig(\n    r=32,\n    lora_alpha=16,\n    target_modules=[\n        \"self_attn.q_proj\",\n        \"self_attn.k_proj\",\n        \"self_attn.v_proj\",\n        \"self_attn.o_proj\",\n        \"mlp.gate_proj\",\n        \"mlp.up_proj\",\n        \"mlp.down_proj\",\n    ],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\nmodel = prepare_model_for_kbit_training(model) #  as we using Kbit for quantization\nmodel = get_peft_model(model, lora_config)\nOUTPUT_DIR = \"./l3Webex1\"\n</code></pre> <ul> <li>To view total parameters of the model  <pre><code>model.print_trainable_parameters()\n</code></pre></li> </ul> <p>Step 18: Configuring and Initializing a Supervised Fine-Tuning (SFT)</p> <ul> <li>We will setup configuration for supervised fine-tuning (SFT) of a language model and then initializes a trainer to perform the fine-tuning process. As discussed, SFT is a method used to fine-tune pre-trained language models on specific datasets with labeled data, enabling the model to perform better on specific tasks.</li> </ul> <p><pre><code>sft_config = SFTConfig(\n    output_dir=OUTPUT_DIR,\n    dataset_text_field=\"text\",\n    max_seq_length=512,\n    num_train_epochs=1,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=4,\n    optim=\"paged_adamw_8bit\", #8 bit optimmizer\n    eval_strategy=\"steps\",\n    save_steps=0.2,\n    eval_steps=0.2,\n    logging_steps=10,\n    learning_rate=2e-4,\n    fp16=True,  # or bf16=True,\n    save_strategy=\"steps\",\n    warmup_ratio=0.1,\n    save_total_limit=2,\n    lr_scheduler_type=\"constant\",\n    report_to=\"tensorboard\",\n    save_safetensors=True,\n    dataset_kwargs={\n        \"add_special_tokens\": False,  # We template with special tokens\n        \"append_concat_token\": False,  # No need to add additional separator token\n    }\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=sft_config,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"validation\"],\n    tokenizer=tokenizer,\n)\n</code></pre> </p> <p>Note: eval_dataset=dataset[\"validation\"]: The validation dataset is specified for info only As in the future this dataset can be used to evaluate the model's performance during training. </p> <p>Step 19: Executing the Training Process </p> <ul> <li>trainer.train() initiates the training process. This method triggers the training loop, where the model is fine-tuned on the training dataset (train_dataset) according to the configuration (sft_config) passed to the SFTTrainer in step18  </li> </ul> <pre><code>trainer.train()\n</code></pre> <p></p> <p>Step 20: Saving the Fine-Tuned Model </p> <pre><code>trainer.save_model(NEW_MODEL)\n</code></pre> <p>Step 21: Loading and Preparing a Fine-Tuned Model </p> <p> Note: This step may not work on Google Colab due to its resource limitations. It involves resizing token embeddings and merging a PEFT (Parameter-Efficient Fine-Tuning) model with the base model before pushing it to Hugging Face. If you're using your own servers (not Google Colab), feel free to proceed. Otherwise, this is for your information only, and you might encounter an error similar to the one shown below. </p> <p></p> <ul> <li> <p>We will merge PEFT model with the base model and unloads unnecessary components. </p> </li> <li> <p>Reloads and configures the tokenizer for consistent padding and token handling.</p> </li> </ul> <p><pre><code>import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n# Function to clear GPU cache\ndef clear_gpu_cache():\n    torch.cuda.empty_cache()\n\n# Load model with low memory usage\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"cpu\",\n)\nclear_gpu_cache()\n\n# Resize token embeddings\nmodel.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)\nclear_gpu_cache()\n\n# Load PeftModel and merge\nmodel = PeftModel.from_pretrained(model, NEW_MODEL)\nmodel = model.merge_and_unload()\nclear_gpu_cache()\n\n# Reload tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(NEW_MODEL, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Clear GPU cache one last time\nclear_gpu_cache()\n</code></pre> * Similar to the model, this command uploads the tokenizer to the same Hugging Face Hub repository that was defined at the begining of this section </p> <pre><code>model.push_to_hub(NEW_MODEL, use_temp_dir=False)\ntokenizer.push_to_hub(NEW_MODEL, use_temp_dir=False)\n</code></pre> <p></p>"},{"location":"Task8b/#use-the-uploaded-model-and-test-for-inferencing","title":"Use the uploaded model and test for inferencing","text":"<p> Note: Since we were unable to upload our fine-tuned model to the Hugging Face Hub, I have already trained and fine-tuned the model using the same datasets we discussed in the lab. You can download the model and use it for inference. </p> <ul> <li> <p>Log in to the Hugging Face Hub and navigate to your model's page (from above)</p> </li> <li> <p>Click on \"Use this model\" and then \"Transformers\".</p> </li> <li> <p>Copy the provided code snippet for using the model.</p> </li> </ul> <p>Step 22: Load model directly </p> <pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\ntokenizer = AutoTokenizer.from_pretrained(\"compile2011/WebexOne-lllama-3\")\nmodel = AutoModelForCausalLM.from_pretrained(\"compile2011/WebexOne-lllama-3\")\n</code></pre> <p></p> <pre><code>from transformers import pipeline\n\npipe = pipeline(\n    task=\"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    max_new_tokens=128,\n    return_full_text=False,\n)\n</code></pre> <pre><code>from textwrap import dedent\ndef create_test_prompt(question, context):\n    prompt = dedent(\n        f\"\"\"\n        {question}\n\n        Information:\n        ```\n        {context}\n        ```\n        \"\"\"\n    )\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"Use only the information to answer the question. If you dont know the answer just say it.\",\n        },\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    return tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n</code></pre> <pre><code>context = \"How does Webex Calling support call analytics?\"\nquestion = \"\"\nprompt = create_test_prompt(question,context)\nprint(prompt)\n</code></pre> <p></p> <p><pre><code>%%time\noutputs = pipe(prompt)\nresponse = f\"\"\"\nprediction: {outputs[0][\"generated_text\"]}\n\"\"\"\nprint(response)\n</code></pre> </p> <p>Note:You can compare the answers with those in the dataset file that was uploaded to Hugging Face.</p>"},{"location":"Task8b/#troubleshooting-best-practices","title":"Troubleshooting - Best Practices","text":"<ul> <li>Challenges are an inherent part of model training. Let's discuss some common issues and their resolutions.</li> </ul>"},{"location":"Task8b/#out-of-memory-oom-errors","title":"Out of Memory (OOM) Errors","text":"<ul> <li> <p>If you encounter an Out of Memory (OOM) error:</p> <ul> <li>Reduce Batch Size: Lowering the batch size can help fit the model into memory.</li> <li>Shorten Training Samples: Decrease the context length (e.g., max_length in tokenize()).</li> </ul> </li> </ul>"},{"location":"Task8b/#slow-training","title":"Slow Training","text":"<ul> <li> <p>If training seems sluggish:</p> <ul> <li> <p>Increase Batch Size: A larger batch size can speed up training.</p> </li> <li> <p>Use Multiple GPUs: Consider using multiple GPUs, either by purchasing or renting (e.g., on platforms like Runpod). The provided code is compatible with accelerate for multi-GPU settings. Simply launch it with  accelerate launch your_file.py  instead of  python your_file.py </p> </li> </ul> </li> </ul>"},{"location":"Task8b/#poor-model-quality","title":"Poor Model Quality","text":"<p>The quality of your model reflects the quality of your dataset. To improve model quality ensure your dataset is rich and relevant.</p>"},{"location":"Task8b/#conclusions","title":"Conclusions","text":"<p>So, we have explored the process of fine-tuning our model and performing inference with it. Now, let\u2019s say you have successfully trained or fine-tuned your model and are ready to deploy it for widespread use. However, you notice that the latency is too slow, and you want to speed up the model. There are four key methods you can employ to enhance your model's performance. While these steps are not covered in this lab, they serve as a good starting point for further investigation:</p> <ul> <li>Quantization</li> <li>Pruning</li> <li>Model or Knowledge Distillation</li> <li>Engineering Optimizations</li> </ul>"},{"location":"Task8c/","title":"Task 8c - Configuring and Fine Tuning - Using ChatGPT","text":""},{"location":"Task8c/#introduction","title":"Introduction","text":"<p>Just to summarise fine-tuning is the process of adjusting the parameters of a pre-trained large language model to a specific task or domain. Although pre-trained language models like GPT possess vast language knowledge, they lack specialization in specific areas e.g specific info about Webex. Fine-tuning addresses this limitation by allowing the model to learn from domain-specific data to make it more accurate and effective for targeted applications.</p> <p>Having already customized LLaMA 2 and LLaMA 3, let's now dive into fine-tuning another AI model GPT4. As we have seen, fine-tuning allows you to adapt a model for specific tasks, enhancing its accuracy and reliability. It can also make your prompts more efficient, saving both time and costs. </p> <p>In our demo, we\u2019re going to try something different: fine-tuning a language model using synthetic datasets created by a more advanced AI (GPT4).</p> <p>Note: it's important to note that fine-tuning this type of model involves some costs. More info for pricing available here</p>"},{"location":"Task8c/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>Fine-tuning is currently available for the following models: </li> </ul> <p>Note: Fine-tuning for GPT-4 (gpt-4-0613 and gpt-4o-*) is in an experimental access program\u2014eligible users can request access in the fine-tuning UI when creating a new fine-tuning job.</p> <ul> <li> <p>While there is no strict minimum, the quality and quantity of the data greatly affect the fine-tuning process. To fine-tune a GPT model, you are requiorange to provide a dataset with atleast 10 examples </p> </li> <li> <p>Before you start a fine-tuning job, it's crucial to check your dataset's formatting. OpenAI has provided a simple Python script that helps you spot potential errors, review token counts, and estimate the cost of fine-tuning. You can find the code here.</p> </li> </ul>"},{"location":"Task8c/#datasets-creation","title":"Datasets Creation","text":"<p>As we will be using and fine tuning GPT4  model we need to convert datasets into a uniform format. More Info at</p>"},{"location":"Task8c/#example-format","title":"Example Format","text":"Sample Format - reference ONLY<pre><code>{\"messages\": [{\"role\": \"system\", \"content\": \"WebexOne is a chatbot that provides the most relevant and up-to-date information.\"}, \n{\"role\": \"user\", \"content\": \"What's the cloud calling solution provided by Cisco\"}, \n{\"role\": \"assistant\", \"content\": \"Webex Calling, as if everyone doesn't know that already.\"}]}\n</code></pre> <ul> <li>JSON setup with three different inputs <ul> <li>System Prompt</li> <li>User/Input Prompt</li> <li>Response from Model or Output Prompt</li> </ul> </li> </ul>"},{"location":"Task8c/#datasets-creation_1","title":"Datasets Creation","text":"<p>The next step is to prepare a dataset for fine-tuning. </p> <p>In my scenario, I was able to generate synthetic dataset from the Cisco IP DECT Phone 6800 Series Deployment Guide.</p>"},{"location":"Task8c/#sample-dataset","title":"Sample Dataset","text":"<ul> <li>Below is the sample Dataset we will train our model on:</li> </ul> <p>Note: My User_Prompt and Assistant_Prompt are defined below.</p> GPT.txt<pre><code>User_Prompt: What is the first step in the quick setup and installation process for the Cisco IP DECT 6800 Series? Assistant_Prompt: The first step is to review the entire document to ensure success before starting the deployment. This initial section provides a quick step-by-step guide, but the details and context are found in the remainder of the document.\nUser_Prompt: What should be consideorange when planning the location of base stations for the DECT system? Assistant_Prompt: When planning the location of base stations for the DECT system, consider that each base station covers 50 meters (164 feet) indoors. They should be installed within this range for good coverage. The location might be impacted by objects or barriers that create radio obstruction and interference. Using the handsets' free site survey tool can help identify these potential barriers.\nUser_Prompt: What equipment is requiorange for the DECT system installation? Assistant_Prompt: The equipment requiorange includes Cisco IP DECT DBS-110 Single-Cell Base Station, Cisco IP DECT DBS-210 Multi-Cell Base Station, and Cisco IP DECT RPT-110 Repeater. Additionally, handsets such as Cisco IP DECT Phone 6823, 6825, and 6825 Ruggedized Handset are needed.\nUser_Prompt: How do you upgrade the devices for the Cisco IP DECT 6800 Series? Assistant_Prompt: Unpack and register the base stations to the call system, upgrade them to the desiorange firmware version if necessary, and select and set the desiorange configuration parameters. Unpack the handsets, remove the battery's plastic shield, and charge the battery before using the handsets.\nUser_Prompt: What are the steps to install the base station? Assistant_Prompt: Place the base stations around the site in high locations without securing them, use the site survey tool on handsets to check the location for good coverage, make a few test calls from the handsets, and then mount the base stations with the included foot stands or wall/ceiling mount kits.\nUser_Prompt: What should be done if the Ethernet switch does not provide Power over Ethernet for the base stations? Assistant_Prompt: If the Ethernet switch does not provide Power over Ethernet, a power outlet is requiorange for each base station.\nUser_Prompt: What is the recommended firmware practice for the base stations and handsets? Assistant_Prompt: Cisco's best practice recommendation is to use the most recent firmware load available on Cisco.com. Each base station may take up to one hour to upgrade.\nUser_Prompt: What is the procedure for using the site survey tool on the handsets? Assistant_Prompt: Press Menu and *47*. The IP Search screen appears. Press the Select softkey when the MAC address of the base station appears. You can manually switch between base stations and repeaters during the site survey test.\nUser_Prompt: What precautions should be taken during the handset firmware upgrade? Assistant_Prompt: The handset must be placed on the cradle until the upgrade is completed. Don\u2019t remove the handset during the upgrade to avoid permanent damage.\nUser_Prompt: What are the available handsets for the Cisco IP DECT 6800 Series? Assistant_Prompt: The available handsets are Cisco IP DECT Phone 6823 Handset, Cisco IP DECT Phone 6825 Handset, and Cisco IP DECT Phone 6825 Ruggedized Handset.\nUser_Prompt: How should the handsets be prepaorange for registration and use? Assistant_Prompt: Unpack the handsets, remove the battery's plastic shield, reinstall the battery, and place the handsets on their cradle to charge fully before using. You can register each handset with the base station before charging and then upgrade the firmware while they charge on the cradle.\nUser_Prompt: What should be done if there is more than one base station to install? Assistant_Prompt: Run the site survey to check the coverage of each base station. Make a few test calls from the handsets and then mount the base stations accordingly.\nUser_Prompt: What is one advantage of Cisco\u2019s DECT solution over Wi-Fi? Assistant_Prompt: One advantage of Cisco\u2019s DECT solution over Wi-Fi is that the DECT solution components, such as the phones and base stations, have the same architecture and are designed to work exclusively with each other.\nUser_Prompt: Can third-party DECT base stations support Cisco handsets? Assistant_Prompt: No, third-party DECT base stations, repeaters, and handsets are not supported with the Cisco DECT Phone solution. The product design ensures that the DECT base station and the handset run on compatible frequencies.\nUser_Prompt: What frequency band does the Cisco IP DECT Phone 6800 Series use to avoid interference? Assistant_Prompt: The Cisco IP DECT Phone 6800 Series works close to the 1.9 GHz frequency band to avoid interference with Bluetooth, Wi-Fi, and microwave frequencies.\nUser_Prompt: What is the recommended synchronization method for dual cell and multicell deployments? Assistant_Prompt: For dual cell and multicell deployments, LAN synchronization is recommended.\nUser_Prompt: What is the coverage radius for the DBS-110, DBS-210 base stations, and RPT-110 repeater indoors? Assistant_Prompt: The coverage radius indoors is 20-50 meters (60-164 feet) depending on the nature and quantity of any radio signal impeding items or material in the installation site.\nUser_Prompt: What is the primary use case for a single cell DECT system? Assistant_Prompt: The single cell system is mostly used in small businesses and home offices. The DBS-110 is cost-effective for a single cell system not expected to grow, while the DBS-210 is ideal for future expansion.\nUser_Prompt: What are the key factors to consider in capacity planning for DECT systems? Assistant_Prompt: Key factors include the number of simultaneous users, projected number of simultaneous calls, the area to be coveorange, building architecture, and any potential signal obstacles.\nUser_Prompt: What are the indoor and outdoor coverage radii for DECT base stations and repeaters? Assistant_Prompt: Indoors, the coverage radius is 20-50 meters (60-164 feet), and outdoors, it is 300 meters (984 feet).\nUser_Prompt: How does the signal area produced by a DECT base station behave? Assistant_Prompt: The signal area produced by the base station is omnidirectional and resembles a sphere. Mounting the base station very close to the ground or in a corner can orangeuce the optimal signal strength.\nUser_Prompt: What is the best practice for mounting base stations in a large hall? Assistant_Prompt: For a large hall, the base station should be installed in the middle of the space below the false ceiling. If the signal strength is poor in some areas, more base stations may be needed.\nUser_Prompt: What does the Cisco IP DECT 6800 Series include? Assistant_Prompt: The Cisco IP DECT 6800 Series includes the Cisco IP DECT Phone DBS-110 Single-Cell Base Station, Cisco IP DECT Phone DBS-210 Multi-Cell Base Station, Cisco IP DECT Phone RPT-110 Repeater, Cisco IP DECT Phone 6823 Handset, Cisco IP DECT Phone 6825 Handset, and Cisco IP DECT Phone 6825 Ruggedized Handset.\nUser_Prompt: What are the regulatory areas coveorange with Cisco DECT solutions? Assistant_Prompt: The regulatory areas coveorange are 1880-1900 MHz (Australia and New Zealand), 1880-1900 MHz (E.U. and APAC), and 1920-1930 MHz (U.S. and Canada).\nUser_Prompt: What technology is the Cisco IP DECT Phone 6800 Series built on? Assistant_Prompt: The Cisco IP DECT Phone 6800 Series is built on Digital Enhanced Cordless Telecommunications (DECT) technology.\nUser_Prompt: What protocol does the Cisco IP DECT Phone 6800 Series use for Voice over IP (VoIP)? Assistant_Prompt: The series uses the Session Initiation Protocol (SIP) to provide a Voice over IP (VoIP) solution over Ethernet.\nUser_Prompt: What is the frequency band used by DECT to avoid interference? Assistant_Prompt: DECT works close to the 1.9 GHz frequency band to avoid interference with Bluetooth, Wi-Fi, and microwave frequencies.\nUser_Prompt: What must be consideorange when deploying a DECT system in a service area with many rooms and obstacles? Assistant_Prompt: You may need a multicell deployment if the service area has many rooms and obstacles for good DECT signaling.\nUser_Prompt: What is the best mounting area for a base station to ensure optimal signal strength? Assistant_Prompt: The best mounting area is a location high up on the wall, on the wall, or ceiling away from any corners.\nUser_Prompt: What factors are typically included in network requirements for system planning? Assistant_Prompt: Factors include the area to be coveorange, the type or architecture of the building, the estimated traffic in each coverage area, signaling obstacles, and any new Ethernet cables or power outlets requiorange.\nUser_Prompt: What are the recommended conditions for base station density in high density areas with less interference? Assistant_Prompt: A Cisco IP DECT Base Station provides coverage of 20-50 meters (60\u2013164 feet) radius indoors, and 300 meters (984 feet) radius outdoors.\nUser_Prompt: What is the primary use of the DECT site survey tool? Assistant_Prompt: The DECT site survey tool is used to determine the best base station locations for maximum coverage.\nUser_Prompt: How many base stations and repeaters can a single cell DECT system support? Assistant_Prompt: A single cell DECT system with DBS-110 can support 6 repeaters, and with DBS-210, it can support 3 repeaters.\nUser_Prompt: What is the difference between the DBS-110 and DBS-210 base stations? Assistant_Prompt: The DBS-110 is cost-effective and ideal for single cell systems not expected to grow, while the DBS-210 is suitable for future expansion and can support more base stations.\nUser_Prompt: What is the maximum number of base stations in a multicell deployment? Assistant_Prompt: A multicell deployment can have 3 to 254 base stations.\nUser_Prompt: What synchronization method is recommended for multicell systems? Assistant_Prompt: LAN synchronization is recommended for multicell systems.\nUser_Prompt: What is the recommended distance between base stations to minimize interference in high density areas? Assistant_Prompt: Base stations should be installed at least 2 meters (6.5 feet) away from each other.\nUser_Prompt: How does the wideband codec affect channel capacity? Assistant_Prompt: The wideband codec occupies 2 channels, so if you expect more simultaneous calls, you may add more base stations to increase the channel capacity.\nUser_Prompt: What should be done if there are dead spots during the site survey? Assistant_Prompt: Move the base station towards the dead spot and repeat the site survey until you get the coverage you need, or add a second base station in multicell mode.\nUser_Prompt: What are the steps for installing the base station in a dual cell system? Assistant_Prompt: Ensure an overlap of at least 20% for seamless handover, place the base stations to maximize coverage, and consider RF attenuation in the deployment site.\nUser_Prompt: What are the key components to consider in deployment scenarios for capacity planning? Assistant_Prompt: Consider the intended number of handsets, traffic load, coverage area, and any potential interference or signal barriers.\nUser_Prompt: How can you optimize the location for installing a base station? Assistant_Prompt: Place the base station in the center with a line of sight to the coverage area, run the site survey tool, move around to locate dead spots, and adjust the base station as needed.\nUser_Prompt: What are the key elements described in the Cisco IP DECT 6800 Series Deployment Guide? Assistant_Prompt: The guide describes the elements in planning and designing a Cisco IP DECT 6800 Series Phone system, provides a simplified design procedure, and includes the network planning and deployment processes used in North America and Europe.\nUser_Prompt: What should be done if there is an interference with DECT signaling from wireless frequencies? Assistant_Prompt: DECT is resilient to interference from wireless frequencies commonly used for cell phones, Wi-Fi radios, and Bluetooth headsets. The DECT protocol adapts the channel to minimize the impact of interference.\nUser_Prompt: What are the recommended steps to optimize base station installation location? Assistant_Prompt: Place the base station in the center with a line of sight to the coverage area, run the site survey tool, move around the entire coverage area to locate any dead spots, and adjust the base station as needed.\nUser_Prompt: What is a key consideration when planning the DECT system for different regions? Assistant_Prompt: Consider the regulatory requirements for DECT frequencies in different regions. Cisco offers units set up correctly for each region, such as 1880-1900 MHz for Australia and New Zealand, 1880-1900 MHz for E.U. and APAC, and 1920-1930 MHz for U.S. and Canada.\nUser_Prompt: What are some of the radio considerations to take into account for DECT installations? Assistant_Prompt: Consider building penetration, interference sources, radio/cell range, and attenuation when planning DECT installations to ensure optimal signal strength and coverage.\nUser_Prompt: What is the importance of running the site survey tool during installation? Assistant_Prompt: Running the site survey tool helps determine the best base station locations for maximum coverage and identifies any dead spots that may require additional base stations or adjustments.\nUser_Prompt: What are some best practices for DECT deployment? Assistant_Prompt: Best practices include planning the location of base stations to avoid signal blocking obstacles, using the site survey tool to check coverage, and ensuring that base stations are mounted in high locations with a clear line of sight.\nUser_Prompt: What are the primary steps in the upgrade process for DECT devices? Assistant_Prompt: Unpack and register the base stations to the call system, upgrade them to the desiorange firmware version if necessary, and select and set the desiorange configuration parameters.\nUser_Prompt: What factors should be consideorange when ordering equipment for DECT installation? Assistant_Prompt: Consider the number of base stations, repeaters, and handsets requiorange, as well as any additional mounting kits or power adapters needed for the installation site.\nUser_Prompt: What is the recommended approach for troubleshooting DECT installations? Assistant_Prompt: Rerun the site survey tool to identify any signal issues, check the placement of base stations, and ensure that all devices are upgraded to the latest firmware version.\nUser_Prompt: What should be done if there is a need to upgrade from old firmware? Assistant_Prompt: Follow the upgrade instructions provided in the Cisco IP DECT 6800 Series Administration Guide to ensure that all devices are running the latest firmware version supported by the call control system.\nUser_Prompt: What is the role of provisioning servers in DECT installations? Assistant_Prompt: Provisioning servers are used to configure and manage the DECT devices, ensuring that they are set up correctly and receive the necessary firmware updates and configuration parameters.\nUser_Prompt: What should be done if there are radio signal blockers between base stations in a multicell deployment? Assistant_Prompt: Ensure that there is a clear line of sight between the base stations or add additional base stations to work around the signal blockers.\nUser_Prompt: What are some network considerations for DECT deployment? Assistant_Prompt: Consider the network capacity, coverage requirements, and quality of service to ensure that the DECT system meets the needs of the users and provides reliable communication.\nUser_Prompt: What are the conditions for using default settings in DECT deployment scenarios? Assistant_Prompt: Default settings can be used when there is a single physical location with no overlapping base stations, or when multiple switches are used in single or multiple physical locations with no overlapping base stations.\nUser_Prompt: What should be done if the base stations do not overlap in a single physical location? Assistant_Prompt: Ensure that the base stations are properly synchronized and that there is adequate coverage in all areas where the handsets are used.\nUser_Prompt: What are the benefits of using LAN synchronization in DECT deployments? Assistant_Prompt: LAN synchronization ensures that the base stations are properly synchronized, providing seamless handover for handsets moving between base stations and orangeucing the risk of dropped calls.\nUser_Prompt: What are the recommended audio codecs for DECT systems? Assistant_Prompt: The recommended audio codecs are G722 for wideband and G726 for narrowband. The DECT protocol supports only two codecs, so choose one wideband and one narrowband codec.\nUser_Prompt: What is the importance of matching base stations to the density requirements? Assistant_Prompt: Matching base stations to the density requirements ensures that the DECT system can support the number of users and concurrent calls, providing reliable communication and avoiding capacity issues.\nUser_Prompt: What should be consideorange when planning the capacity for DECT installations? Assistant_Prompt: Consider the number of simultaneous users, projected number of simultaneous calls, the area to be coveorange, building architecture, and any potential signal obstacles.\nUser_Prompt: What is one advantage of Cisco\u2019s DECT solution over Wi-Fi? Assistant_Prompt: One advantage of Cisco\u2019s DECT solution over Wi-Fi is that the DECT solution components, the phones and the base stations have the same architecture and are designed to work exclusively with each other.\nUser_Prompt: Can third-party DECT base stations support Cisco handsets? Assistant_Prompt: No, third-party DECT base stations, repeaters, and handsets are not supported with the Cisco DECT Phone solution. The product design ensures that the DECT base station and the handset run on compatible frequencies.\nUser_Prompt: What is the frequency band used by DECT to avoid interference? Assistant_Prompt: DECT works close to the 1.9 GHz frequency band to avoid interference with Bluetooth, Wi-Fi, and microwave frequencies.\nUser_Prompt: What is the recommended synchronization method for dual cell and multicell deployments? Assistant_Prompt: For dual cell and multicell deployments, LAN synchronization is recommended.\nUser_Prompt: What is the coverage radius for the DBS-110, DBS-210 base stations, and RPT-110 repeater indoors? Assistant_Prompt: The coverage radius indoors is 20-50 meters (60-164 feet) depending on the nature and quantity of any radio signal impeding items or material in the installation site.\nUser_Prompt: What is the primary use case for a single cell DECT system? Assistant_Prompt: The single cell system is mostly used in small businesses and home offices. The DBS-110 is cost-effective for a single cell system not expected to grow, while the DBS-210 is ideal for future expansion.\nUser_Prompt: What are the key factors to consider in capacity planning for DECT systems? Assistant_Prompt: Key factors include the number of simultaneous users, projected number of simultaneous calls, the area to be coveorange, building architecture, and any potential signal obstacles.\nUser_Prompt: What are the indoor and outdoor coverage radii for DECT base stations and repeaters? Assistant_Prompt: Indoors, the coverage radius is 20-50 meters (60-164 feet), and outdoors, it is 300 meters (984 feet).\nUser_Prompt: How does the signal area produced by a DECT base station behave? Assistant_Prompt: The signal area produced by the base station is omnidirectional and resembles a sphere. Mounting the base station very close to the ground or in a corner can orangeuce the optimal signal strength.\nUser_Prompt: What is the best practice for mounting base stations in a large hall? Assistant_Prompt: For a large hall, the base station should be installed in the middle of the space below the false ceiling. If the signal strength is poor in some areas, more base stations may be needed.\nUser_Prompt: What is the purpose of using the site survey tool? Assistant_Prompt: The site survey tool helps determine the best base station locations for maximum coverage and identifies any dead spots that might require additional coverage.\nUser_Prompt: What should be done if there are dead spots during the site survey? Assistant_Prompt: Move the base station towards the dead spot and repeat the site survey until you get the coverage you need, or add a second base station in multicell mode.\nUser_Prompt: What is the primary use of the DECT site survey tool? Assistant_Prompt: The DECT site survey tool is used to determine the best base station locations for maximum coverage.\nUser_Prompt: How many base stations and repeaters can a single cell DECT system support? Assistant_Prompt: A single cell DECT system with DBS-110 can support 6 repeaters, and with DBS-210, it can support 3 repeaters.\nUser_Prompt: What is the difference between the DBS-110 and DBS-210 base stations? Assistant_Prompt: The DBS-110 is cost-effective and ideal for single cell systems not expected to grow, while the DBS-210 is suitable for future expansion and can support more base stations.\nUser_Prompt: What is the maximum number of base stations in a multicell deployment? Assistant_Prompt: A multicell deployment can have 3 to 254 base stations.\nUser_Prompt: What synchronization method is recommended for multicell systems? Assistant_Prompt: LAN synchronization is recommended for multicell systems.\nUser_Prompt: What is the recommended distance between base stations to minimize interference in high density areas? Assistant_Prompt: Base stations should be installed at least 2 meters (6.5 feet) away from each other.\nUser_Prompt: How does the wideband codec affect channel capacity? Assistant_Prompt: The wideband codec occupies 2 channels, so if you expect more simultaneous calls, you may add more base stations to increase the channel capacity.\nUser_Prompt: What should be done if there are radio signal blockers between base stations in a multicell deployment? Assistant_Prompt: Ensure that there is a clear line of sight between the base stations or add additional base stations to work around the signal blockers.\nUser_Prompt: What is the recommended practice for base station density in high-density areas? Assistant_Prompt: In high-density areas, install base stations at least 2 meters (6.5 feet) away from each other. In the EU spectrum (20 MHz), the density should be 7 or fewer base stations in 100 square meters. In the Americas spectrum (10 MHz), the density should be 4 or fewer base stations in 100 square meters.\nUser_Prompt: What is the best practice for firmware upgrades for DECT devices? Assistant_Prompt: Cisco's best practice recommendation is to use the most recent firmware load available on Cisco.com. Each base station may take up to one hour to upgrade.\nUser_Prompt: What are the components of the Cisco IP DECT 6800 Series? Assistant_Prompt: The series includes the Cisco IP DECT Phone DBS-110 Single-Cell Base Station, Cisco IP DECT Phone DBS-210 Multi-Cell Base Station, Cisco IP DECT Phone RPT-110 Repeater, Cisco IP DECT Phone 6823 Handset, Cisco IP DECT Phone 6825 Handset, and Cisco IP DECT Phone 6825 Ruggedized Handset.\nUser_Prompt: What regulatory areas are coveorange with Cisco DECT solutions? Assistant_Prompt: The coveorange areas are 1880-1900 MHz (Australia and New Zealand), 1880-1900 MHz (E.U. and APAC), and 1920-1930 MHz (U.S. and Canada). For Mexico, Cisco offers the NA (North America) version with NOM compliance.\nUser_Prompt: What is the purpose of the DECT site survey tool? Assistant_Prompt: The site survey tool helps determine the best base station locations for maximum coverage and identifies any dead spots that may require additional base stations or adjustments.\nUser_Prompt: How should base stations be mounted in a large hall? Assistant_Prompt: In a large hall, base stations should be installed in the middle of the space below the false ceiling. If the signal strength is poor in some areas, more base stations may be needed.\nUser_Prompt: What is the recommended method for synchronizing base stations in a multicell system? Assistant_Prompt: For a multicell system, LAN synchronization is recommended to ensure seamless handover for handsets moving between base stations.\nUser_Prompt: What are the audio codec considerations for DECT systems? Assistant_Prompt: The Cisco IP DECT 6823 and 6825 handsets support G722 for wideband and G726 for narrowband. The base stations support G.711 a-law &amp; \u00b5-law, G.722.2, G.726, and G.729a/ab. Choose one wideband and one narrowband codec, as the DECT protocol supports only two codecs.\nUser_Prompt: What is the coverage radius of the DBS-110 and DBS-210 base stations indoors? Assistant_Prompt: The coverage radius indoors is 20-50 meters (60-164 feet), depending on the nature and quantity of any radio signal impeding items or material in the installation site.\nUser_Prompt: How many repeaters can be connected to a single cell DECT system? Assistant_Prompt: A single cell DECT system with DBS-110 can support up to 6 repeaters, while DBS-210 can support up to 3 repeaters.\nUser_Prompt: What is the primary use case for a single cell DECT system? Assistant_Prompt: Single cell systems are mostly used in small businesses and home offices. The DBS-110 is cost-effective for systems not expected to grow, while the DBS-210 is suitable for future expansion.\nUser_Prompt: What should be consideorange when planning the capacity for DECT installations? Assistant_Prompt: Consider the number of simultaneous users, projected number of simultaneous calls, the area to be coveorange, building architecture, and any potential signal obstacles.\nUser_Prompt: What is the importance of matching base stations to the density requirements? Assistant_Prompt: Matching base stations to the density requirements ensures that the DECT system can support the number of users and concurrent calls, providing reliable communication and avoiding capacity issues.\nUser_Prompt: What are the key elements described in the Cisco IP DECT 6800 Series Deployment Guide? Assistant_Prompt: The guide describes the elements in planning and designing a Cisco IP DECT 6800 Series Phone system, provides a simplified design procedure, and includes the network planning and deployment processes used in North America and Europe.\nUser_Prompt: What should be done if there is an interference with DECT signaling from wireless frequencies? Assistant_Prompt: DECT is resilient to interference from wireless frequencies commonly used for cell phones, Wi-Fi radios, and Bluetooth headsets. The DECT protocol adapts the channel to minimize the impact of interference.\nUser_Prompt: What are the recommended steps to optimize base station installation location? Assistant_Prompt: Place the base station in the center with a line of sight to the coverage area, run the site survey tool, move around the entire coverage area to locate any dead spots, and adjust the base station as needed.\nUser_Prompt: What is a key consideration when planning the DECT system for different regions? Assistant_Prompt: Consider the regulatory requirements for DECT frequencies in different regions. Cisco offers units set up correctly for each region, such as 1880-1900 MHz for Australia and New Zealand, 1880-1900 MHz for E.U. and APAC, and 1920-1930 MHz for U.S. and Canada.\nUser_Prompt: What are some of the radio considerations to take into account for DECT installations? Assistant_Prompt: Consider building penetration, interference sources, radio/cell range, and attenuation when planning DECT installations to ensure optimal signal strength and coverage.\nUser_Prompt: What is the importance of running the site survey tool during installation? Assistant_Prompt: Running the site survey tool helps determine the best base station locations for maximum coverage and identifies any dead spots that may require additional base stations or adjustments.\nUser_Prompt: What is the recommended practice for base station density in high-density areas? Assistant_Prompt: In high-density areas, install base stations at least 2 meters (6.5 feet) away from each other. In the EU spectrum (20 MHz), the density should be 7 or fewer base stations in 100 square meters. In the Americas spectrum (10 MHz), the density should be 4 or fewer base stations in 100 square meters.\nUser_Prompt: What is the best practice for firmware upgrades for DECT devices? Assistant_Prompt: Cisco's best practice recommendation is to use the most recent firmware load available on Cisco.com. Each base station may take up to one hour to upgrade.\nUser_Prompt: What are the components of the Cisco IP DECT 6800 Series? Assistant_Prompt: The series includes the Cisco IP DECT Phone DBS-110 Single-Cell Base Station, Cisco IP DECT Phone DBS-210 Multi-Cell Base Station, Cisco IP DECT Phone RPT-110 Repeater, Cisco IP DECT Phone 6823 Handset, Cisco IP DECT Phone 6825 Handset, and Cisco IP DECT Phone 6825 Ruggedized Handset.\nUser_Prompt: What regulatory areas are coveorange with Cisco DECT solutions? Assistant_Prompt: The coveorange areas are 1880-1900 MHz (Australia and New Zealand), 1880-1900 MHz (E.U. and APAC), and 1920-1930 MHz (U.S. and Canada). For Mexico, Cisco offers the NA (North America) version with NOM compliance.\nUser_Prompt: What is the purpose of the DECT site survey tool? Assistant_Prompt: The site survey tool helps determine the best base station locations for maximum coverage and identifies any dead spots that may require additional base stations or adjustments.\nUser_Prompt: How should base stations be mounted in a large hall? Assistant_Prompt: In a large hall, base stations should be installed in the middle of the space below the false ceiling. If the signal strength is poor in some areas, more base stations may be needed.\nUser_Prompt: What is the recommended method for synchronizing base stations in a multicell system? Assistant_Prompt: For a multicell system, LAN synchronization is recommended to ensure seamless handover for handsets moving between base stations.\nUser_Prompt: What are the audio codec considerations for DECT systems? Assistant_Prompt: The Cisco IP DECT 6823 and 6825 handsets support G722 for wideband and G726 for narrowband. The base stations support G.711 a-law &amp; \u00b5-law, G.722.2, G.726, and G.729a/ab. Choose one wideband and one narrowband codec, as the DECT protocol supports only two codecs.\nUser_Prompt: What is the coverage radius of the DBS-110 and DBS-210 base stations indoors? Assistant_Prompt: The coverage radius indoors is 20-50 meters (60-164 feet), depending on the nature and quantity of any radio signal impeding items or material in the installation site.\nUser_Prompt: How many repeaters can be connected to a single cell DECT system? Assistant_Prompt: A single cell DECT system with DBS-110 can support up to 6 repeaters, while DBS-210 can support up to 3 repeaters.\nUser_Prompt: What is the primary use case for a single cell DECT system? Assistant_Prompt: Single cell systems are mostly used in small businesses and home offices. The DBS-110 is cost-effective for systems not expected to grow, while the DBS-210 is suitable for future expansion.\nUser_Prompt: What should be consideorange when planning the capacity for DECT installations? Assistant_Prompt: Consider the number of simultaneous users, projected number of simultaneous calls, the area to be coveorange, building architecture, and any potential signal obstacles.\nUser_Prompt: What is the importance of matching base stations to the density requirements? Assistant_Prompt: Matching base stations to the density requirements ensures that the DECT system can support the number of users and concurrent calls, providing reliable communication and avoiding capacity issues.\nUser_Prompt: What are the key elements described in the Cisco IP DECT 6800 Series Deployment Guide? Assistant_Prompt: The guide describes the elements in planning and designing a Cisco IP DECT 6800 Series Phone system, provides a simplified design procedure, and includes the network planning and deployment processes used in North America and Europe.\nUser_Prompt: What should be done if there is an interference with DECT signaling from wireless frequencies? Assistant_Prompt: DECT is resilient to interference from wireless frequencies commonly used for cell phones, Wi-Fi radios, and Bluetooth headsets. The DECT protocol adapts the channel to minimize the impact of interference.\nUser_Prompt: What are the recommended steps to optimize base station installation location? Assistant_Prompt: Place the base station in the center with a line of sight to the coverage area, run the site survey tool, move around the entire coverage area to locate any dead spots, and adjust the base station as needed.\nUser_Prompt: What is a key consideration when planning the DECT system for different regions? Assistant_Prompt: Consider the regulatory requirements for DECT frequencies in different regions. Cisco offers units set up correctly for each region, such as 1880-1900 MHz for Australia and New Zealand, 1880-1900 MHz for E.U. and APAC, and 1920-1930 MHz for U.S. and Canada.\nUser_Prompt: What are some of the radio considerations to take into account for DECT installations? Assistant_Prompt: Consider building penetration, interference sources, radio/cell range, and attenuation when planning DECT installations to ensure optimal signal strength and coverage.\nUser_Prompt: What is the importance of running the site survey tool during installation? Assistant_Prompt: Running the site survey tool helps determine the best base station locations for maximum coverage and identifies any dead spots that may require additional base stations or adjustments.\nUser_Prompt: What is the purpose of using the DECT site survey tool? Assistant_Prompt: The DECT site survey tool helps determine the best base station locations for maximum coverage and identifies any dead spots that may require additional base stations or adjustments.\nUser_Prompt: What is the recommended distance between base stations in high density areas to minimize interference? Assistant_Prompt: In high density areas, base stations should be installed at least 2 meters (6.5 feet) away from each other to minimize interference.\nUser_Prompt: What should be consideorange when planning the capacity for DECT installations? Assistant_Prompt: Consider the number of simultaneous users, projected number of simultaneous calls, the area to be coveorange, building architecture, and any potential signal obstacles.\nUser_Prompt: What are the audio codec considerations for DECT systems? Assistant_Prompt: The Cisco IP DECT 6823 and 6825 handsets support G722 for wideband and G726 for narrowband. The base stations support G.711 a-law &amp; \u00b5-law, G.722.2, G.726, and G.729a/ab. Choose one wideband and one narrowband codec, as the DECT protocol supports only two codecs.\nUser_Prompt: How does the signal area produced by a DECT base station behave? Assistant_Prompt: The signal area produced by the base station is omnidirectional and resembles a sphere. Mounting the base station very close to the ground or in a corner can orangeuce the optimal signal strength.\nUser_Prompt: What should be done if there are dead spots during the site survey? Assistant_Prompt: Move the base station towards the dead spot and repeat the site survey until you get the coverage you need, or add a second base station in multicell mode.\nUser_Prompt: What is the coverage radius of the DBS-110 and DBS-210 base stations indoors? Assistant_Prompt: The coverage radius indoors is 20-50 meters (60-164 feet), depending on the nature and quantity of any radio signal impeding items or material in the installation site.\nUser_Prompt: How many repeaters can be connected to a single cell DECT system? Assistant_Prompt: A single cell DECT system with DBS-110 can support up to 6 repeaters, while DBS-210 can support up to 3 repeaters.\nUser_Prompt: What is the primary use case for a single cell DECT system? Assistant_Prompt: Single cell systems are mostly used in small businesses and home offices. The DBS-110 is cost-effective for systems not expected to grow, while the DBS-210 is suitable for future expansion.\nUser_Prompt: What are the recommended conditions for base station density in high density areas with less interference? Assistant_Prompt: A Cisco IP DECT Base Station provides coverage of 20-50 meters (60\u2013164 feet) radius indoors, and 300 meters (984 feet) radius outdoors.\nUser_Prompt: What is the recommended synchronization method for dual cell and multicell deployments? Assistant_Prompt: For dual cell and multicell deployments, LAN synchronization is recommended.\nUser_Prompt: What factors should be consideorange when ordering equipment for DECT installation? Assistant_Prompt: Consider the number of base stations, repeaters, and handsets requiorange, as well as any additional mounting kits or power adapters needed for the installation site.\nUser_Prompt: What should be done if the Ethernet switch does not provide Power over Ethernet for the base stations? Assistant_Prompt: If the Ethernet switch does not provide Power over Ethernet, a power outlet is requiorange for each base station.\nUser_Prompt: What is the best practice for mounting base stations in a large hall? Assistant_Prompt: For a large hall, the base station should be installed in the middle of the space below the false ceiling. If the signal strength is poor in some areas, more base stations may be needed.\nUser_Prompt: What should be done if there is an interference with DECT signaling from wireless frequencies? Assistant_Prompt: DECT is resilient to interference from wireless frequencies commonly used for cell phones, Wi-Fi radios, and Bluetooth headsets. The DECT protocol adapts the channel to minimize the impact of interference.\nUser_Prompt: What is the role of provisioning servers in DECT installations? Assistant_Prompt: Provisioning servers are used to configure and manage the DECT devices, ensuring that they are set up correctly and receive the necessary firmware updates and configuration parameters.\nUser_Prompt: What should be done if there are radio signal blockers between base stations in a multicell deployment? Assistant_Prompt: Ensure that there is a clear line of sight between the base stations or add additional base stations to work around the signal blockers.\nUser_Prompt: What is the importance of matching base stations to the density requirements? Assistant_Prompt: Matching base stations to the density requirements ensures that the DECT system can support the number of users and concurrent calls, providing reliable communication and avoiding capacity issues.\nUser_Prompt: What is the primary use of the DECT site survey tool? Assistant_Prompt: The DECT site survey tool is used to determine the best base station locations for maximum coverage.\nUser_Prompt: What should be consideorange when planning the DECT system for different regions? Assistant_Prompt: Consider the regulatory requirements for DECT frequencies in different regions. Cisco offers units set up correctly for each region, such as 1880-1900 MHz for Australia and New Zealand, 1880-1900 MHz for E.U. and APAC, and 1920-1930 MHz for U.S. and Canada.\nUser_Prompt: What should be consideorange when planning the capacity for DECT installations? Assistant_Prompt: Consider the number of simultaneous users, projected number of simultaneous calls, the area to be coveorange, building architecture, and any potential signal obstacles.\nUser_Prompt: What are the audio codec considerations for DECT systems? Assistant_Prompt: The Cisco IP DECT 6823 and 6825 handsets support G722 for wideband and G726 for narrowband. The base stations support G.711 a-law &amp; \u00b5-law, G.722.2, G.726, and G.729a/ab. Choose one wideband and one narrowband codec, as the DECT protocol supports only two codecs.\nUser_Prompt: How does the signal area produced by a DECT base station behave? Assistant_Prompt: The signal area produced by the base station is omnidirectional and resembles a sphere. Mounting the base station very close to the ground or in a corner can orangeuce the optimal signal strength.\nUser_Prompt: What should be done if there are dead spots during the site survey? Assistant_Prompt: Move the base station towards the dead spot and repeat the site survey until you get the coverage you need, or add a second base station in multicell mode.\nUser_Prompt: What is the coverage radius of the DBS-110 and DBS-210 base stations indoors? Assistant_Prompt: The coverage radius indoors is 20-50 meters (60-164 feet), depending on the nature and quantity of any radio signal impeding items or material in the installation site.\nUser_Prompt: How many repeaters can be connected to a single cell DECT system? Assistant_Prompt: A single cell DECT system with DBS-110 can support up to 6 repeaters, while DBS-210 can support up to 3 repeaters.\nUser_Prompt: What is the primary use case for a single cell DECT system? Assistant_Prompt: Single cell systems are mostly used in small businesses and home offices. The DBS-110 is cost-effective for systems not expected to grow, while the DBS-210 is suitable for future expansion.\nUser_Prompt: What are the recommended conditions for base station density in high density areas with less interference? Assistant_Prompt: A Cisco IP DECT Base Station provides coverage of 20-50 meters (60\u2013164 feet) radius indoors, and 300 meters (984 feet) radius outdoors.\nUser_Prompt: What is the recommended synchronization method for dual cell and multicell deployments? Assistant_Prompt: For dual cell and multicell deployments, LAN synchronization is recommended.\nUser_Prompt: What factors should be consideorange when ordering equipment for DECT installation? Assistant_Prompt: Consider the number of base stations, repeaters, and handsets requiorange, as well as any additional mounting kits or power adapters needed for the installation site.\nUser_Prompt: What should be done if the Ethernet switch does not provide Power over Ethernet for the base stations? Assistant_Prompt: If the Ethernet switch does not provide Power over Ethernet, a power outlet is requiorange for each base station.\nUser_Prompt: What is the best practice for mounting base stations in a large hall? Assistant_Prompt: For a large hall, the base station should be installed in the middle of the space below the false ceiling. If the signal strength is poor in some areas, more base stations may be needed.\nUser_Prompt: What should be done if there is an interference with DECT signaling from wireless frequencies? Assistant_Prompt: DECT is resilient to interference from wireless frequencies commonly used for cell phones, Wi-Fi radios, and Bluetooth headsets. The DECT protocol adapts the channel to minimize the impact of interference.\nUser_Prompt: What is the role of provisioning servers in DECT installations? Assistant_Prompt: Provisioning servers are used to configure and manage the DECT devices, ensuring that they are set up correctly and receive the necessary firmware updates and configuration parameters.\n</code></pre> <p>Copy and Save the file as GPT.txt as we will be using it for fine-tuning</p> <p> Note: As previously discussed, we'll utilize GPT-4 to format our dataset and convert it into a JSONL file, preparing it for fine-tuning the GPT model. </p>"},{"location":"Task8c/#uset-gpt4-to-create-our-synthetic-data","title":"Uset GPT4 to create our synthetic data","text":"<ul> <li>Browse to GPT and copy the below txt </li> </ul> copy on GPT<pre><code>Here is my fine tuning example: System Role Prompt:\nYou are creative and exceptional Webex helper. You write information about Webex Dect Phones only from the info provided. If you dont know the info just mention that you have no idea. \nBe creative and write in a very conversational way that keeps the audience engaged.\n\nUser Prompt:\nHow should the handsets be prepaorange for registration and use? \n\nSystem Response:\n\nAnswer: Unpack the handsets, remove the battery's plastic shield, reinstall the battery, and place the handsets on their cradle to charge fully before using. You can register each handset with the base station before charging and then upgrade the firmware while they charge on the cradle.\n\nExample Format:\n\nin this example our goal is to create a fine tuning strategy that can answer questions about Dect Phone , Below are the three training examples (conversations) we could create for a dataset:\n\n{\n  \"messages\": [\n    { \"role\": \"system\", \"content\": \"You are an assistant that occasionally misspells words\" },\n    { \"role\": \"user\", \"content\": \"Tell me a story.\" },\n    { \"role\": \"assistant\", \"content\": \"One day a student went to schoool.\" }\n  ]\n}\n</code></pre> <ul> <li>In the above example  we used GPT4 to create a format that matches info(format) provided by OpenAI. Based on the above format we can create our dataset</li> </ul> <p>Attaching the below example for your easy reference </p> GPT.jsonl<pre><code>{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the first step in the quick setup and installation process for the Cisco IP DECT 6800 Series?\"}, {\"role\": \"assistant\", \"content\": \"The first step is to review the entire document to ensure success before starting the deployment. This initial section provides a quick step-by-step guide, but the details and context are found in the remainder of the document.\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be consideorange when planning the location of base stations for the DECT system?\"}, {\"role\": \"assistant\", \"content\": \"When planning the location of base stations for the DECT system, consider that each base station covers 50 meters (164 feet) indoors. They should be installed within this range for good coverage. The location might be impacted by objects or barriers that create radio obstruction and interference. Using the handsets' free site survey tool can help identify these potential barriers.\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What equipment is requiorange for the DECT system installation?\"}, {\"role\": \"assistant\", \"content\": \"The equipment requiorange includes Cisco IP DECT DBS-110 Single-Cell Base Station, Cisco IP DECT DBS-210 Multi-Cell Base Station, and Cisco IP DECT RPT-110 Repeater. Additionally, handsets such as Cisco IP DECT Phone 6823, 6825, and 6825 Ruggedized Handset are needed.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know he info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"How do you upgrade the devices for the Cisco IP DECT 6800 Series?\"}, {\"role\": \"assistant\", \"content\": \"Unpack and register the base stations to the call system, upgrade them to the desiorange firmware version if necessary, and select and set the desiorange configuration parameters. Unpack the handsets, remove the battery's plastic shield, and charge the battery before using the handsets.\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the steps to install the base station?\"}, {\"role\": \"assistant\", \"content\": \"Place the base stations around the site in high locations without securing them, use the site survey tool on handsets to check the location for good coverage, make a few test calls from the handsets, and then mount the base stations with the included foot stands or wall/ceiling mount kits.\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be done if the Ethernet switch does not provide Power over Ethernet for the base stations?\"}, {\"role\": \"assistant\", \"content\": \"If the Ethernet switch does not provide Power over Ethernet, a power outlet is requiorange for each base station.\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the recommended firmware practice for the base stations and handsets?\"}, {\"role\": \"assistant\", \"content\": \"Cisco's best practice recommendation is to use the most recent firmware load available on Cisco.com. Each base station may take up to one hour to upgrade.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the procedure for using the site survey tool on the handsets?\"}, {\"role\": \"assistant\", \"content\": \"Press Menu and *47*. The IP Search screen appears. Press the Select softkey when the MAC address of the base station appears. You can manually switch between base stations and repeaters during the site survey test.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What precautions should be taken during the handset firmware upgrade?\"}, {\"role\": \"assistant\", \"content\": \"The handset must be placed on the cradle until the upgrade is completed. Don\\u2019t remove the handset during the upgrade to avoid permanent damage.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the available handsets for the Cisco IP DECT 6800 Series?\"}, {\"role\": \"assistant\", \"content\": \"The available handsets are Cisco IP DECT Phone 6823 Handset, Cisco IP DECT Phone 6825 Handset, and Cisco IP DECT Phone 6825 Ruggedized Handset.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"How should the handsets be prepaorange for registration and use?\"}, {\"role\": \"assistant\", \"content\": \"Alright, let's get those handsets ready to go! First things first\\u2014unpack each handset and find the plastic shield protecting the battery. You'll need to remove that, then pop the battery back in. Next up, place the handsets on their charging cradle to juice up. While they\\u2019re charging, you can go ahead and register each handset with the base station. And here\\u2019s a pro tip: consider updating the firmware during this time. It's a perfect multitask moment\\u2014charging and upgrading! Got any more questions or need further clarification?\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be done if there is more than one base station to install?\"}, {\"role\": \"assistant\", \"content\": \"Run the site survey to check the coverage of each base station. Make a few test calls from the handsets and then mount the base stations accordingly.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is one advantage of Cisco\\u2019s DECT solution over Wi-Fi?\"}, {\"role\": \"assistant\", \"content\": \"One advantage of Cisco\\u2019s DECT solution over Wi-Fi is that the DECT solution components, such as the phones and base stations, have the same architecture and are designed to work exclusively with each other.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"Can third-party DECT base stations support Cisco handsets?\"}, {\"role\": \"assistant\", \"content\": \"No, third-party DECT base stations, repeaters, and handsets are not supported with the Cisco DECT Phone solution. The product design ensures that the DECT base station and the handset run on compatible frequencies.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What frequency band does the Cisco IP DECT Phone 6800 Series use to avoid interference?\"}, {\"role\": \"assistant\", \"content\": \"The Cisco IP DECT Phone 6800 Series works close to the 1.9 GHz frequency band to avoid interference with Bluetooth, Wi-Fi, and microwave frequencies.\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the recommended synchronization method for dual cell and multicell deployments?\"}, {\"role\": \"assistant\", \"content\": \"For dual cell and multicell deployments, LAN synchronization is recommended.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the coverage radius for the DBS-110, DBS-210 base stations, and RPT-110 repeater indoors?\"}, {\"role\": \"assistant\", \"content\": \"The coverage radius indoors is 20-50 meters (60-164 feet) depending on the nature and quantity of any radio signal impeding items or material in the installation site.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the primary use case for a single cell DECT system?\"}, {\"role\": \"assistant\", \"content\": \"The single cell system is mostly used in small businesses and home offices. The DBS-110 is cost-effective for a single cell system not expected to grow, while the DBS-210 is ideal for future expansion.\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the key factors to consider in capacity planning for DECT systems?\"}, {\"role\": \"assistant\", \"content\": \"Key factors include the number of simultaneous users, projected number of simultaneous calls, the area to be coveorange, building architecture, and any potential signal obstacles.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the indoor and outdoor coverage radii for DECT base stations and repeaters?\"}, {\"role\": \"assistant\", \"content\": \"Indoors, the coverage radius is 20-50 meters (60-164 feet), and outdoors, it is 300 meters (984 feet).\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"How does the signal area produced by a DECT base station behave?\"}, {\"role\": \"assistant\", \"content\": \"The signal area produced by the base station is omnidirectional and resembles a sphere. Mounting the base station very close to the ground or in a corner can orangeuce the optimal signal strength.\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the best practice for mounting base stations in a large hall?\"}, {\"role\": \"assistant\", \"content\": \"For a large hall, the base station should be installed in the middle of the space below the false ceiling. If the signal strength is poor in some areas, more base stations may be needed.\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What does the Cisco IP DECT 6800 Series include?\"}, {\"role\": \"assistant\", \"content\": \"The Cisco IP DECT 6800 Series includes the Cisco IP DECT Phone DBS-110 Single-Cell Base Station, Cisco IP DECT Phone DBS-210 Multi-Cell Base Station, Cisco IP DECT Phone RPT-110 Repeater, Cisco IP DECT Phone 6823 Handset, Cisco IP DECT Phone 6825 Handset, and Cisco IP DECT Phone 6825 Ruggedized Handset.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the regulatory areas coveorange with Cisco DECT solutions?\"}, {\"role\": \"assistant\", \"content\": \"The regulatory areas coveorange are 1880-1900 MHz (Australia and New Zealand), 1880-1900 MHz (E.U. and APAC), and 1920-1930 MHz (U.S. and Canada).\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What technology is the Cisco IP DECT Phone 6800 Series built on?\"}, {\"role\": \"assistant\", \"content\": \"The Cisco IP DECT Phone 6800 Series is built on Digital Enhanced Cordless Telecommunications (DECT) technology.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What protocol does the Cisco IP DECT Phone 6800 Series use for Voice over IP (VoIP)?\"}, {\"role\": \"assistant\", \"content\": \"The series uses the Session Initiation Protocol (SIP) to provide a Voice over IP (VoIP) solution over Ethernet.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the frequency band used by DECT to avoid interference?\"}, {\"role\": \"assistant\", \"content\": \"DECT works close to the 1.9 GHz frequency band to avoid interference with Bluetooth, Wi-Fi, and microwave frequencies.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What must be consideorange when deploying a DECT system in a service area with many rooms and obstacles?\"}, {\"role\": \"assistant\", \"content\": \"You may need a multicell deployment if the service area has many rooms and obstacles for good DECT signaling.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the best mounting area for a base station to ensure optimal signal strength?\"}, {\"role\": \"assistant\", \"content\": \"The best mounting area is a location high up on the wall, on the wall, or ceiling away from any corners.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What factors are typically included in network requirements for system planning?\"}, {\"role\": \"assistant\", \"content\": \"Factors include the area to be coveorange, the type or architecture of the building, the estimated traffic in each coverage area, signaling obstacles, and any new Ethernet cables or power outlets requiorange.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the recommended conditions for base station density in high density areas with less interference?\"}, {\"role\": \"assistant\", \"content\": \"A Cisco IP DECT Base Station provides coverage of 20-50 meters (60\\u2013164 feet) radius indoors, and 300 meters (984 feet) radius outdoors.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the primary use of the DECT site survey tool?\"}, {\"role\": \"assistant\", \"content\": \"The DECT site survey tool is used to determine the best base station locations for maximum coverage.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"How many base stations and repeaters can a single cell DECT system support?\"}, {\"role\": \"assistant\", \"content\": \"A single cell DECT system with DBS-110 can support 6 repeaters, and with DBS-210, it can support 3 repeaters.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the difference between the DBS-110 and DBS-210 base stations?\"}, {\"role\": \"assistant\", \"content\": \"The DBS-110 is cost-effective and ideal for single cell systems not expected to grow, while the DBS-210 is suitable for future expansion and can support more base stations.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the maximum number of base stations in a multicell deployment?\"}, {\"role\": \"assistant\", \"content\": \"A multicell deployment can have 3 to 254 base stations.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What synchronization method is recommended for multicell systems?\"}, {\"role\": \"assistant\", \"content\": \"LAN synchronization is recommended for multicell systems.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the recommended distance between base stations to minimize interference in high density areas?\"}, {\"role\": \"assistant\", \"content\": \"Base stations should be installed at least 2 meters (6.5 feet) away from each other.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"How does the wideband codec affect channel capacity?\"}, {\"role\": \"assistant\", \"content\": \"The wideband codec occupies 2 channels, so if you expect more simultaneous calls, you may add more base stations to increase the channel capacity.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be done if there are dead spots during the site survey?\"}, {\"role\": \"assistant\", \"content\": \"Move the base station towards the dead spot and repeat the site survey until you get the coverage you need, or add a second base station in multicell mode.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the steps for installing the base station in a dual cell system?\"}, {\"role\": \"assistant\", \"content\": \"Ensure an overlap of at least 20% for seamless handover, place the base stations to maximize coverage, and consider RF attenuation in the deployment site.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the key components to consider in deployment scenarios for capacity planning?\"}, {\"role\": \"assistant\", \"content\": \"Consider the intended number of handsets, traffic load, coverage area, and any potential interference or signal barriers.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"How can you optimize the location for installing a base station?\"}, {\"role\": \"assistant\", \"content\": \"Place the base station in the center with a line of sight to the coverage area, run the site survey tool, move around to locate dead spots, and adjust the base station as needed.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the key elements described in the Cisco IP DECT 6800 Series Deployment Guide?\"}, {\"role\": \"assistant\", \"content\": \"The guide describes the elements in planning and designing a Cisco IP DECT 6800 Series Phone system, provides a simplified design procedure, and includes the network planning and deployment processes used in North America and Europe.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be done if there is an interference with DECT signaling from wireless frequencies?\"}, {\"role\": \"assistant\", \"content\": \"DECT is resilient to interference from wireless frequencies commonly used for cell phones, Wi-Fi radios, and Bluetooth headsets. The DECT protocol adapts the channel to minimize the impact of interference.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the recommended steps to optimize base station installation location?\"}, {\"role\": \"assistant\", \"content\": \"Place the base station in the center with a line of sight to the coverage area, run the site survey tool, move around the entire coverage area to locate any dead spots, and adjust the base station as needed.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is a key consideration when planning the DECT system for different regions?\"}, {\"role\": \"assistant\", \"content\": \"Consider the regulatory requirements for DECT frequencies in different regions. Cisco offers units set up correctly for each region, such as 1880-1900 MHz for Australia and New Zealand, 1880-1900 MHz for E.U. and APAC, and 1920-1930 MHz for U.S. and Canada.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are some of the radio considerations to take into account for DECT installations?\"}, {\"role\": \"assistant\", \"content\": \"Consider building penetration, interference sources, radio/cell range, and attenuation when planning DECT installations to ensure optimal signal strength and coverage.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the importance of running the site survey tool during installation?\"}, {\"role\": \"assistant\", \"content\": \"Running the site survey tool helps determine the best base station locations for maximum coverage and identifies any dead spots that may require additional base stations or adjustments.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are some best practices for DECT deployment?\"}, {\"role\": \"assistant\", \"content\": \"Best practices include planning the location of base stations to avoid signal blocking obstacles, using the site survey tool to check coverage, and ensuring that base stations are mounted in high locations with a clear line of sight.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the primary steps in the upgrade process for DECT devices?\"}, {\"role\": \"assistant\", \"content\": \"Unpack and register the base stations to the call system, upgrade them to the desiorange firmware version if necessary, and select and set the desiorange configuration parameters.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What factors should be consideorange when ordering equipment for DECT installation?\"}, {\"role\": \"assistant\", \"content\": \"Consider the number of base stations, repeaters, and handsets requiorange, as well as any additional mounting kits or power adapters needed for the installation site.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the recommended approach for troubleshooting DECT installations?\"}, {\"role\": \"assistant\", \"content\": \"Rerun the site survey tool to identify any signal issues, check the placement of base stations, and ensure that all devices are upgraded to the latest firmware version.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be done if there is a need to upgrade from old firmware?\"}, {\"role\": \"assistant\", \"content\": \"Follow the upgrade instructions provided in the Cisco IP DECT 6800 Series Administration Guide to ensure that all devices are running the latest firmware version supported by the call control system.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the role of provisioning servers in DECT installations?\"}, {\"role\": \"assistant\", \"content\": \"Provisioning servers are used to configure and manage the DECT devices, ensuring that they are set up correctly and receive the necessary firmware updates and configuration parameters.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be done if there are radio signal blockers between base stations in a multicell deployment?\"}, {\"role\": \"assistant\", \"content\": \"Ensure that there is a clear line of sight between the base stations or add additional base stations to work around the signal blockers.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are some network considerations for DECT deployment?\"}, {\"role\": \"assistant\", \"content\": \"Consider the network capacity, coverage requirements, and quality of service to ensure that the DECT system meets the needs of the users and provides reliable communication.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the conditions for using default settings in DECT deployment scenarios?\"}, {\"role\": \"assistant\", \"content\": \"Default settings can be used when there is a single physical location with no overlapping base stations, or when multiple switches are used in single or multiple physical locations with no overlapping base stations.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be done if the base stations do not overlap in a single physical location?\"}, {\"role\": \"assistant\", \"content\": \"Ensure that the base stations are properly synchronized and that there is adequate coverage in all areas where the handsets are used.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the benefits of using LAN synchronization in DECT deployments?\"}, {\"role\": \"assistant\", \"content\": \"LAN synchronization ensures that the base stations are properly synchronized, providing seamless handover for handsets moving between base stations and orangeucing the risk of dropped calls.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the recommended audio codecs for DECT systems?\"}, {\"role\": \"assistant\", \"content\": \"The recommended audio codecs are G722 for wideband and G726 for narrowband. The DECT protocol supports only two codecs, so choose one wideband and one narrowband codec.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the importance of matching base stations to the density requirements?\"}, {\"role\": \"assistant\", \"content\": \"Matching base stations to the density requirements ensures that the DECT system can support the number of users and concurrent calls, providing reliable communication and avoiding capacity issues.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be consideorange when planning the capacity for DECT installations?\"}, {\"role\": \"assistant\", \"content\": \"Consider the number of simultaneous users, projected number of simultaneous calls, the area to be coveorange, building architecture, and any potential signal obstacles.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is one advantage of Cisco\\u2019s DECT solution over Wi-Fi?\"}, {\"role\": \"assistant\", \"content\": \"One advantage of Cisco\\u2019s DECT solution over Wi-Fi is that the DECT solution components, the phones and the base stations have the same architecture and are designed to work exclusively with each other.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"Can third-party DECT base stations support Cisco handsets?\"}, {\"role\": \"assistant\", \"content\": \"No, third-party DECT base stations, repeaters, and handsets are not supported with the Cisco DECT Phone solution. The product design ensures that the DECT base station and the handset run on compatible frequencies.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the frequency band used by DECT to avoid interference?\"}, {\"role\": \"assistant\", \"content\": \"DECT works close to the 1.9 GHz frequency band to avoid interference with Bluetooth, Wi-Fi, and microwave frequencies.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the recommended synchronization method for dual cell and multicell deployments?\"}, {\"role\": \"assistant\", \"content\": \"For dual cell and multicell deployments, LAN synchronization is recommended.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the coverage radius for the DBS-110, DBS-210 base stations, and RPT-110 repeater indoors?\"}, {\"role\": \"assistant\", \"content\": \"The coverage radius indoors is 20-50 meters (60-164 feet) depending on the nature and quantity of any radio signal impeding items or material in the installation site.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the primary use case for a single cell DECT system?\"}, {\"role\": \"assistant\", \"content\": \"The single cell system is mostly used in small businesses and home offices. The DBS-110 is cost-effective for a single cell system not expected to grow, while the DBS-210 is ideal for future expansion.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the key factors to consider in capacity planning for DECT systems?\"}, {\"role\": \"assistant\", \"content\": \"Key factors include the number of simultaneous users, projected number of simultaneous calls, the area to be coveorange, building architecture, and any potential signal obstacles.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the indoor and outdoor coverage radii for DECT base stations and repeaters?\"}, {\"role\": \"assistant\", \"content\": \"Indoors, the coverage radius is 20-50 meters (60-164 feet), and outdoors, it is 300 meters (984 feet).\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"How does the signal area produced by a DECT base station behave?\"}, {\"role\": \"assistant\", \"content\": \"The signal area produced by the base station is omnidirectional and resembles a sphere. Mounting the base station very close to the ground or in a corner can orangeuce the optimal signal strength.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the best practice for mounting base stations in a large hall?\"}, {\"role\": \"assistant\", \"content\": \"For a large hall, the base station should be installed in the middle of the space below the false ceiling. If the signal strength is poor in some areas, more base stations may be needed.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the purpose of using the site survey tool?\"}, {\"role\": \"assistant\", \"content\": \"The site survey tool helps determine the best base station locations for maximum coverage and identifies any dead spots that might require additional coverage.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be done if there are dead spots during the site survey?\"}, {\"role\": \"assistant\", \"content\": \"Move the base station towards the dead spot and repeat the site survey until you get the coverage you need, or add a second base station in multicell mode.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the primary use of the DECT site survey tool?\"}, {\"role\": \"assistant\", \"content\": \"The DECT site survey tool is used to determine the best base station locations for maximum coverage.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"How many base stations and repeaters can a single cell DECT system support?\"}, {\"role\": \"assistant\", \"content\": \"A single cell DECT system with DBS-110 can support 6 repeaters, and with DBS-210, it can support 3 repeaters.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the difference between the DBS-110 and DBS-210 base stations?\"}, {\"role\": \"assistant\", \"content\": \"The DBS-110 is cost-effective and ideal for single cell systems not expected to grow, while the DBS-210 is suitable for future expansion and can support more base stations.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the maximum number of base stations in a multicell deployment?\"}, {\"role\": \"assistant\", \"content\": \"A multicell deployment can have 3 to 254 base stations.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What synchronization method is recommended for multicell systems?\"}, {\"role\": \"assistant\", \"content\": \"LAN synchronization is recommended for multicell systems.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the recommended distance between base stations to minimize interference in high density areas?\"}, {\"role\": \"assistant\", \"content\": \"Base stations should be installed at least 2 meters (6.5 feet) away from each other.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"How does the wideband codec affect channel capacity?\"}, {\"role\": \"assistant\", \"content\": \"The wideband codec occupies 2 channels, so if you expect more simultaneous calls, you may add more base stations to increase the channel capacity.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be done if there are radio signal blockers between base stations in a multicell deployment?\"}, {\"role\": \"assistant\", \"content\": \"Ensure that there is a clear line of sight between the base stations or add additional base stations to work around the signal blockers.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the recommended practice for base station density in high-density areas?\"}, {\"role\": \"assistant\", \"content\": \"In high-density areas, install base stations at least 2 meters (6.5 feet) away from each other. In the EU spectrum (20 MHz), the density should be 7 or fewer base stations in 100 square meters. In the Americas spectrum (10 MHz), the density should be 4 or fewer base stations in 100 square meters.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the best practice for firmware upgrades for DECT devices?\"}, {\"role\": \"assistant\", \"content\": \"Cisco's best practice recommendation is to use the most recent firmware load available on Cisco.com. Each base station may take up to one hour to upgrade.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the components of the Cisco IP DECT 6800 Series?\"}, {\"role\": \"assistant\", \"content\": \"The series includes the Cisco IP DECT Phone DBS-110 Single-Cell Base Station, Cisco IP DECT Phone DBS-210 Multi-Cell Base Station, Cisco IP DECT Phone RPT-110 Repeater, Cisco IP DECT Phone 6823 Handset, Cisco IP DECT Phone 6825 Handset, and Cisco IP DECT Phone 6825 Ruggedized Handset.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What regulatory areas are coveorange with Cisco DECT solutions?\"}, {\"role\": \"assistant\", \"content\": \"The coveorange areas are 1880-1900 MHz (Australia and New Zealand), 1880-1900 MHz (E.U. and APAC), and 1920-1930 MHz (U.S. and Canada). For Mexico, Cisco offers the NA (North America) version with NOM compliance.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the purpose of the DECT site survey tool?\"}, {\"role\": \"assistant\", \"content\": \"The site survey tool helps determine the best base station locations for maximum coverage and identifies any dead spots that may require additional base stations or adjustments.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"How should base stations be mounted in a large hall?\"}, {\"role\": \"assistant\", \"content\": \"In a large hall, base stations should be installed in the middle of the space below the false ceiling. If the signal strength is poor in some areas, more base stations may be needed.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the recommended method for synchronizing base stations in a multicell system?\"}, {\"role\": \"assistant\", \"content\": \"For a multicell system, LAN synchronization is recommended to ensure seamless handover for handsets moving between base stations.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the audio codec considerations for DECT systems?\"}, {\"role\": \"assistant\", \"content\": \"The Cisco IP DECT 6823 and 6825 handsets support G722 for wideband and G726 for narrowband. The base stations support G.711 a-law &amp; \\u00b5-law, G.722.2, G.726, and G.729a/ab. Choose one wideband and one narrowband codec, as the DECT protocol supports only two codecs.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the coverage radius of the DBS-110 and DBS-210 base stations indoors?\"}, {\"role\": \"assistant\", \"content\": \"The coverage radius indoors is 20-50 meters (60-164 feet), depending on the nature and quantity of any radio signal impeding items or material in the installation site.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"How many repeaters can be connected to a single cell DECT system?\"}, {\"role\": \"assistant\", \"content\": \"A single cell DECT system with DBS-110 can support up to 6 repeaters, while DBS-210 can support up to 3 repeaters.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the primary use case for a single cell DECT system?\"}, {\"role\": \"assistant\", \"content\": \"Single cell systems are mostly used in small businesses and home offices. The DBS-110 is cost-effective for systems not expected to grow, while the DBS-210 is suitable for future expansion.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be consideorange when planning the capacity for DECT installations?\"}, {\"role\": \"assistant\", \"content\": \"Consider the number of simultaneous users, projected number of simultaneous calls, the area to be coveorange, building architecture, and any potential signal obstacles.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the importance of matching base stations to the density requirements?\"}, {\"role\": \"assistant\", \"content\": \"Matching base stations to the density requirements ensures that the DECT system can support the number of users and concurrent calls, providing reliable communication and avoiding capacity issues.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the key elements described in the Cisco IP DECT 6800 Series Deployment Guide?\"}, {\"role\": \"assistant\", \"content\": \"The guide describes the elements in planning and designing a Cisco IP DECT 6800 Series Phone system, provides a simplified design procedure, and includes the network planning and deployment processes used in North America and Europe.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be done if there is an interference with DECT signaling from wireless frequencies?\"}, {\"role\": \"assistant\", \"content\": \"DECT is resilient to interference from wireless frequencies commonly used for cell phones, Wi-Fi radios, and Bluetooth headsets. The DECT protocol adapts the channel to minimize the impact of interference.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the recommended steps to optimize base station installation location?\"}, {\"role\": \"assistant\", \"content\": \"Place the base station in the center with a line of sight to the coverage area, run the site survey tool, move around the entire coverage area to locate any dead spots, and adjust the base station as needed.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is a key consideration when planning the DECT system for different regions?\"}, {\"role\": \"assistant\", \"content\": \"Consider the regulatory requirements for DECT frequencies in different regions. Cisco offers units set up correctly for each region, such as 1880-1900 MHz for Australia and New Zealand, 1880-1900 MHz for E.U. and APAC, and 1920-1930 MHz for U.S. and Canada.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are some of the radio considerations to take into account for DECT installations?\"}, {\"role\": \"assistant\", \"content\": \"Consider building penetration, interference sources, radio/cell range, and attenuation when planning DECT installations to ensure optimal signal strength and coverage.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the importance of running the site survey tool during installation?\"}, {\"role\": \"assistant\", \"content\": \"Running the site survey tool helps determine the best base station locations for maximum coverage and identifies any dead spots that may require additional base stations or adjustments.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the recommended practice for base station density in high-density areas?\"}, {\"role\": \"assistant\", \"content\": \"In high-density areas, install base stations at least 2 meters (6.5 feet) away from each other. In the EU spectrum (20 MHz), the density should be 7 or fewer base stations in 100 square meters. In the Americas spectrum (10 MHz), the density should be 4 or fewer base stations in 100 square meters.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the best practice for firmware upgrades for DECT devices?\"}, {\"role\": \"assistant\", \"content\": \"Cisco's best practice recommendation is to use the most recent firmware load available on Cisco.com. Each base station may take up to one hour to upgrade.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the components of the Cisco IP DECT 6800 Series?\"}, {\"role\": \"assistant\", \"content\": \"The series includes the Cisco IP DECT Phone DBS-110 Single-Cell Base Station, Cisco IP DECT Phone DBS-210 Multi-Cell Base Station, Cisco IP DECT Phone RPT-110 Repeater, Cisco IP DECT Phone 6823 Handset, Cisco IP DECT Phone 6825 Handset, and Cisco IP DECT Phone 6825 Ruggedized Handset.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What regulatory areas are coveorange with Cisco DECT solutions?\"}, {\"role\": \"assistant\", \"content\": \"The coveorange areas are 1880-1900 MHz (Australia and New Zealand), 1880-1900 MHz (E.U. and APAC), and 1920-1930 MHz (U.S. and Canada). For Mexico, Cisco offers the NA (North America) version with NOM compliance.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the purpose of the DECT site survey tool?\"}, {\"role\": \"assistant\", \"content\": \"The site survey tool helps determine the best base station locations for maximum coverage and identifies any dead spots that may require additional base stations or adjustments.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"How should base stations be mounted in a large hall?\"}, {\"role\": \"assistant\", \"content\": \"In a large hall, base stations should be installed in the middle of the space below the false ceiling. If the signal strength is poor in some areas, more base stations may be needed.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the recommended method for synchronizing base stations in a multicell system?\"}, {\"role\": \"assistant\", \"content\": \"For a multicell system, LAN synchronization is recommended to ensure seamless handover for handsets moving between base stations.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the audio codec considerations for DECT systems?\"}, {\"role\": \"assistant\", \"content\": \"The Cisco IP DECT 6823 and 6825 handsets support G722 for wideband and G726 for narrowband. The base stations support G.711 a-law &amp; \\u00b5-law, G.722.2, G.726, and G.729a/ab. Choose one wideband and one narrowband codec, as the DECT protocol supports only two codecs.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the coverage radius of the DBS-110 and DBS-210 base stations indoors?\"}, {\"role\": \"assistant\", \"content\": \"The coverage radius indoors is 20-50 meters (60-164 feet), depending on the nature and quantity of any radio signal impeding items or material in the installation site.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"How many repeaters can be connected to a single cell DECT system?\"}, {\"role\": \"assistant\", \"content\": \"A single cell DECT system with DBS-110 can support up to 6 repeaters, while DBS-210 can support up to 3 repeaters.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the primary use case for a single cell DECT system?\"}, {\"role\": \"assistant\", \"content\": \"Single cell systems are mostly used in small businesses and home offices. The DBS-110 is cost-effective for systems not expected to grow, while the DBS-210 is suitable for future expansion.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be consideorange when planning the capacity for DECT installations?\"}, {\"role\": \"assistant\", \"content\": \"Consider the number of simultaneous users, projected number of simultaneous calls, the area to be coveorange, building architecture, and any potential signal obstacles.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the importance of matching base stations to the density requirements?\"}, {\"role\": \"assistant\", \"content\": \"Matching base stations to the density requirements ensures that the DECT system can support the number of users and concurrent calls, providing reliable communication and avoiding capacity issues.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the key elements described in the Cisco IP DECT 6800 Series Deployment Guide?\"}, {\"role\": \"assistant\", \"content\": \"The guide describes the elements in planning and designing a Cisco IP DECT 6800 Series Phone system, provides a simplified design procedure, and includes the network planning and deployment processes used in North America and Europe.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be done if there is an interference with DECT signaling from wireless frequencies?\"}, {\"role\": \"assistant\", \"content\": \"DECT is resilient to interference from wireless frequencies commonly used for cell phones, Wi-Fi radios, and Bluetooth headsets. The DECT protocol adapts the channel to minimize the impact of interference.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the recommended steps to optimize base station installation location?\"}, {\"role\": \"assistant\", \"content\": \"Place the base station in the center with a line of sight to the coverage area, run the site survey tool, move around the entire coverage area to locate any dead spots, and adjust the base station as needed.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is a key consideration when planning the DECT system for different regions?\"}, {\"role\": \"assistant\", \"content\": \"Consider the regulatory requirements for DECT frequencies in different regions. Cisco offers units set up correctly for each region, such as 1880-1900 MHz for Australia and New Zealand, 1880-1900 MHz for E.U. and APAC, and 1920-1930 MHz for U.S. and Canada.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are some of the radio considerations to take into account for DECT installations?\"}, {\"role\": \"assistant\", \"content\": \"Consider building penetration, interference sources, radio/cell range, and attenuation when planning DECT installations to ensure optimal signal strength and coverage.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the importance of running the site survey tool during installation?\"}, {\"role\": \"assistant\", \"content\": \"Running the site survey tool helps determine the best base station locations for maximum coverage and identifies any dead spots that may require additional base stations or adjustments.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the purpose of using the DECT site survey tool?\"}, {\"role\": \"assistant\", \"content\": \"The DECT site survey tool helps determine the best base station locations for maximum coverage and identifies any dead spots that may require additional base stations or adjustments.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the recommended distance between base stations in high density areas to minimize interference?\"}, {\"role\": \"assistant\", \"content\": \"In high density areas, base stations should be installed at least 2 meters (6.5 feet) away from each other to minimize interference.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be consideorange when planning the capacity for DECT installations?\"}, {\"role\": \"assistant\", \"content\": \"Consider the number of simultaneous users, projected number of simultaneous calls, the area to be coveorange, building architecture, and any potential signal obstacles.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the audio codec considerations for DECT systems?\"}, {\"role\": \"assistant\", \"content\": \"The Cisco IP DECT 6823 and 6825 handsets support G722 for wideband and G726 for narrowband. The base stations support G.711 a-law &amp; \\u00b5-law, G.722.2, G.726, and G.729a/ab. Choose one wideband and one narrowband codec, as the DECT protocol supports only two codecs.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"How does the signal area produced by a DECT base station behave?\"}, {\"role\": \"assistant\", \"content\": \"The signal area produced by the base station is omnidirectional and resembles a sphere. Mounting the base station very close to the ground or in a corner can orangeuce the optimal signal strength.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be done if there are dead spots during the site survey?\"}, {\"role\": \"assistant\", \"content\": \"Move the base station towards the dead spot and repeat the site survey until you get the coverage you need, or add a second base station in multicell mode.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the coverage radius of the DBS-110 and DBS-210 base stations indoors?\"}, {\"role\": \"assistant\", \"content\": \"The coverage radius indoors is 20-50 meters (60-164 feet), depending on the nature and quantity of any radio signal impeding items or material in the installation site.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"How many repeaters can be connected to a single cell DECT system?\"}, {\"role\": \"assistant\", \"content\": \"A single cell DECT system with DBS-110 can support up to 6 repeaters, while DBS-210 can support up to 3 repeaters.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the primary use case for a single cell DECT system?\"}, {\"role\": \"assistant\", \"content\": \"Single cell systems are mostly used in small businesses and home offices. The DBS-110 is cost-effective for systems not expected to grow, while the DBS-210 is suitable for future expansion.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the recommended conditions for base station density in high density areas with less interference?\"}, {\"role\": \"assistant\", \"content\": \"A Cisco IP DECT Base Station provides coverage of 20-50 meters (60\\u2013164 feet) radius indoors, and 300 meters (984 feet) radius outdoors.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the recommended synchronization method for dual cell and multicell deployments?\"}, {\"role\": \"assistant\", \"content\": \"For dual cell and multicell deployments, LAN synchronization is recommended.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What factors should be consideorange when ordering equipment for DECT installation?\"}, {\"role\": \"assistant\", \"content\": \"Consider the number of base stations, repeaters, and handsets requiorange, as well as any additional mounting kits or power adapters needed for the installation site.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be done if the Ethernet switch does not provide Power over Ethernet for the base stations?\"}, {\"role\": \"assistant\", \"content\": \"If the Ethernet switch does not provide Power over Ethernet, a power outlet is requiorange for each base station.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the best practice for mounting base stations in a large hall?\"}, {\"role\": \"assistant\", \"content\": \"For a large hall, the base station should be installed in the middle of the space below the false ceiling. If the signal strength is poor in some areas, more base stations may be needed.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be done if there is an interference with DECT signaling from wireless frequencies?\"}, {\"role\": \"assistant\", \"content\": \"DECT is resilient to interference from wireless frequencies commonly used for cell phones, Wi-Fi radios, and Bluetooth headsets. The DECT protocol adapts the channel to minimize the impact of interference.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the role of provisioning servers in DECT installations?\"}, {\"role\": \"assistant\", \"content\": \"Provisioning servers are used to configure and manage the DECT devices, ensuring that they are set up correctly and receive the necessary firmware updates and configuration parameters.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be done if there are radio signal blockers between base stations in a multicell deployment?\"}, {\"role\": \"assistant\", \"content\": \"Ensure that there is a clear line of sight between the base stations or add additional base stations to work around the signal blockers.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the importance of matching base stations to the density requirements?\"}, {\"role\": \"assistant\", \"content\": \"Matching base stations to the density requirements ensures that the DECT system can support the number of users and concurrent calls, providing reliable communication and avoiding capacity issues.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the primary use of the DECT site survey tool?\"}, {\"role\": \"assistant\", \"content\": \"The DECT site survey tool is used to determine the best base station locations for maximum coverage.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be consideorange when planning the DECT system for different regions?\"}, {\"role\": \"assistant\", \"content\": \"Consider the regulatory requirements for DECT frequencies in different regions. Cisco offers units set up correctly for each region, such as 1880-1900 MHz for Australia and New Zealand, 1880-1900 MHz for E.U. and APAC, and 1920-1930 MHz for U.S. and Canada.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be consideorange when planning the capacity for DECT installations?\"}, {\"role\": \"assistant\", \"content\": \"Consider the number of simultaneous users, projected number of simultaneous calls, the area to be coveorange, building architecture, and any potential signal obstacles.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the audio codec considerations for DECT systems?\"}, {\"role\": \"assistant\", \"content\": \"The Cisco IP DECT 6823 and 6825 handsets support G722 for wideband and G726 for narrowband. The base stations support G.711 a-law &amp; \\u00b5-law, G.722.2, G.726, and G.729a/ab. Choose one wideband and one narrowband codec, as the DECT protocol supports only two codecs.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"How does the signal area produced by a DECT base station behave?\"}, {\"role\": \"assistant\", \"content\": \"The signal area produced by the base station is omnidirectional and resembles a sphere. Mounting the base station very close to the ground or in a corner can orangeuce the optimal signal strength.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be done if there are dead spots during the site survey?\"}, {\"role\": \"assistant\", \"content\": \"Move the base station towards the dead spot and repeat the site survey until you get the coverage you need, or add a second base station in multicell mode.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the coverage radius of the DBS-110 and DBS-210 base stations indoors?\"}, {\"role\": \"assistant\", \"content\": \"The coverage radius indoors is 20-50 meters (60-164 feet), depending on the nature and quantity of any radio signal impeding items or material in the installation site.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"How many repeaters can be connected to a single cell DECT system?\"}, {\"role\": \"assistant\", \"content\": \"A single cell DECT system with DBS-110 can support up to 6 repeaters, while DBS-210 can support up to 3 repeaters.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the primary use case for a single cell DECT system?\"}, {\"role\": \"assistant\", \"content\": \"Single cell systems are mostly used in small businesses and home offices. The DBS-110 is cost-effective for systems not expected to grow, while the DBS-210 is suitable for future expansion.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What are the recommended conditions for base station density in high density areas with less interference?\"}, {\"role\": \"assistant\", \"content\": \"A Cisco IP DECT Base Station provides coverage of 20-50 meters (60\\u2013164 feet) radius indoors, and 300 meters (984 feet) radius outdoors.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the recommended synchronization method for dual cell and multicell deployments?\"}, {\"role\": \"assistant\", \"content\": \"For dual cell and multicell deployments, LAN synchronization is recommended.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What factors should be consideorange when ordering equipment for DECT installation?\"}, {\"role\": \"assistant\", \"content\": \"Consider the number of base stations, repeaters, and handsets requiorange, as well as any additional mounting kits or power adapters needed for the installation site.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be done if the Ethernet switch does not provide Power over Ethernet for the base stations?\"}, {\"role\": \"assistant\", \"content\": \"If the Ethernet switch does not provide Power over Ethernet, a power outlet is requiorange for each base station.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What is the best practice for mounting base stations in a large hall?\"}, {\"role\": \"assistant\", \"content\": \"For a large hall, the base station should be installed in the middle of the space below the false ceiling. If the signal strength is poor in some areas, more base stations may be needed.\"}]} \n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"What should be done if there is an interference with DECT signaling from wireless frequencies?\"}, {\"role\": \"assistant\", \"content\": \"DECT is resilient to interference from wireless frequencies commonly used for cell phones, Wi-Fi radios, and Bluetooth headsets. The DECT protocol adapts the channel to minimize the impact of interference.\"}]}\n{\"messages\": [{\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"}, {\"role\": \"user\", \"content\": \"Who is Omer Ilyas?\"}, {\"role\": \"assistant\", \"content\": \"he is a TME@webex and the author of this lab.\"}]}\n</code></pre> <p>Download JSONL file</p> <p>Copy and Save the file as GPT.jsonl</p> <p> Note: Total of 153 examples we have created. </p> <p>Note: I\u2019ve included a unique entry to ensure our fine-tuned model can learn from it, such as, \"Who is Omer Ilyas? </p>"},{"location":"Task8c/#create-a-fine-tuning-job","title":"Create a fine-tuning job","text":"<p>Once our data is prepaorange and gatheorange in the correct format, the next step is uploading our gpt.jsonl file to OpenAI using a Python script or via WebUI. </p> <ul> <li>In the below example we will show you how to use WebUI to finetune your model</li> </ul> <p>Note: You'll need to log in to your OpenAI account to perform fine-tuning. </p> <ul> <li>Click Fine-tuning &gt; Create</li> </ul> <p></p> <ul> <li>Click Fine-tuning &gt; Create</li> </ul> <p></p> <ul> <li> <p>Model name (Requiorange) is the name of the model you want to fine-tune (gpt-4o-mini, gpt-3.5-turbo, babbage-002, davinci-002, or an existing fine-tuned model)</p> </li> <li> <p>Training_file (Requiorange) is our JSONL dataset</p> </li> <li> <p>Suffix:A string of up to 18 characters that will be added to our fine-tuned model name.</p> </li> </ul> <p>Note: After you've started a fine-tuning job, it may take some time to complete. Our job may be queued behind other jobs in OpenAI system, and training a model can take minutes or hours depending on the model and dataset size.</p> <p> Remember: In addition to creating a final fine-tuned model at the end of each fine-tuning job, OpenAI will create one full model checkpoint for you at the end of each training epoch. These checkpoints are themselves full models that can be used within our completions and chat-completions endpoints.</p> <p></p> <ul> <li>Job ID: This is a unique identifier for the fine-tuning job.</li> <li>Suffix: webexone is the suffix we chose for this job, likely to distinguish it from others.</li> <li>Base model: The fine-tuning was done on the gpt-4o-mini-2024-07-18 model.</li> <li>Output model: The result of the fine-tuning is saved as ft:gpt-4o-mini-2024-07-18:personal:webexone:9vVp3n9J.</li> <li>Created at: The fine-tuning job was created on August 12, 2024, at 21:11.</li> <li>Trained tokens: The model was trained on 22,836 tokens.</li> <li>Epochs: The fine-tuning process involved 3 epochs, meaning the dataset was cycled through three times.</li> <li>Batch size: The batch size was set to 1, indicating that one example was processed at a time during training.</li> <li>Learning rate (LR) multiplier: The learning rate multiplier was set to 1.8, which adjusts the learning rate during training.</li> <li>Seed: A seed value of 3 was used to create randomness in the training process. Ensuring the reproducibility of the fine-tuning process.</li> <li>Checkpoints: The process included checkpoints at steps 62, 124, and the final step, allowing you to save intermediate models during training.</li> <li>Training loss: The final training loss achieved was 0.0010, which is a measure of how well the model is fitting the data.</li> </ul> <p>Once the fine-tuning job is complete, you can start using the fine-tuned model either through the API or via the playground available on OpenAI Platform</p>"},{"location":"Task8c/#using-the-fine-tuned-model","title":"Using the fine-tuned model","text":"<ul> <li>You'll notice a new fine-tune option under the Model dropdown and is now available for selection. Let's give it a try!</li> </ul> <ul> <li>Enter the system prompt we created earlier</li> </ul> <ul> <li>In the user tab enter your input to see the response from your fine-tune model </li> </ul> <ul> <li>Our responses</li> </ul> <p>Note: it's important to note that fine-tuning this type of model involves some costs. More info for pricing available here</p>"},{"location":"Task8c/#step-by-step-guide-to-fine-tuning-using-python-optional","title":"Step-by-Step Guide To Fine-tuning Using Python (Optional)","text":""},{"location":"Task8c/#set-openai-token","title":"Set OpenAI token","text":"<p>Note: First, create an account from the OpenAI official website.</p> <ul> <li>Create a new project API key by browsing to API Keys web page. Select Create new secret key. The API key is automatically generated. Save the APi Key as we will be using it in the later steps .</li> </ul> <p></p>"},{"location":"Task8c/#prepare-the-training-data","title":"Prepare the training data","text":"<ul> <li>We have already prepaorange the dataset in the previous step. Be sure to copy and save the file locally as GPT.jsonl.</li> </ul>"},{"location":"Task8c/#prepare-your-python-envoirnment-using-hugging-face","title":"Prepare your python envoirnment using Hugging Face","text":"<ul> <li>Create a new Colab Notebook.</li> </ul> <ul> <li>Connect to the default Runtime Environment. For this task, you can use the CPU as the runtime environment.</li> </ul> <p>Reminder: Whenever you want to copy the code in Google Colab and run it, be sure to click on + Code to add a new code cell.</p> <p>Reminder: Click the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\" while the cell is selected.</p> <ul> <li>Within the Google Colab notebook navigate to the new \u201cSecrets\u201d section in the sidebar.</li> </ul> <p></p> <ul> <li> <p>Click on \u201cAdd a new secret.\u201d Enter the name example: OPENAI_API_KEY and value of the secret(the API key created above). Note: The name is permanent once set. </p> </li> <li> <p>The list of secrets is global across all your notebooks.</p> </li> <li> <p>Use the \u201cNotebook access\u201d toggle to grant or revoke access to a secret for each notebook.</p> </li> </ul> <p></p>"},{"location":"Task8c/#incorporating-secrets-into-your-code-we-will-use-it-later-in-our-lab","title":"Incorporating Secrets into Your Code - We will use it later in our lab","text":"<ul> <li>To use a secret in your notebook, use the following code snippet</li> </ul> <pre><code>from google.colab import userdata\nmy_secret_key = userdata.get('OPENAI_API_KEY')\n</code></pre> <ul> <li>Upload the JSONL file on HuggingFace. Click on Folder and create a new folder called \"data\"</li> </ul> <ul> <li>Click on [...], select Upload</li> </ul> <ul> <li>Choose your GPT.jsonl file and click Open</li> </ul> <p>Note\u00a0 Ensure that your files are saved elsewhere. This runtime's files will be deleted when it is terminated.</p> <ul> <li>Install openai library</li> </ul> <pre><code>!pip install openai\n</code></pre> <p> OUTPUT</p> <p></p> <ul> <li>Setting up an API token</li> </ul> <pre><code>import os\nfrom google.colab import userdata\n# Set the OPENAI_API_KEY environment variable\nos.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n</code></pre> <p></p> <ul> <li>Uploading the training File. Your training file must be in jsonl format. Once you've uploaded the file, processing might take a while. The maximum size for file uploads is 1 GB. To upload a file on the OpenAI server:</li> </ul> <pre><code>from openai import OpenAI\nclient = OpenAI()\nresponse = client.files.create(\n  file=open(\"/content/TrainingData/GPT.jsonl\", \"rb\"),\n  purpose=\"fine-tune\"\n)\nfile_id= response.id\nprint(f\"file uploaded successfully with id: {file_id}\")\n</code></pre> <p> OUTPUT</p> <p></p> <p>You can also access the uploaded file by navigating to the OpenAI Dashboard</p> <p> OUTPUT </p> <ul> <li>Create a fine-tune model. After ensuring the file has been successfully uploaded, the next step is to create a fine-tuning job. The file_id  was returned when the training file was uploaded to the OpenAI API. To start a fine-tuning job:</li> </ul> <p><pre><code>model = \"gpt-4o-mini-2024-07-18\"\nresponse = client.fine_tuning.jobs.create(\n  training_file=file_id,\n  model=model \n)\njob_id = response.id\nprint(f\"Fine Tuning job created  successfully with Job id: {job_id}\")\n</code></pre> </p> <pre><code>* You can view the fine-tuning job by navigating to the [OpenAI Dashboard](https://platform.openai.com/finetune)\n</code></pre> <p> OUTPUT</p> <p></p> <pre><code>* You can also view the  status of fine-tuning by entering the below command:\n</code></pre> <p><pre><code># Retrieve the state of a fine-tune\nclient.fine_tuning.jobs.retrieve(job_id)\n</code></pre>  OUTPUT</p> <p></p> <ul> <li>After successfully completing the fine-tuning process in the previous step, the job details will include a fine_tuned_model field, which shows the name of the tuned model. You can use this model name in an API call to get a response from the model we just fine-tuned.</li> </ul> <p> Note: The model ID in the following step will be different for your specific model.  </p> <pre><code>completion = client.chat.completions.create(\n  model=\"ft:gpt-4o-mini-2024-07-18:personal::9vWS0JQF\",\n\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are a creative and exceptional Webex helper. You provide information about Webex Dect Phones only from the info provided. If you don't know the info, just mention that you have no idea. Be creative and write in a very conversational way that keeps the audience engaged.\"},\n    {\"role\": \"user\", \"content\": \"who is Omer Ilyas?\"}\n  ]\n)\nprint(completion.choices[0].message)\n</code></pre> <p> OUTPUT </p> <p></p>"},{"location":"Task8c/#conclusion","title":"Conclusion","text":"<p>In this session, we've exploorange the capabilities of OpenAI's models and walked through the process of fine-tuning them to enhance their performance for specific tasks. We've also coveorange the key requirements needed to successfully fine-tune a model. Fine-tuning allows us to adjust the language model\u2019s responses, making them more effective at addressing particular tasks, styles, or domains with greater accuracy and efficiency.</p> <p>Unlike open-source models like LLaMA, Mistral, or Stable Diffusion, where fine-tuning can be more complex, the process for fine-tuning GPT is straightforward, involving simple API calls.</p>"},{"location":"Task8d/","title":"Language Model Merging","text":"<p> Note: The below section is for information only</p> <p>Language model merging involves integrating the weights of two or more fine-tuned large language models (LLMs) into a single, unified model. By merging these models, each specialized in different tasks or domains, you can combine their fine-tuned capabilities into one model, enhancing its overall performance and versatility. This process is an effective way to maximize the potential of your LLMs. Our preferred tool for this process is Mergekit, an open-source repository available at Mergekit. For a deeper understanding of the underlying concepts, you can refer to the related Research paper</p>"},{"location":"Task9/","title":"GenAI- Frameworks","text":"<p>In the previous sections of this lab, we started by exploring  basics of neural networks/tokenization, which are key to understanding how modern AI models, particularly large language models (LLMs), operate. We moved on to embeddings and vector databases, which play a vital role in how these models represent and retrieve information.</p> <p>We also looked into the concept of context windows and how they influence model performance. To address some of the limitations inherent in traditional context windows, we introduced Retrieval Augmented Generation (RAG). This approach enables models to dynamically retrieve relevant information, significantly improving their ability to generate accurate and contextually appropriate responses. To make you understand better, we even built a quick RAG application. Additionally, we touched on multimodal RAG, which takes this a step further by enabling the model to process and integrate information from multiple sources, such as text, images, and more, making it even more versatile.</p> <p>While we\u2019ve covered the basics, it\u2019s crucial to recognize that the field of AI is rapidly advancing. Companies like OpenAI, Mistral, Google, Meta, and others are in a race to develop the most sophisticated large language models (LLMs). </p> <p></p> <p>However, effectively using these powerful LLMs in real-world scenarios involves more than just sending and receiving API requests. Challenges such as fine-tuning models, integrating them with third-party systems e.g. Webex, and ensuring they perform well in complex environments require more than what LLMs alone can offer. </p> <p></p> <p>This is where GenAI frameworks come into play, providing the essential infrastructure to bridge the gap between raw LLM capabilities and practical, scalable AI solutions. There are numerous frameworks available for use.</p> <p></p> <p>The question of which framework to choose ultimately depends on your specific preferences and needs. Many of these frameworks have overlapping functionalities. In this section, we'll focus on  LangChain, exploring what it is and how it can be used to create AI-powered applications.</p>"},{"location":"Task9/#langchain","title":"Langchain","text":"<p>LangChain is an open-source framework that enables developers to integrate large language models (LLMs) with external data sources and computational resources. Available as a Python, JavaScript, or TypeScript package, it offers a versatile pipeline abstraction that can be used for various purposes, from creating AI enabled applications to managing data ingestion from external sources.</p> <p>While an LLM like (GPT) has impressive general knowledge, LangChain is essential for retrieving information from your own documents or fine-tuning models for specific tasks. It allows you to connect LLMs to your data sources, enabling actions like sending messages to a Webex Space. The process involves breaking your data or documents into smaller chunks and storing them in a vector database as embeddings. LangChain supports various vector stores, such as Chroma, Faiss, and Cassandra e.t.c making data retrieval and action-taking both efficient and seamless. More info on langchain can be found here.</p> <p></p> <p></p> <p></p> <p>More info about Embeddings can be found in Task4</p> <p></p> <p></p> <p>Below is a summary from a Langchain blog that outlines the most commonly used LLMs, embeddings, and vector stores.</p> <p></p> <p></p> <p></p> <p>The beauty of Langchain lies in its ability to provide a comprehensive ecosystem, regardless of the LLM model you choose to work with. Within the Langchain ecosystem, there's a tool called Langsmith, which is designed for monitoring and debugging your applications, think of it as LLM ops. If you're building applications as APIs, Langchain offers Langserve (create services in the form of APIs). Additionally, the ecosystem includes agents, chains, retrievers, and LLM tools, making it a versatile platform for various NLP tasks.</p> <p></p> <p>Note: Chains are a sequence of steps or operations that are linked together to achieve a particular task. Each step in the chain can involve various processes, such as invoking a language model, performing a calculation, calling an external API, or manipulating data. Chains allow developers to structure these steps in a way that the output of one step becomes the input of the next, creating a pipeline of operations. </p> <p></p> <p></p> <p>Note: The above image was obtained from Langchain"},{"location":"Task9/#langchain-usecase-1","title":"Langchain - UseCase-1","text":"<p>Note: Make sure you have created a Langchain account as explained in the \"Getting Started with LangChain\" section in Task1</p> <p>Let's start building chatbot application using LangChain, I'll guide you through the basic steps. </p> <p>With LangChain, when creating an application, you have the flexibility to use either paid LLMs or open-source LLMs. One way to integrate open-source LLMs is through Hugging Face, but since we're focusing on LangChain, we'll explore how this can be done within its ecosystem.</p> <p></p> <p>We\u2019ll build a chatbot using both paid models (like OpenAI) and open-source LLMs. LangChain provides modules that can seamlessly interact with both paid services and open-source models, allowing you to create versatile and powerful applications.</p> <p></p> <p>In this example, we'll explore the use of Langchain and Langserve, starting with the installation of essential libraries like langchain_openai, langchain, and langchain_core. We'll demonstrate how to leverage Langserve for managing large language models (LLM) operations.  Furthermore, I'll show you how to utilize Streamlit, a server-based application, to interactively visualize and interact with your code. Given the challenges of running Streamlit directly on Google Colab, we'll deploy NGROK to facilitate executing and accessing our code through a web interface.</p>"},{"location":"Task9/#task-1-log-into-the-lab-environment","title":"Task 1: Log into the Lab Environment","text":"<ul> <li>Open Google Colab and create a new notebook or use an existing one. Click on \"File\" &gt; \"New notebook\". Please refer to the following section to create Google Colab account.</li> </ul> <ul> <li>Make sure you are connected to a runtime. For this task, you can use the CPU as the runtime environment.</li> </ul>"},{"location":"Task9/#set-openai-token","title":"Set OpenAI token","text":"<p>Note: Skip this step and jump to set-langchain-tokens if you have already created the OpenAI account . If this is the first module you working with please, create an account from the OpenAI official website.</p> <ul> <li>Create a new project API key by browsing to API Keys web page. Select Create new secret key. The API key is automatically generated. Save the APi Key as we will be using it in the later steps .</li> </ul> <p></p> <ul> <li>Within your existing Google Colab notebook navigate to the new \u201cSecrets\u201d section in the sidebar.</li> </ul> <p></p> <ul> <li> <p>Click on \u201cAdd a new secret.\u201d Enter the name example: OPENAI_API_KEY and value of the secret(the API key created above). Note: The name is permanent once set. </p> </li> <li> <p>The list of secrets is global across all your notebooks.</p> </li> <li> <p>Use the \u201cNotebook access\u201d toggle to grant or revoke access to a secret for each notebook.</p> </li> </ul> <p></p>"},{"location":"Task9/#set-langchain-tokens","title":"Set Langchain tokens","text":"<p>Note: Make sure you have created a Langchain account as explained in the \"Getting Started with LangChain\" section in Task1</p> <ul> <li>Copy your Langchain Key and save it </li> </ul> <p></p> <p></p> <ul> <li>Within your existing Google Colab notebook navigate again to the new \u201cSecrets\u201d section in the sidebar. Click on \u201cAdd a new secret.\u201d Enter the name example: LANGCHAIN_API_KEY and value of the secret(the API key created in Langchain). </li> </ul> <p></p> <p></p>"},{"location":"Task9/#set-ngrok","title":"Set NGROK","text":"<ul> <li> <p>Browse to NGROK and click on Login</p> </li> <li> <p>I\u2019ll be using the \"Login with Google\" option to access my account, but feel free to choose the login method that works best for you.</p> </li> </ul> <p></p> <ul> <li>Get your NGROK auth key</li> </ul> <p></p> <p> Note: Since we are using a free NGROK account, we can only run up to 3 tunnels simultaneously. If you need to terminate any active tunnels, please use the following steps </p> <pre><code>!pkill ngrok\n</code></pre>"},{"location":"Task9/#configuration","title":"Configuration","text":"<ul> <li>Lets start by installing all the necessary packages</li> </ul> <pre><code>! pip install langchain_openai langchain langchain_core python-dotenv streamlit langchain_community langserve streamlit pyngrok ngrok\n</code></pre> <ul> <li>We will import libraries</li> </ul> <pre><code>from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nimport requests\nimport streamlit as st\nimport os\nfrom dotenv import load_dotenv\nfrom google.colab import userdata\nfrom pyngrok import ngrok\nimport subprocess\nimport os\n</code></pre> <ul> <li>Lets start by configuring the environment variables. We will use and define Langserve for our LLM ops</li> </ul> <pre><code>import os\nfrom google.colab import userdata\nos.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\nos.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"WebexOne\"\n</code></pre> <p> Note: Due to the challenges of running Streamlit directly on Google Colab, we'll use NGROK to enable the execution and access of our code via a web interface. The code below is for your understanding or if you're running it as Python packages in your own environment. To continue and run the code in this lab or on Google Colab, please proceed and copy the code from Configuration1 section. </p> <ul> <li>(Optional) As we creating a simple chatbot application lets create our prompt</li> </ul> <pre><code>prompt=ChatPromptTemplate.from_messages(\n    [\n        (\"system\",\"You are a helpful Cisco assistant. Please respond to the user queries\"),\n        (\"user\",\"Question:{question}\")\n    ]\n)\n</code></pre> <ul> <li>(Optional) Lets define Streamlit Framework</li> </ul> <pre><code>st.title('Langchain using OPENAI API')\ninput_text=st.text_input(\"Search the topic you want\")\n</code></pre> <ul> <li>(Optional) We will initialize the Language Model and set-up the Output parser </li> </ul> <pre><code>llm=ChatOpenAI(model=\"gpt-4o\")\noutput_parser=StrOutputParser()\n</code></pre> <ul> <li>(Optional) Langchain offers features that allow us to connect various components into a seamless workflow, known as chains. So far, we have created a chat prompt template, initialized the LLM (Language Model), and set up an Output Parser. Now, let's combine all these elements.</li> </ul> <pre><code>chain=prompt|llm|output_parser\n</code></pre> <ul> <li>(Optional) Lets give our input as question and pass it to our chain </li> </ul> <pre><code>if input_text:\n    response = chain.invoke({'question': input_text})\n    st.write(response)\n    send_webex_message(response)\n</code></pre> <p></p>"},{"location":"Task9/#configuration1","title":"Configuration1","text":"<ul> <li> <p>Running Streamlit apps directly within Google Colab can be challenging due to the need for a continuous web interface, which is not natively supported in Colab's environment. To address this, we will use NGROK, which creates a secure tunnel to make the Streamlit app accessible via a web interface.</p> </li> <li> <p>The code will start by creating a Streamlit application using a prompt template, a language model (LLM), and an output parser. These components are combined into a chain to process user input and generate responses using  GPT-4o model. The app will prompt the user to input a query and then processes it through the chain, displaying the generated response in the Streamlit interface. We will save Streamlit app code to a file (app.py), and a subprocess is started to run the app.</p> </li> </ul> <pre><code>streamlit_code = \"\"\"\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nimport requests\nimport streamlit as st\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n# Set up the prompt template\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a helpful Cisco Live assistant. Please respond to the user queries\"),\n        (\"user\", \"Question:{question}\")\n    ]\n)\n\n# Streamlit interface\nst.title('Langchain using OPENAI API')\ninput_text = st.text_input(\"Search the topic you want\")\n\n# Initialize the OpenAI LLM\nllm = ChatOpenAI(model=\"gpt-4o\")\noutput_parser = StrOutputParser()\n\n# Create the chain\nchain = prompt | llm | output_parser\n\nif input_text:\n    response = chain.invoke({'question': input_text})\n    st.write(response)\n    send_webex_message(response)\n\"\"\"\n\n# Save the streamlit app code to a file\nwith open(\"app.py\", \"w\") as file:\n    file.write(streamlit_code)\n\n# Start the Streamlit app using subprocess\nprocess = subprocess.Popen(['streamlit', 'run', 'app.py'])\n\n# Start ngrok tunnel\nngrok.set_auth_token(\"Replace-with-your-ngrok-authtoken-created-above\")  \npublic_url = ngrok.connect(8501, \"http\")  # Use http instead of specifying as a named argument\nprint(f\"Streamlit app is live at: {public_url}\")\n</code></pre> <p></p> <p></p> <p></p> <ul> <li>Since we're using Langserve for llm-ops, let's log in and explore its features there.</li> </ul> <p></p> <p></p> <p></p> <p></p> <p>This dashboard provides a summary of the performance, usage, and cost metrics for the \"WebexOne\" project within the last 7 days. It offers insights into how frequently the project has been run, the efficiency (in terms of error rates and latency), and the associated costs, helping users to monitor and optimize their LLM (Large Language Model) operations.</p>"},{"location":"Task9/#enhancing-our-streamlit-app-to-send-messages-to-a-webex-space","title":"Enhancing Our Streamlit App to Send Messages to a Webex Space","text":"<p>We\u2019ve seen how simple it is to interact with LLM and create an application using Langchain + Streamlit. Now, let\u2019s take it a step further by modifying the application so that the responses are not only displayed in the Streamlit interface but are also sent directly to a specific Webex space.</p> <p> Note: Ensure you have the Webex Bearer Token and Space ID available. If you don't have them, please reach out to your lab proctor. </p> <ul> <li>let's modify our code to send the generated responses directly to a Webex space. This will enable real-time collaboration and communication with team members through Webex, enhancing the utility of our AI applications.</li> </ul> <pre><code># Create the Streamlit app code\nstreamlit_code = \"\"\"\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nimport requests\nimport streamlit as st\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Webex setup\nwebex_access_token = \"Your-Webex-Token\"\nwebex_space_id = \"Your-Space-ID\"\n\n# Function to send a message to Webex\ndef send_webex_message(content):\n    url = \"https://webexapis.com/v1/messages\"\n    headers = {\n        \"Authorization\": f\"Bearer {webex_access_token}\",\n        \"Content-Type\": \"application/json\"\n    }\n    payload = {\n        \"roomId\": webex_space_id,\n        \"text\": content\n    }\n    response = requests.post(url, headers=headers, json=payload)\n    if response.status_code == 200:\n        print(\"Message sent to Webex:\", response.json())\n    else:\n        print(\"Failed to send message:\", response.text)\n\n# Set up the prompt template\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are a helpful Cisco Live assistant. Please respond to the user queries\"),\n        (\"user\", \"Question:{question}\")\n    ]\n)\n\n# Streamlit interface\nst.title('Langchain using OPENAI API')\ninput_text = st.text_input(\"Search the topic you want\")\n\n# Initialize the OpenAI LLM\nllm = ChatOpenAI(model=\"gpt-4o\")\noutput_parser = StrOutputParser()\n\n# Create the chain\nchain = prompt | llm | output_parser\n\nif input_text:\n    response = chain.invoke({'question': input_text})\n    st.write(response)\n    send_webex_message(response)\n\"\"\"\n\n# Save the streamlit app code to a file\nwith open(\"app.py\", \"w\") as file:\n    file.write(streamlit_code)\n\n# Start the Streamlit app using subprocess\nprocess = subprocess.Popen(['streamlit', 'run', 'app.py'])\n\n# Start ngrok tunnel\nngrok.set_auth_token(\"Replace-with-your-ngrok-authtoken-created-above\")  # Replace with your ngrok authtoken\npublic_url = ngrok.connect(8501, \"http\")  # Use http instead of specifying as a named argument\nprint(f\"Streamlit app is live at: {public_url}\")\n</code></pre> <p></p> <p></p> <p></p>"},{"location":"Task9/#building-using-ollama-modelsopensource-optional","title":"Building using Ollama Models(opensource) - OPTIONAL","text":"<p> Note: The code below will work if you are running Ollama locally on your machine as explained in Task1</p> <p>In this example, we're leveraging LangChain to build an AI assistant using the LLama3 model, accessible through the Ollama API. The assistant is designed to help with queries and is implemented using Streamlit, which provides a user-friendly interface as shown above. </p> <p>Note: Make sure you are running Ollama locally on your machine, as explained in Task1, to ensure the Llama3 model is properly accessible.</p> Optional Code<pre><code>from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_community.llms import Ollama\nimport streamlit as st\nimport os\nfrom dotenv import load_dotenv\nload_dotenv()\n## Prompt Template\nprompt=ChatPromptTemplate.from_messages(\n    [\n        (\"system\",\"You are a helpful Cisco  assistant. Please respond to the user queries\"),\n        (\"user\",\"Question:{question}\")\n    ]\n)\nst.title('Langchain With LLAMA2 API')\ninput_text=st.text_input(\"Search the topic you want\")\n\n# ollama LLAma3 LLm \nllm=Ollama(model=\"llama3:latest\")\noutput_parser=StrOutputParser()\nchain=prompt|llm|output_parser\n\nif input_text:\n    st.write(chain.invoke({\"question\":input_text}))\n</code></pre> <p></p> <p></p>"},{"location":"Task9/#langchain-usecase-2-advanced-rag-chains-and-retrievers","title":"Langchain - UseCase-2 - Advanced Rag - Chains and Retrievers","text":"<p>In Task5, we explored the basics of a simple Retrieval-Augmented Generation (RAG) application, where we utilized similarity search to find and retrieve information from our database. While this approach is effective for specific, straightforward use cases, it does come with some limitations among them being lack of flexibility.</p> <p>One of the drawbacks of relying solely on similarity search is that it doesn\u2019t easily accommodate more complex workflows or integration with additional tools. It\u2019s powerful for direct matches but can struggle when the task requires multi-step logic, advanced filtering, or a combination of different data sources.</p> <p>LangChain\u2019s retrievals and chains, on the other hand, provide a more versatile and scalable framework. They enable the creation of sophisticated RAG applications by allowing you to define workflows that can incorporate multiple retrieval strategies, conditional logic, and integration with other LLM's or APIs.</p> <p>Let\u2019s now shift our focus towards creating an advanced RAG application by leveraging the capabilities of LangChain\u2019s chains and retrievers. This will not only improve the flexibility and power of our application but also open up new possibilities for handling complex queries and delivering more refined results.</p> <p></p> <p>Chain: A chain in LangChain is like a sequence of tasks that you want to accomplish step by step. </p> <p>Retriever: is like a search engine inside your application. Its job is to find and pull out the most relevant information from a vector database or a collection of documents.</p> <p>A retrieval chain is when you combine the power of both chains and retrievers.</p>"},{"location":"Task9/#task-1-log-into-the-lab-environment_1","title":"Task 1: Log into the Lab Environment","text":"<ul> <li> <p>Open Google Colab and start a new notebook, or you can use an existing one. Ensure that your OpenAI and LangChain tokens are already set up and activated for this notebook, as described in Use Case 1.</p> </li> <li> <p>Let's load our PDF files into Google Colab. For this example, we can use the article titled \"Cisco Preferred Architecture for Webex Calling\". You can  download the  article here as we will be using in the next step.</p> </li> <li> <p>Within Google Colab, Click on Folder and create a new folder called \"data\"</p> </li> </ul> <p></p> <ul> <li>Click on [...], select Upload</li> </ul> <p></p> <ul> <li>Choose your webex_calling.pdf file and click Open</li> </ul>"},{"location":"Task9/#configuration_1","title":"Configuration","text":"<ul> <li>Let's start by installing all the packages</li> </ul> <pre><code>! pip install langchain_openai langchain langchain_core python-dotenv langchain_community langserve PyPDF chromadb\n</code></pre> <ul> <li>Let's start by configuring the environment variables. We will use and define Langserve for our LLM ops</li> </ul> <pre><code>import os\nfrom google.colab import userdata\nos.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\nos.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"WebexOne\"\n</code></pre> <ul> <li>We will import libraries</li> </ul> <pre><code>from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nimport requests\nimport os\nfrom dotenv import load_dotenv\nfrom google.colab import userdata\n</code></pre> <p>If we examine the RAG pipeline, we typically need to follow these steps</p> <p></p> <ul> <li>Let's load our data source</li> </ul> <pre><code>from langchain_community.document_loaders import PyPDFLoader\nloader=PyPDFLoader(\"/content/data/webex_calling.pdf\")\ndocs=loader.load()\n</code></pre> <ul> <li>We will do transformation to break data into smaller chunks</li> </ul> <pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter\ntext_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\ndocuments=text_splitter.split_documents(docs)\n</code></pre> <ul> <li>We will now convert chunks into embeddings and store in Vector database e.g. Chroma</li> </ul> <pre><code>from langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\ndb = Chroma.from_documents(documents,OpenAIEmbeddings())\n</code></pre> <ul> <li>At this stage, I can query my vector database and retrieve information based on similarity search.</li> </ul> <pre><code>query = \"Explain me webex calling solution overview\"\nresult = db.similarity_search(query)\nprint(result[0].page_content)\n</code></pre> <p></p> <ul> <li>We will  now combine prompts with chains and retrievers to generate responses based on the prompts. Let's start by defining our LLM</li> </ul> <pre><code>llm=ChatOpenAI(model=\"gpt-4o\")\n</code></pre> <p>Note: You can also use open source models like Ollama using the following code: from langchain_community.llms import Ollama  llm=Ollama(model=\"llama2\")  </p> <ul> <li>Let's design our chat prompt templates instead of querying our database using similarity search. We'll be utilizing langchain_core.prompts for this purpose.</li> </ul> <pre><code>from langchain_core.prompts import ChatPromptTemplate\nprompt= ChatPromptTemplate.from_template(\"\"\"\n                                         Answer the following question based only on the provided context. If no answer is available just say I dont know.\n                                         &lt;context&gt;\n                                         {context}\n                                         &lt;/context&gt;\n                                         Question: {input}\"\"\")\n</code></pre> <ul> <li>Let's create our Chain now. Chains are sequences of operations that process input to produce the desired output. We will use create_stuff_documents_chain</li> </ul> <pre><code>from langchain.chains.combine_documents import create_stuff_documents_chain\ndocument_chain=create_stuff_documents_chain(llm,prompt)\n</code></pre> <ul> <li>Now, let's define our retrievers. As previously mentioned, a retriever is an interface that returns documents based on an unstructured query. It is broader in scope than a vector store, as it doesn't need to store documents, only to retrieve them. While vector stores can serve as the foundation for a retriever, there are other types of retrievers available as well. For more information, you can refer to the documentation here</li> </ul> <pre><code>retriever=db.as_retriever()\nretriever\n</code></pre> <p></p> <ul> <li>In this step, we'll set up a retrieval chain. This chain will start by taking a user inquiry and passing it to the retriever to fetch relevant documents. The retrieved documents, along with the original inputs, will then be sent to a language model (LLM) to generate a response. For further details on how to implement a retrieval chain, refer to the following doc</li> </ul> <pre><code>from langchain.chains import create_retrieval_chain\nretrieval_chain=create_retrieval_chain(retriever,document_chain)\n</code></pre> <ul> <li>Now, let's combine our retriever and chain to generate responses:</li> </ul> <pre><code>response=retrieval_chain.invoke({\"input\":\" what are the bandwidth required on the internet access for Webex Calling\"})\n</code></pre> <pre><code>response['answer']\n</code></pre> <p></p>"},{"location":"Task9/#summary","title":"Summary","text":"<p>In this lab, we've built an advanced Retrieval-Augmented Generation (RAG) system. The process begins with a user query, which is passed through a retrieval chain to fetch relevant documents from a vector database. These documents, along with the original query, are then processed by a Language Model (LLM) to generate a detailed and accurate response. This setup allows us to efficiently handle complex queries by combining the strengths of chains, retrieval and language models.</p> <p></p>"},{"location":"Task9/#langchain-usecase-3-advanced-rag-multisearch-agents-rag-application","title":"Langchain - UseCase-3 - Advanced Rag - Multisearch Agents RAG application","text":"<p>If you recall in our previous usecase, we discussed chains where we manually defined the sequence of actions to execute. However, the core concept behind agents is different. Instead of hardcoding the sequence of actions (as we do in chains), agents leverage a language model as a reasoning engine. This allows the model/agents to dynamically decide which actions to take and in what order.</p> <p>In this context, tools play an essential role. Tools are like wrappers that the model can call upon to perform specific tasks, acting as interfaces between the agent and the outside world. By integrating tools with agents, the language model not only decides what action to take but also knows how to use the available tools to achieve the desired outcome. This flexible system allows agents to interact with different data sources or APIs dynamically, without the need for a fixed sequence of operations as seen in chains.</p>"},{"location":"Task9/#lab-configuration","title":"Lab Configuration","text":"<p>In this use-case, we will build a system that leverages the power of agents combined with tools to create a flexible and dynamic environment. We will create multiple tools that interact with different data sources, including:</p> <ul> <li> <p>A Wikipedia-based tool for querying concise information.</p> </li> <li> <p>A custom retriever tool that searches through a specific dataset or webpage example: (ThousandEyes API documentation).</p> </li> <li> <p>An Arxiv-based tool for retrieving AI research papers.</p> </li> <li> <p>Custom PDF-based tool for extracting information from a document (Webex Calling Preffered Architecture).</p> </li> </ul> <p>We will combine these tools into an agent that can decide which tool to use based on the task at hand. Instead of manually defining the steps like in a chain, this agent will use a language model to reason and determine the right sequence of actions dynamically.</p> <p></p> <p>By the end of the lab, you'll have an agent-driven system capable of dynamically selecting and using tools to perform complex tasks, like retrieving data from a variety of sources without the need for hardcoding specific steps.</p>"},{"location":"Task9/#task-1-log-into-the-lab-environment_2","title":"Task 1: Log into the Lab Environment","text":"<ul> <li> <p>Open Google Colab and start a new notebook, or you can use an existing one. Ensure that your OpenAI and LangChain tokens are already set up and activated for this notebook, as described in Use Case 1 and Use Case 2.</p> </li> <li> <p>Let's load our PDF files into Google Colab. For this example, we can use the article titled \"Cisco Preferred Architecture for Webex Calling\". You can  download the  article here as we will be using in the next step.</p> </li> <li> <p>Within Google Colab, Click on Folder and create a new folder called \"data\"</p> </li> </ul> <p></p> <ul> <li>Click on [...], select Upload</li> </ul> <p></p> <ul> <li>Choose your webex_calling.pdf file and click Open</li> </ul>"},{"location":"Task9/#configuration_2","title":"Configuration","text":"<ul> <li>Let's start by installing all the packages</li> </ul> <pre><code>!pip install langchain_openai langchain langchain_core python-dotenv langchain_community langserve PyPDF chromadb arxiv wikipedia langchainhub \n</code></pre> <ul> <li>We will configure the environment variables. We will use and define Langserve for our LLM ops</li> </ul> <pre><code>import os\nfrom google.colab import userdata\nos.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\nos.environ['LANGCHAIN_API_KEY'] = userdata.get('LANGCHAIN_API_KEY')\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\nos.environ[\"LANGCHAIN_PROJECT\"] = \"WebexOne\"\n</code></pre> <ul> <li>We will be creating Tools (wrappers) around our data sources. Tools act as interfaces that can be used by agents, chains, or LLMs to interact with various data sources. Langchain provides built-in toolkits that allow us to easily create these wrappers and interact with data sources to extract data. In our lab guide, we will develop four different wrappers. The first wrapper tool will be built around Wikipedia.</li> </ul> <p>Tool 1</p> <pre><code>from langchain_community.tools import WikipediaQueryRun\nfrom langchain_community.utilities import WikipediaAPIWrapper\n# I need only one result and 300 charectors- interacting with Wikipedia in this lab\napi_wrapper = WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=300)\nwiki = WikipediaQueryRun(api_wrapper=api_wrapper)\n</code></pre> <pre><code>wiki.run(\"history of cisco\")\n</code></pre> <p></p> <p>Tool 2</p> <ul> <li>Let's create a custom wrapper. Since this is my data, I won't be using predefined tools like I did for Wikipedia. Instead, I will define my own embeddings, vectors, and retrievers. In this Wrapper we will read API guide for ThousandEyes</li> </ul> <p><pre><code>from langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import ChatOpenAI\nloader = WebBaseLoader(\"https://docs.thousandeyes.com/product-documentation/getting-started/getting-started-with-the-thousandeyes-api\")\ndocs = loader.load()\ndocuments = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200).split_documents(docs)\nvectordb = Chroma.from_documents(documents,OpenAIEmbeddings())\nretriever = vectordb.as_retriever()\nretriever\n</code></pre> * Lets create a custom retriever tool using Langchain to search through our TE dataset. More info on create_retriever_tool available here</p> <pre><code># Use prompts from Hub for my retriver\nfrom langchain.tools.retriever import create_retriever_tool\nretriever_tool=create_retriever_tool(retriever,\"ThousandEyes_Search\",\n                      \"Search for information about ThousandEyes APis. For any questions about APIs, you must use this tool!\")\n</code></pre> <p>Tool 3</p> <ul> <li>Arxiv is a free distribution service and an open-access archive where researchers from various fields, such as computer science, physics, and more, share their research papers.  In this section, we will create a tool that interfaces with Arxiv to search and retrieve research papers. The below code sets up a tool to query Arxiv and extract  results from the top research papers relevant to the query.</li> </ul> <pre><code>from langchain_community.utilities import ArxivAPIWrapper\nfrom langchain_community.tools import ArxivQueryRun\n# I need only one result and 300 charectors- interacting with Arxiv in this lab\narxiv_wrapper=ArxivAPIWrapper(top_k_results=1, doc_content_chars_max=300)\narxiv=ArxivQueryRun(api_wrapper=arxiv_wrapper)\narxiv.name\n</code></pre> <p>Tool 4</p> <ul> <li>Let's create another custom wrapper for my PDF files. Since this is my data, I won't be using predefined tools like I did for Wikipedia and Arxiv. Instead, I will define my own embeddings, vectors, and retrievers, similar to how we did with the ThousandEyes API guide. We'll use the Webex Calling Preferred Architecture Guide, make sure it's uploaded according to the steps we've covered earlier.</li> </ul> <pre><code>from langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import Chroma\nloader=PyPDFLoader(\"/content/data/webex_calling.pdf\")\ndocs=loader.load()\ntext_splitter=RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\ndocuments=text_splitter.split_documents(docs)\ndb = Chroma.from_documents(documents,OpenAIEmbeddings())\nretriever1=db.as_retriever()\nretriever1\n</code></pre> <ul> <li>Lets create a custom retriever tool using Langchain to search through our PDF. More info on create_retriever_tool available here.</li> </ul> <pre><code>from langchain.tools.retriever import create_retriever_tool\nretriever_tool1=create_retriever_tool(retriever1,\"Webex_Calling_Search\",\n                      \"Search for information about Webex Calling. For any questions about Webex Calling, you must use this tool!\")\n</code></pre> <ul> <li>We will combine all the tools/wrappers together now</li> </ul> <pre><code>tools=[wiki,arxiv,retriever_tool,retriever_tool1]\n</code></pre> <ul> <li>The next step is to query these tools, and that's where agents come in. Whenever I send a query to the LLM, it will search across all the available tools (Wiki, my custom retrieval tool, Arxiv, etc.) to find the most relevant source and provide the answer. Agents typically use the LLM to perform a sequence of actions. In this case, the LLM acts as a reasoning engine, determining which tool to use based on the query, searching the relevant sources, and delivering the most accurate response. Lets define our LLM </li> </ul> <pre><code>from dotenv import load_dotenv\nfrom langchain_openai import ChatOpenAI\nload_dotenv()\nimport os\nos.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\nllm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n</code></pre> <ul> <li>Let's use the prompt from the hub. If you'd like to create your own custom prompt, feel free to do so as well! </li> </ul> <pre><code># Get prompts\nfrom langchain import hub\nprompt = hub.pull(\"hwchase17/openai-functions-agent\")\nprompt.messages\n</code></pre> <ul> <li>As we are creating an agent that can interact with multiple tools by leveraging the OpenAI language model. The create_openai_tools_agent function combines the LLM, tools, and a prompt to form an agent. This agent will use the LLM as a reasoning engine to dynamically decide which tool to use based on the input query. It allows us to query Wikipedia, custom retrieval tools, Arxiv, and more, all within one unified system.</li> </ul> <pre><code>from langchain.agents import create_openai_tools_agent\nagent=create_openai_tools_agent(llm,tools,prompt)\n</code></pre> <ul> <li>Let's create an Agent Executor. It will use the previously defined agent and tools, allowing the agent to execute queries using the appropriate tool dynamically. The verbose=True option ensures that detailed output is provided during execution, helping to understand the agent's decision-making process as it selects and interacts with the tools.</li> </ul> <pre><code>from langchain.agents import AgentExecutor\nagent_executor=AgentExecutor(agent=agent,tools=tools,verbose=True)\nagent_executor\n</code></pre> <ul> <li>Lets execute our code</li> </ul> <pre><code>agent_executor.invoke({\"input\":\"tell me about Webex Calling\"})\n</code></pre> <p></p> <pre><code>agent_executor.invoke({\"input\":\"tell me about Getting Started with the ThousandEyes API\"})\n</code></pre> <p></p> <pre><code>agent_executor.invoke({\"input\":\"tell me about cisco systems\"})\n</code></pre> <p></p> <pre><code>agent_executor.invoke({\"input\":\"tell me about research paper 2106.09685v2\"})\n</code></pre> <p></p>"},{"location":"conclusion/","title":"Conclusion","text":"<p>This lab has provided a comprehensive journey through key AI and ML concepts and hands-on tasks. Starting with lab setup using platforms like Google Colab, Hugging Face, and Langchain, we explored tokenization, embeddings, and vector databases. As we progressed, we delved into advanced topics such as Retrieval-Augmented Generation (RAG) and its multimodal applications. The lab introduced the fundamentals of fine-tuning large language models, covering frameworks like LLaMA and ChatGPT, and even dove into the intricacies of model quantization, LoRA, and SFT. </p> <p>Finally, this lab ensured you are equipped with a holistic understanding of modern AI technologies, ready to apply them to real-world use cases.</p> <p>Thank you again for also taking the time and exploring Cisco's innovations in AI. Please share your feedback on oilyas@cisco.com. I'm always looking for ways to improve this content. If you have any suggestions, feel free to submit them so we can better assist you in solving your end users' challenges and give you more time to focus on what you enjoy.</p>"},{"location":"overview/","title":"Overview - Understanding AI and Its Integration with Webex","text":"<p>Artificial Intelligence (AI) is transforming the way we work, enabling innovative solutions and enhancing productivity. Cisco is committed to innovating responsibly, with responsible AI being non-negotiable. Our approach is grounded in the principles of transparency, fairness, accountability, reliability, security, and privacy. Cisco's Responsible AI Framework, aligned with the  AI Risk Management Framework, ensures that all AI initiatives undergo rigorous assessments, particularly around privacy. </p> <p>Remember data has value, Cisco's AI strategy emphasizes building ethical and trustworthy systems that mitigate risks while enhancing innovation across products and services.</p> <p></p> <p>Please refer to the below info for more detailed insights about Cisco Responsible AI:</p> <ul> <li>Responsible AI is built on a foundation of privacy</li> <li>Cisco's Responsible Artificial Intelligence Principles</li> <li>Cisco's Responsible Approach to Governing Artificial Intelligence</li> <li>Cisco's Responsible Artificial Intelligence Framework</li> </ul> <p></p>"},{"location":"overview/#the-evolution-of-ai-at-cisco","title":"The Evolution of AI at Cisco","text":"<p>At Cisco, innovation is woven into the fabric of everything we do. We've been at the forefront of AI development long before it became a buzzword.  Our journey spans decades, from early developments in audio and video intelligence, like echo cancellation, media resilience, to recent breakthroughs like noise removal, face recognition, and immersive experiences, just to name a few. We continue to push boundaries, enhancing AI across multiple domains to deliver immersive and intelligent experiences. Innovation in AI isn't just a trend for us\u2014it's a longstanding commitment to excellence and progress.</p> <p></p> <p>What we want everyone to understand is that AI is the core fabric that powers our platforms, enabling  reimagining of work, workspaces, and customer experiences. Whether through the Webex Suite, advanced devices, or   contact center solutions, AI drives the seamless, intelligent, and immersive experiences that define modern collaboration. Cisco's AI-powered Webex platform is designed to transform the way we work, ensuring that every interaction is smarter, more efficient, and more personalized.</p> <p></p> <p>In recent years, we've expanded our platform with several new AI capabilities. Here are a few highlights:</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>AI Codec Improve audio quality in poor network conditions </p> <p></p> <p></p> <p></p>"},{"location":"overview/#summary-of-newly-integrated-ai-features-within-our-platform","title":"Summary of newly integrated AI features within our platform","text":"<p>REMINDER: Our product team is hosting AI sessions. Please review the agenda and join the ones that interest you.</p>"},{"location":"overview/#lab-guide-overview","title":"Lab Guide Overview","text":"<p>In this lab guide you will go through the fundamental concepts of AI, various techniques, and how AI can be integrated with Webex to create efficient workflows.</p>"},{"location":"overview/#upon-completion-of-this-lab-you-will-be-able-to","title":"Upon completion of this lab you will be able to","text":"<ul> <li>Understand the basics of AI and its applications</li> <li>Learn about embedding techniques</li> <li>Vector databases</li> <li>Gain insights into Generative AI models</li> <li>Familiarize yourself with different AI frameworks</li> <li>Integrate AI with Webex to create seamless workflows - Call Recording</li> <li>Develop hands-on skills through practical exercises</li> <li>Understand Fine-tuning and Quantization</li> <li>Deploy Fine Tuning techniques</li> <li>Retrieval Augmented Generation (RAG) and Multimodal RAG</li> </ul>"},{"location":"overview/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of AI concepts is helpful but not required.</li> <li>We will be using Google Colab for this lab. However, if you have your own machine with an Nvidia chipset and prefer to run the lab locally, feel free to do so.</li> </ul>"},{"location":"overview/#disclaimer","title":"Disclaimer","text":"<p>The lab design and configuration examples provided are for reference/learning purposes only. This is a sample deployment, and not all recommended features are used or enabled optimally. For design-related questions, please contact your representative at Cisco or a TME team.</p>"},{"location":"overview/#lab-overview-generative-ai-and-webex-integration","title":"Lab Overview - Generative AI and Webex integration","text":"<ul> <li>Lab Setup - Google Colab and Hugging Face</li> <li>AI/ML Revolution Unveiled: Task2.md</li> <li>Tokenization: </li> <li>Embeddings and Vector Database:</li> <li>Context Windows and Retrieval Augmented Generation (RAG)</li> <li>Multimodal RAG</li> <li>Understanding Fine-Tuning for Large Language Models</li> <li>Fine Tuning - Deep Dive into Quantization , LoRA and SFT</li> <li>Configuring and Fine Tuning - Using llama2</li> <li>Configuring and Fine Tuning - Using llama3</li> <li>Configuring and Fine Tuning - Using ChatGPT</li> <li>Model Merging (Theory Only)</li> </ul> <p>Let's get started! Click on Task 1 - Google Collab- Accessing Google Collab and creating account.</p>"}]}