{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"RenamedTask2/","title":"Task 2: ThousandEyes Overview","text":"<p>Feel free to take a few minutes while the lab spins up and review the Getting Started with Endpoint Agents guide.</p> <p></p> <p></p> <p></p> <p>Connectivity is your business, but it\u2019s changing rapidly. Your brand is only as good as your network.\u200b Digital experience is how you are measured, but most of the path is outside your control (outside the core)\u200b What do you need to do about it?\u200b How can you understand digital experience to the things your customers are accessing\u200b and understand health of the Internet cloud and peering providers. Continue your journey to learn how ThousandEyes can help answer these questions and help solve your network and application acceess issues.</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Now that you have a great background on ThousandEyes it's time to go back and verify the lab has started and login. Task 3.</p> <p>Also feel free to check our awesome TE Blog and Webex site.</p>"},{"location":"RenamedTask7a/","title":"Techniques for Finetuning LLMs","text":""},{"location":"RenamedTask7a/#introduction","title":"Introduction","text":"<p>In this lesson, we will examine the main techniques for fine-tuning Large Language Models for superior performance on specific tasks. We explore why and how to fine-tune LLMs, the strategic importance of instruction fine-tuning, and several fine-tuning methods, such as  Low-Rank Adaptation (LoRA), Supervised Finetuning (SFT). We also touch upon the benefits of the Parameter-Efficient Fine-tuning (PEFT) approach using Hugging Face's PEFT library, promising both efficiency and performance gains in fine-tuning.</p>"},{"location":"RenamedTask7a/#why-we-finetune-llms","title":"Why We Finetune LLMs","text":"<p>While pretrained Large Language Models (LLMs) provide a broad understanding of language, it doesn't equip them with the specialized knowledge needed for complex tasks. For instance, a pre-trained LLM may excel at generating text but encounter difficulties when tasked with sentiment analysis or even providing information from your own Knowledge base. This is where fine-tuning comes into play.</p> <p>Fine-tuning is the process of adapting a pretrained model to a specific task by further training it using task-specific data. For example, if we aim to make an LLM proficient in answering questions about Webex Calling or Webex CC, we would fine-tune it using a dataset comprising Webex question-answer pairs. This process enables the model to recalibrate its internal parameters and representations to align with the intended task, enhancing its capacity to address domain-specific challenges effectively.</p> <p>However, fine-tuning LLMs conventionally can be resource-intensive and costly. It involves adjusting  the parameters in the pretrained LLM models, which can number in the billions, necessitating significant computational power and time. Consequently, it's crucial to explore more efficient and cost-effective methods for fine-tuning, such as Low-Rank Adaptation (LoRA).</p>"},{"location":"RenamedTask7a/#a-reminder-on-instruction-and-conversational-finetuning","title":"A Reminder On Instruction and Conversational Finetuning","text":"<p>In Conversational fine tuning the model engages in a dialogue with the user, maintaining context over multiple turns.The interaction mimics a natural conversation, with the model responding in a way that feels like a human interlocutor.</p> <p>Instruction fine-tuning is a specific type of fine-tuning that grants precise control over a model's behavior. The objective is to train a Language Model (LLM) to interpret prompts as instructions rather than simply treating them as text to continue generating. </p>"},{"location":"RenamedTask7a/#introduction-to-efficient-finetuning-with-parameter-efficient-fine-tuning-peft","title":"Introduction to Efficient Finetuning with Parameter-Efficient Fine-tuning (PEFT)","text":"<p>Parameter-Efficient Fine-tuning (PEFT) approaches address the need for computational and storage efficiency in fine-tuning LLMs. Hugging Face developed the PEFT library specifically for this purpose. PEFT leverages architectures that only fine-tune a small number of additional model parameters while freezing most parameters of the pretrained LLMs, significantly reducing computational and storage costs.</p> <p>PEFT methods offer benefits beyond just efficiency. These methods have been proven to outperform standard fine-tuning methods, particularly in low-data situations, and provide improved generalization for out-of-domain scenarios. Furthermore, they contribute to the portability of models by generating tiny model checkpoints that require substantially less storage space compared to extensive full fine-tuning checkpoints.</p> <p>The PEFT library supports popular methods such as Low-Rank Adaptation (LoRA) and Prompt Tuning. </p>"},{"location":"RenamedTask7a/#a-reminder-of-the-techniques-for-finetuning-llms","title":"A Reminder of the Techniques For Finetuning LLMs","text":"<p>There are several techniques to make the finetuning process more efficient and effective:</p> <ul> <li> <p>Full Finetuning: This method involves adjusting all the parameters in the pretrained LLM models to adapt to a specific task. While effective, it is resource-intensive and requires extensive computational power, therefore it\u2019s rarely used. Not in our Scope</p> </li> <li> <p>Low-Rank Adaptation (LoRA): LoRA is a technique that aims to adapt LLMs to specific tasks and datasets while simultaneously reducing computational resources and costs. By applying low-rank approximations to the downstream layers of LLMs, LoRA significantly reduces the number of parameters to be trained, thereby lowering the GPU memory requirements and training costs. We\u2019ll also see QLoRA, a variant of LoRA that is more optimized and leverages quantization.</p> </li> </ul> <p>With a focus on the number of parameters involved in finetuning, there are multiple methods, such as:</p> <ul> <li> <p>Supervised Finetuning (SFT): SFT involves doing standard supervised finetuning with a pretrained LLM on a small amount of demonstration data. This method is less resource-intensive than full finetuning but still requires significant computational power. Scope of this lab</p> </li> <li> <p>Reinforcement Learning from Human Feedback (RLHF): RLHF is a training methodology where models are trained to follow human feedback over multiple iterations. This method can be more effective than SFT, as it allows for continuous improvement based on human feedback. We\u2019ll also see some alternatives to RLHF, such as Direct Preference Optimization (DPO), and Reinforcement Learning from AI Feedback (RLAIF).Not in our Scope</p> </li> </ul>"},{"location":"RenamedTask7a/#conclusion","title":"Conclusion","text":"<p>In this lesson, we've learned that while pretraining equips LLMs with a broad understanding of language, fine-tuning is necessary to specialize these models for complex tasks. We've looked into various fine-tuning techniques, including Full Finetuning, Low-Rank Adaptation (LoRA), Supervised Finetuning (SFT), and Reinforcement Learning from Human Feedback (RLHF). </p>"},{"location":"RnamedTask1/","title":"Task 1: Lab Login and Setup","text":"<p>Click to access the lab</p> <p>This lab requires a set of characters which will be aligned with roles to access ThousandEyes and Webex. Optional you can also register your Webex device on the platform that can be used with your demo.</p> <p>Click My Characters then click Add New Character as shown in the below image. You only need to create one character for the lab.</p> <p></p> <p>Click Create Character.  Note (They will be randomly created so yours may not match the below screenshot. Also you have an option to modify the names as per your preferences.)</p> <p></p> <p>Optional Step Click on the My Devices tab, select Add New Room Device, if you want your device to be available in the demo and thousandEyes agent installed on your device. Please remember its an optional step.</p> <p></p> <p>Make sure you are on your demo page Click Start Demo (you may have to use the back arrow to navigate back to the main overview page)</p> <p></p> <p>Next we need to provide a few details in order for the lab to be spun up.</p> <ul> <li>Audience: External</li> <li>External Type: Trade Show</li> <li>Tradeshow Name: Roadshow</li> <li>Demo Session Name: Enter your name \u2013 or leave it default</li> </ul> <p></p> <p>Click Next and then choose the character you created previously. (Note: this will be needed for automated session testing - AST)</p> <p></p> <p>Click Next and add a virtual workstation using the '+\" sign. This will be used to install the ThousandEyes Endpoint Agent (EPA) which can be configured to monitor web browser sessions, automtically monitor dynamic Webex meeting network connections and run scheduled tests in the background. Note you also have an option to install the ThousandEyes agent on your personal machine</p> <p></p> <p>Optional Step  If you have a Webex device select your device and click Next </p> <p></p> <p>Click Next select your duration and Click Start Demo as shown in the below screenshot.</p> <p></p> <p>This is roughly a 45 min lab but the access duration can be upto 3 days. It will take a few minutes for the lab to spin up. While this is occurring proceed on to the ThousandEyes Overview content and extra background information in Task 2.</p>"},{"location":"Task1/","title":"Pre-Requisites and Setup","text":""},{"location":"Task1/#google-collab-accessing-google-collab-and-creating-account","title":"Google Collab- Accessing Google Collab and creating account","text":"<p>Google Colab is a free, cloud-based platform that provides a convenient environment for running notebooks. If you want to create a machine learning model but don't have a computer that can handle the workload, Google Colab is the platform for you. In our lab, we will be using Google Colab to test and run our code. However, if you have your own Python environment and prefer to run the code on your local machine, please feel free to do so.</p> <p>Here are some reasons why using Google Colab can be beneficial for this lab:</p> <ul> <li>Free Access to GPUs and TPUs: Google Colab offers free access to powerful GPUs and TPUs, which can significantly accelerate the training and fine-tuning of machine learning models.</li> <li>No Setup Required: With Colab, there is no need to set up your local environment. Everything runs in the cloud, which saves time and avoids configuration issues.</li> <li>Easy Collaboration: Colab notebooks can be easily shared and collaborated on with team members, making it an ideal tool for collaborative projects.</li> <li>Integration with Google Drive: Colab integrates seamlessly with Google Drive, allowing you to save and manage your work conveniently.</li> <li>Pre-installed Libraries: Many popular machine learning libraries, including TensorFlow and PyTorch, come pre-installed in Colab, making it easy to start working on your projects immediately.</li> </ul>"},{"location":"Task1/#getting-started-with-google-colab","title":"Getting Started With Google Colab","text":"<p>To start working with Google Collaboratory Notebook you first need to log in to your Google account, then go to this link Google Colab</p> <ul> <li>Create a new Jupyter Notebook  </li> </ul> <p></p> <ul> <li>On creating a new notebook, it will create a Jupyter notebook with Untitled0.ipynb and save it to your google drive in a folder named Colab Notebooks. Now as it is essentially a Jupyter Notebook, all commands of Jupyter Notebooks will work here. </li> </ul> <p></p> <ul> <li> <p>There might be times when we need to fine-tune models or perform specific tasks that require changing the runtime environment in Colab. Google Colab offers different runtime environments that can be selected based on your requirements:</p> </li> <li> <p>Python Versions: You can select between different versions of Python (e.g., Python 2 or Python 3 )depending on the compatibility of the code and libraries. We will be using Python3 for our lab.</p> </li> <li> <p>Hardware Accelerators: Colab provides access to hardware accelerators, which can be particularly useful for intensive computations. You can choose between:</p> </li> </ul> <pre><code>None: No hardware acceleration, suitable for basic tasks.\n\nGPU: Accelerate your computations with a Graphics Processing Unit.\n\nTPU: Use a Tensor Processing Unit for even faster performance, especially beneficial for deep learning tasks.\n</code></pre> <ul> <li>Change Runtime Environment: Click the \u201cRuntime\u201d dropdown menu at the top of the Colab interface.</li> </ul> <p></p> <ul> <li> <p>Select \u201cChange runtime type\u201d: This will open a dialog box where you can configure the runtime environment.</p> </li> <li> <p>Select Python Version: Choose Python 3 from the \u201cRuntime type\u201d dropdown menu.</p> </li> <li> <p>Select Hardware Accelerator: From the \u201cHardware accelerator\u201d dropdown menu, choose  GPU, or TPU based on your needs.</p> </li> </ul> <p></p> <ul> <li> <p>Save Settings: Click \u201cSave\u201d to apply the changes.</p> </li> <li> <p>New Cell: Whenever you want to copy the code in Google Colab and run it, be sure to click on + Code to add a new code cell.</p> </li> </ul> <p></p> <ul> <li>Execute Code: Click the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\" while the cell is selected.</li> </ul> <p></p>"},{"location":"Task1/#notes-on-gpu-and-tpu-access","title":"Notes: On GPU and TPU Access:","text":"<p>While Google Colab offers free access to GPUs and TPUs, there are limitations. For more consistent access to high-performance GPUs and TPUs, you might need to subscribe to Colab Pro or Colab Pro+ accounts. These paid plans provide priority access to better hardware, longer runtimes, and more memory.</p>"},{"location":"Task1/#using-huggingface-hub-to-share-our-datasets","title":"Using Huggingface Hub to share our Datasets","text":"<p>In this lab, we will be utilizing the Hugging Face Hub to load our custom datasets. Hugging Face provides an extensive repository of datasets that can be easily integrated into your machine learning workflows. For the purposes of this lab, we will demonstrate how to access/upload and use our custom datasets effectively.</p> <p>However, when fine-tuning models in your own work environment, especially if you are using private data, there are important considerations to keep in mind:</p> <ul> <li>Private Datastores: If you are working with proprietary or sensitive data, it is crucial to use your organization's secure datastores. Ensure that all data handling complies with your organization's data privacy policies and regulations.</li> <li>Hugging Face Datasets: If you prefer to use Hugging Face for dataset storage and management, make sure to mark your datasets as private. This setting ensures that your data cannot be accessed by anyone outside your organization, maintaining the confidentiality and integrity of your information. Please refer to Huggingface documentation for more info.</li> </ul> <p>Few more Condsideration</p> <ul> <li>Upload Dataset: When uploading your dataset to Hugging Face, choose the appropriate privacy settings. You can set your dataset to private during the upload process.</li> <li>Check Permissions: Regularly review and manage the permissions of your datasets to ensure they remain private and secure.</li> <li>Collaborator Access: If you need to share the dataset with specific team members, use the Hugging Face interface to grant access to trusted collaborators only.</li> </ul> <p>By following these guidelines, you can ensure that your data remains secure while leveraging the powerful tools and resources provided by Hugging Face. This approach not only enhances your workflow efficiency but also upholds the best practices in data security and privacy.</p>"},{"location":"Task1/#accessing-hugging-face-hub-and-creating-account","title":"Accessing Hugging Face Hub and creating account","text":"<p>Hugging Face can be accessed by browsing to huggingface.co</p> <p></p>"},{"location":"Task1/#signing-up","title":"Signing up","text":"<ul> <li>Browse to Hugging Face home page and click on Sign up. Follow the instructions as per below images</li> </ul> <ul> <li>Please check your email address for a confirmation link and click to verify your account </li> </ul> <ul> <li>Organization Creation (Optional): While you can upload datasets and fine-tune models directly on Hugging Face without creating an organization, you have the option to create an organization on Hugging Face. This can be particularly useful for team collaboration, as it allows you to upload all your datasets and models in one centralized location.</li> </ul> <ul> <li>At this stage, you will see no models or datasets created under your account.</li> </ul> <ul> <li>Access Your Models and Datasets: The same can be accessed by clicking your profile picture on the top right corner of the Hugging Face website. This will take you to your personal dashboard where you can view and manage your models and datasets.</li> </ul>"},{"location":"Task1/#hugging-face-api-keys","title":"Hugging Face API Keys","text":"<p>Create an API Key: As we will be uploading our datasets to the Hugging Face Hub, we need to create an API key for our account. This API key will be used to authenticate and interact with the Hugging Face services programmatically.</p> <p>you can browse to huggingface API Key</p> <p>or </p> <p>Click on your profile picture &gt; Settings &gt; Access Tokens</p> <p></p> <p></p> <p>Under the \"Access Tokens\" section, click on \"Create new token.\" You will see options to select the token type and provide a token name. For example, you might name your token \"Webexone\" and select the approperiate permissions.  </p> <ul> <li> <p>fine-grained: tokens with this role can be used to provide fine-grained access to specific resources, such as a specific model or models in a specific organization. This type of token is useful in production environments, as you can use your own token without sharing access to all your resources.</p> </li> <li> <p>read: tokens with this role can only be used to provide read access to repositories you could read. That includes public and private repositories that you, or an organization you\u2019re a member of, own. Use this role if you only need to read content from the Hugging Face Hub (e.g. when downloading private models or doing inference).</p> </li> <li> <p>write: tokens with this role additionally grant write access to the repositories you have write access to. Use this token if you need to create or push content to a repository (e.g., when training a model or modifying a model card).</p> </li> </ul> <p>As we have a lab envoirnment we will be using the \"Write\" permission. This token will have read and write access to all your resources and can make calls to inference API on your behalf, as shown in the image below.</p> <p></p> <p>Save and Secure the Token: Once the token is generated, save it securely. This token will be required for accessing and managing your datasets via the API. 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</p> <p></p>"},{"location":"Task1/#accessing-hugging-face-api-in-google-colab","title":"Accessing Hugging Face API in Google Colab","text":"<ul> <li>Open the Google Colab notebook and navigate to the new \u201cSecrets\u201d section in the sidebar.</li> </ul> <ul> <li> <p>Click on \u201cAdd a new secret.\u201d Enter the name example: HF_TOKEN and value of the secret. Note: The name is permanent once set. </p> </li> <li> <p>The list of secrets is global across all your notebooks.</p> </li> <li> <p>Use the \u201cNotebook access\u201d toggle to grant or revoke access to a secret for each notebook.</p> </li> </ul> <p></p> <p> Optional Steps below  </p>"},{"location":"Task1/#incorporating-secrets-into-your-code-we-will-use-it-later-in-our-lab","title":"Incorporating Secrets into Your Code - We will use it later in our lab","text":"<ul> <li>To use a secret in your notebook, use the following code snippet</li> </ul> <pre><code>from google.colab import userdata\nmy_secret_key = userdata.get('HF_TOKEN')\n</code></pre> <ul> <li>Replace  with your secret's name."},{"location":"Task1/#using-secrets-as-environment-variables-optional-step-we-will-use-it-later-in-our-lab","title":"Using Secrets as Environment Variables -  Optional Step , we will use it later in our lab","text":"<ul> <li>For Python modules requiring API keys as environment variables, use the below snippet:</li> </ul> <pre><code># Import Colab Secrets userdata module\n\nfrom google.colab import userdata\nimport os\n\n# Set other API keys similarly\nos.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n</code></pre>"},{"location":"Task20/","title":"Logging Out and Ending the Lab Session","text":"<p>Close out of the Chrome Browser Close out Webex Close the Windows VM browser tab Log out of the ThousandEyes and close the incognito browser</p> <p>In the Webex Demo Toolbox browser tab scroll to the bottom and click End</p> <p></p> <p>Click End Session in the pop up to shutdown the lab and close the tab</p> <p></p> <p>Time to wrap things and with the Lab Conclusion.</p>"},{"location":"Task3/","title":"Task 3: Log into the Lab Environment","text":"<p>Click My Demo Sessions select your ThousandEyes demo and click the green View button</p> <p></p> <p>Lab Administration Information (New Read Only Admin User)</p> <ul> <li>Webex Control Hub/ThousandEyes Admin Access - A new read only admin user will be created. This is the new user you'll use to access the Webex ControlHub and ThousandEyes Platform (Note: Your user will most likely be different than this one as they are radomly assigned.)</li> <li>Email Address will be used for the SSO login access as well as the Password</li> </ul> <p></p> <ul> <li>Access information to the Webex ControlHub Portal -  right click and use an Incognito Browser</li> </ul> <p></p> <ul> <li>Access information to the ThousandEyes Portal be sure to right click and use an Incognito Browser</li> </ul> <p></p> <p>First thing will be to get your VM set up</p> <ul> <li>Click the Workstation 1 link. You should see a Windows Desktop show up in new browser tab.</li> </ul> <p></p> <p>Right Click on the This PC desktop icon and click Properties so you can rename the VM.</p> <p></p> <p>Click Rename this PC</p> <p></p> <p>Name the PC your-name-Roadshow (no spaces), click Next and click Restart Now</p> <p></p> <p>While the VM reboots go back to your previous tab for demo . Right click the Control Hub URL and ThousandEyes Administration Portal link and open both in incognito browsers as we will be using them in the coming steps. The same creds can be used to login into Webex Control Hub and ThousandEyes  </p> <p></p> <p>Use the credentials from the ControlHub/ThousandEyes Read Only Admin in the next steps.</p> <p></p> <p>Your incognito browser will open to the ThousandEyes SSO login page as shown in the below image. Use the Username and Password from the ThousandEyes Read Only Admin to complete the login process. Click Don\u2019t show this again and Yes in the pop up window after you have completed the SSO login process.</p> <p></p> <p></p> <p>You should now see the ThousandEyes Default Dashboard (see the below image). You can leave this browser open and we will come back to it later.</p> <p></p> <p>Note: Follow the above creds/steps to login into Webex Control Hub as we will be using them in coming steps.</p> <p>Log back into the Windows VM. Navigate back to the tab that had the Windows VM running in it and click \u201cReconnect\u201d or if that tab closed you click on the Workstation 1 link to open a new session to it.</p> <p></p> <p>Note if a Let\u2019s finish setting up your PC window appears just click \u201dRemind me Later\". Also close the CC Cleaner browser and Accept the Webex EULA (if they appear).</p> <p></p> <p>Time to install the ThousandEyes Endpoint Agent (EPA) continue to Task 4.</p>"},{"location":"Task4/","title":"Task 4: Install the ThousandEyes EPA","text":"<p>Once on your VM station,make sure no browser sessions are running on Window VM then click the ThousandEyes Agent Installer</p> <p></p> <p>You should NOT have to do this but if you don\u2019t see the ThousandEyes Agent Installer go back to the tab for the lab and click the Instructions link then copy the link for the Windows x64 Agent Install (MSI) \u2013 Full from the Demo Downloads and Links section. Navigate back to the Windows VM tab, open a browser and paste the link into it to download the installer. Locate the installer and click it to install it.</p> <p></p> <p>Click Next and Accept the License Agreement</p> <p></p> <p>Click the disk icon for ThousandEyes Endpoint Agent and Google Chrome and select Entire feature will be installed on local hard drive then click Next and Finish.</p> <p></p> <p>Note the above step will install ThousandEyes Endpoint agent on the VM provided. As mentioned earlier you can also get the install file from the Instruction tab and install the same on your personal machine (windows/mac)</p>"},{"location":"Task4/#start-up-a-webex-meeting-on-your-vm","title":"Start up a Webex Meeting on your VM","text":"<p>This will generate Webex traffic which we will view later in the lab. Navigate back to your lab information tab. Copy the email address for the character you created to schedule a Webex meeting with. You will use this to login into Webex App and start a Webex meeting session.</p> <p></p> <p>Navigate back to your VM web browser tab and click on the Webex icon then sign in using the email address and password from your character.</p> <p></p> <p></p> <p>Click the Webex icon on the task bar. Click Meetings. Click Start a Personal Room meeting.  Note Your WebexApp might do a quick update.</p> <p></p> <p>Click Join meeting and then click Start meeting.</p> <p></p> <p>This will generate Webex traffic which you will view later in the lab. Note: Since no one is joining the meeting it may have a pop up after a while warning the meeting will end unless you click to keep it active. Feel free to click to keep it active or you can add another participant in that meeting or you can always go back and Start a Personal Room Meeting again.</p>"},{"location":"Task4/#login-to-webex-controlhub","title":"Login to Webex ControlHub","text":"<p>Once the user join a meeting you can login into Webex Control Hub using the (read only admin) credentials provided.  </p> <p></p> <p>Click on Troubleshooting search for your userid that you have used to join the Webex meeting.</p> <p></p>"},{"location":"Task4/#locate-your-thousandeyes-epa","title":"Locate your ThousandEyes EPA","text":"<p>By now you have your ThousandEyes Endpoint Agent running. Let\u2019s verify it has connected to the ThousandEyes Platform.</p> <p>Navigate back to your incognito browser that is logged into ThousandEyes.</p> <p>Click Endpoint Agents &gt; Agent Settings (if you were already in the tab you may have to click refresh)</p> <p>Search for your EPA </p> <ul> <li>In the search bar type the name you gave your VM (if it doesn\u2019t show up contact a lab proctor). Also verify that the Google Chrome Browser Extension shows up as shown in the below screen shot.</li> </ul> <p></p> <p>If your Google Chrome Browser Plug-in didn\u2019t install. (Note sometimes it take a minute to show up. Please refresh the browser.) Navigate back to your VM and click on the Google Chrome icon on the task bar.</p> <ul> <li>Install the Google ThousandEyes Google Chrom Plug-in by pasting this link in your Google Chrome Browser</li> <li>https://chrome.google.com/webstore/detail/thousandeyes-endpoint-age/ddnennmeinlkhkmajmmfaojcnpddnpgb</li> </ul> <p>If asked Turn on Sync. Navigate back to the incognito browser logged into ThousandEyes and your ThousandEyes EPA should look like the one below.</p> <p></p> <p>You are now ready to continue your journey see how AST (Automated Session tests) can automatically monitor Webex meetings. Customize and see how your EPA monitors browser sessions and set up scheduled tests to montior the health of critical applications in the background. Scheduled tests are a great way to provide a baseline for application and network health. The browser sessions provide the view of what the end users session is like when sites are accessed in the domains you are interested in monitoring. And Automated sessions tests will integrate with Webex and monitor traffic automatically whenever a user joins a Webex meeting from Webex App or the RoomOS devices. Let's carry on with Task 5.</p>"},{"location":"Task5/","title":"Task 5: Configure ThousandEyes Automated Session tests and EPA Monitoring","text":""},{"location":"Task5/#set-up-automated-session-tests-for-webex-meetings-optional-step","title":"Set up Automated Session tests for Webex Meetings - Optional Step","text":"<p>Automated Session Tests enable the Endpoint Agents installed on your  workstations to monitor and identify network connections between a user\u2019s application and the destination node (Webex); thereby, removing the ambiguity of knowing whether the IP addresses created in synthetic tests are going to the right datacentre or service. Automated Session Tests capture the performance of a desktop application e.g. Webex App, without you having to manually configure an IP address or hostname for the application.</p> <p>Note: The below configuration steps are for information purpose only</p> <ul> <li>Navigate to the Endpoint Agents &gt; Monitoring Settings &gt; Automated Session Tests tab.</li> <li>Click the Add New Test button.</li> </ul> <p></p> <ul> <li>Create an AST to monitor Webex Meetings in your organization  </li> </ul> <p></p> <p>Note: The above steps have already been configured. Showing for information purpose only. </p> <p>Note: More info can be found on Webex help that explains how to create a configuration between ControlHub and ThousandEyes platform.</p> <p>When the user (that was created by you) initiate and join a Webex meeting from the WebexApp, you be able to view the ThousandEyes path in ControlHub troubleshooting section.</p> <ul> <li>If you have logged out, log back in ControlHub using Readonly admin creds provided . Click on Troubleshooting,  search for your userid that you have used to join the Webex meeting and click the meeting that is in progress.</li> </ul> <p>Note Make sure the Webex meeting is still going on in the virtual machine. As you are the only participant, it will disconnect in some time. If disconnected, please start the meeting again.</p> <p></p> <ul> <li>Click your demo user id</li> </ul> <p></p> <ul> <li>You will notice ThousandEyes path. ThousandEyes AST starts capturing network path data as soon as the meeting starts. However, there could be a delay of up to five minutes before that data populates in Troubleshooting section. </li> </ul> <p></p> <ul> <li>The network path route shows the details for each node that the hop connected to. Click on any of the dotted lines </li> </ul> <p></p> <ul> <li> <p>You can hover over a dotted line to see the user's latency (round-trip) average value during that interval. The color changes depending on the threshold that the value met. Thresholds in Webex are are calculated as:</p> <pre><code>- Poor (red)\u2014Latency &gt; 400ms or Loss (round-trip) &gt; 5%\n- Good (green)\u2014Latency &lt; 300ms or Loss (round-trip) &lt; 3%\n- Fair (yellow)\u2014Neither of the above.\n- Unknown (grey)\u2014Not available yet. You still be able to retrieve the metric from the TE dashboard.\n</code></pre> </li> <li> <p>You can also click on Launch the ThousandEyes dashboard . It will cross-launch from the Webex ControlHub Network Path into ThousandEyes platform. You will be navigated to the Automated Session Tests view, which is found within the Endpoint Agents &gt; Views &gt; Automated Session Tests section of the web application. The view is pre-filtered to the corresponding user and time segment from Webex Control Hub.</p> </li> </ul> <p></p>"},{"location":"Task5/#understanding-scheduled-tests","title":"Understanding Scheduled tests","text":"<p>Before setting up scheduled tests it's a good idea to create an Agent Label and assign your EPA to it. Labels are a super powerful way to dynamically manage how ThousandEyes Endpoint Agents are configured for testing and reporting.</p> <p>For more information on Endpoint Agent Labels check out our documentation.</p> <p>Click the Agent Labels if you are still viewing your EPA otherwise navigate to Endpoint Agents &gt; Agent Settings &gt; Agent Labels.</p> <p>Click Add New Label. Name the label e.g RS_PC, select any color.</p> <p>In the Filter section select Agent in and use the pull down to select your agent (note you can use the search field to quickly find your agent). Click Save. See the image below for and example of how to add your agent to your label.</p> <p></p>"},{"location":"Task5/#set-up-scheduled-tests-for-your-thousandeyes-epa","title":"Set up Scheduled Tests for your ThousandEyes EPA","text":"<p>You will build a test that will run every minute to https://office.com for this lab but feel free to build other tests. You can have up to 10 tests run on an EPA.</p> <p>Navigate to Endpoint Agents &gt; Monitoring Settings</p> <p>Click Add New Test</p> <ul> <li>Type Web, HTTP Server (this is the default)</li> <li>Test Name: Set a unique name</li> <li>URL: https://office.com</li> <li>Interval: 1 minute</li> <li>Agents: Agent label then select the label you created</li> <li>Click the prioritize slider (as this is an important test)</li> <li>Click Add New Test (note you can also Run Once but if you do this don\u2019t forget to save the test!)</li> </ul> <p></p>"},{"location":"Task5/#set-up-browser-sessions-for-your-thousandeyes-epa","title":"Set up Browser Sessions for your ThousandEyes EPA","text":"<p>Set up domains to be monitored when the browser on the Windows VM accesses them (Note: Only Chrome, Edge or IE 11+ browsers are supported). Typically these consist of the domains that are critical for your users and businses operations. You'll see shortly how powerful this can for troubleshooting users issues. </p> <p>Navigate to Endpoint Agents &gt; Monitoring Settings &gt; Browser Sessions. For more information on Browser Session monitoring click here.</p> <p>Click Add New Monitored Domain Set</p> <ul> <li>Domain Set Name: Set a unique name</li> <li>Monitored Domains: office.com, webex.com, cisco.com (feel free to add in other domains)</li> <li>Agents: Agent Labels and select your agent label that you created earlier  </li> <li>Click Add New Monitored Domain Set</li> </ul> <p></p>"},{"location":"Task5/#automated-sessions-tests-ast-for-your-thousandeyes-epa","title":"Automated Sessions Tests (AST) for your ThousandEyes EPA","text":""},{"location":"Task5/#navigate-back-to-your-vm","title":"Navigate back to your VM","text":"<p>Open the Chrome Browser and navigate to the domains you set for browser sessions. Open a tab and bring up a site in each tab:</p> <ul> <li>https://cisco.com</li> <li>https://webex.com</li> <li>https://office.com</li> </ul> <p></p> <p>Click refresh on each tab a few times to create some extra sessions. Do this for a few minutes randomly to generate web browser traffic which we will analyze later. If you need to test out having browser traffic randomly generated to a site or set of sites a great chrome plug in to use is Auto Refresh Plus. We won't go into configuring or installing it for this lab.</p> <p>You should have some good data captured with the Webex session running in the background on the VM being catpured with the Webex AST, the scheduled tests runing in background providing a baseline and then the browser sessions. Time to move onto Task 6 and start analyzing the data!</p>"},{"location":"Task6/","title":"Analyzing ThousandEyes EPA Data","text":"<p>Typically when a user experiences an issue they will either suffer in silence, wait for the issue to go away and when it crops up again get frustrated and call in for help. Sometimes while they are working with support or the helpdesk the issue resolves itself. This makes it really difficult to troubleshoot issues if they aren't presistent. ThousandEyes stores 30 days of data so you have the abilty to go back in time and see what was occuring or see if this issue has been occuring and the user just didn't call into support.</p>"},{"location":"Task6/#code-annotation","title":"Code Annotation","text":""},{"location":"Task6/#codeblocks","title":"CODEBLOCKS","text":"<p>Some <code>Code</code> will be below:</p>"},{"location":"Task6/#a-plain-code-block","title":"A plain Code Block","text":"<pre><code>Some Code here\n\nprint(A)\n</code></pre> omer.py<pre><code>Some Code here\n\nprint(A)\n</code></pre> <pre><code>Some Code here\n\nprint(A)\n</code></pre> <pre><code>Some Code here\n\nprint(A)\n</code></pre> <p> If you are troubleshooting a issue and want to quickly see all the test data for a users system Agent Views provides the perfect visualization.</p> <p>Navigate to VM session and browse to ThousandEyes portal. Open Endpoint Agents &gt; Agent Views (be sure to use the incognito browser). Use the search box to find your agent which will be what you named your VM. Then you can begin to explore the metrics and tests. The top section contains your system\u2019s performance metrics.</p> <ul> <li>Use the Metrics pull down to view the different system metrics</li> </ul> <p></p> <p>The middle section shows Automated Session Tests (ASTs) and Scheduled Tests (up to 10 max).</p> <ul> <li>Use the pull down to view the different web and network metrics.</li> </ul> <p></p> <p>The bottom section shows browser sessions. If you don\u2019t see anything you might need to refresh the tabs in your VM\u2019s browser a few extra times or refresh the Agent Views page.</p> <ul> <li>Use the pull down to view the different browser and network metrics.</li> </ul> <p></p>"},{"location":"Task6/#drill-into-a-automated-session-test","title":"Drill into a Automated Session Test","text":"<p>Now that you\u2019ve explored the Agent Views page click on the AST metric to dive into the detailed view associated with the Webex Test. If you don\u2019t have any network loss try changing the metric to latency.</p> <p></p> <p>You will now see a filtered view for your agent showing the network path which is dynamically created based on the endpoints your Webex session is connected to while the Webex session is running.</p> <p>Here\u2019s an example view. I used the upper right Save and Share feature to create a snapshot that anyone can view for a full year! This can be very helpful as it can be attached to tickets, shared with the end user, anoter team or a service provider. They will see what you see making it simple to collaborate and reduce any finger pointing. (think back to the last slide in the ThousandEyes Overview).</p> <ul> <li>Open the snapshot in another browser tab</li> <li>You can mouse over the nodes for extra information</li> <li>Try changing the metric and drag over and navigate the time bar to see how the test changes</li> </ul> <p></p> <p>From this same view you can change the grouping to show IP Addresses and adjust the number of hops to get more a granular hop by hop network path.</p> <ul> <li>You can mouse over the nodes and links for extra information.</li> <li>You can also adjust the link delay. In the below screenshot used 10ms. You can also click on the 8 links to see all links highlighted that are over 10ms.</li> </ul> <p></p> <p>Feel free to navigate back to your Automated Session Test view showing your agent and click around to expermiement with how to see similar data.</p>"},{"location":"Task6/#drill-into-scheduled-tests","title":"Drill into Scheduled Tests","text":"<p>Scheduled tests provide a consistent baseline. Whenever an Endpoint Agent is online the test runs at the specified interval. Navigate back to Agent Views and search and select your agent.</p> <ul> <li>Click Agent Views and use the search box to find your agent.</li> <li>Click into the scheduled test to drill into a filtered view. (If you set up other scheduled tests feel free to use one of them instead)</li> </ul> <p></p> <p>Now you're in a filtered test view. Note the filters at the top of the screen showing the test and your agent. See the below screen shot as an example.</p> <ul> <li>Adjust the metric to latency and change the grouping to IP Address</li> <li>Adjust the hops to max using the slider</li> <li>Set the link delay to 10ms and then click on the links to highlight them in the network path</li> </ul> <p></p> <p>If you wanted to see all agents that are runnig the scheduled test you could just remove the agent filter (this won't work in this lab as we have only one agent). This would be a great way to figure out if an issue is effecting multiple users or start to isolate the users that are having issues as they might all be in the same building or connecting through the same router that is causing congestion, latency or loss. You can also right click on the agent to pull up the traceroute which is run every time the test is executed.</p> <p>Another powerful feature is the Run Now option for when you are actively troubleshooting and need test results right away or you can make changes and run it. Feel free to test it out.</p> <p></p> <p>Note: It might take a few minutes for the test to complete and then be processed for visualization. So you may want to kick it off and then continue on with the lab and come back to the tab in a few minutes.</p>"},{"location":"Task6/#drill-into-browser-sessions","title":"Drill into Browser Sessions","text":"<p>Browser Sessions are generated using the browser plugin and will monitor the user experience and network path for your monitored domain set. The experience score is a calculated metric to help gauge the end users web performance. You can change the metric based on what you are wanting to use for troubleshooting. Navigate back to Agent Views and search and select your agent. Click on the www.cisco.com session to drill into a filtered view. Note if your browser sessions aren\u2019t showing up, verify your agent is associated with the agent label and then make sure to go back to the browser and refresh the tabs.</p> <p></p> <p>The browser session view shows the visited pages and experience score. See the below screenshot for an example. Note the filter for the Visited Site and Agent which was automatically applied based on the browser session test you clicked from the Agent View.</p> <p></p> <p>You can easily explore the other metrics from the session. Click on the web site to bring up the session, system and network details and review the tabs for the path trace and waterfall details. Additional metric will be pulled in based on how the Enpoint Agent is connected to the network like wireless, vpn or proxy.</p> <p> </p> <p>ThousandEyes takes a layered approach with visualizing the data to help you isolate and troubleshoot issues. The Network Layer will show the network path filter on the browser session. Click on the Network Layer that is associated with the Browser Session. Explore the different metrics that are captured in context in time like CPU Load.</p> <p></p>"},{"location":"Task6/#local-network-view","title":"Local Network View","text":"<p>The Local Networks view will provide the local network and DNS information. This can be very helpful for finding bad DNS settings or Wi-Fi related issues. Click into the Local Networks &gt; Network Access layer and make sure your agent is still in the agent filter or clear it out with the X to see all agents. Explore the different metrics as well using the pull down. If this was a production environment the filters could be used for isolation of issues.</p> <p></p>"},{"location":"Task6/#endpoint-agent-overview","title":"Endpoint Agent Overview","text":"<p>This page provides a quick overall health view of your Endpoint Agents. Navigate to the Endpoint Agents &gt; Overview to see all the stats from all the agents. Try adjusting the time range as well.</p> <p></p> <p>Now that you've learned about the metrics, tests and various way that ThousandEyes can help you troubleshoot, isolate and resolve end users issues you're one short step from the final stage of this learning jounery. The last step will be learning about some of the ways a dashboard can be used to visualize the endpoint agent data and shared with other stakeholders. Check out Task 7 to learn about Endpoint Agent data in Dashboards.</p>"},{"location":"Task7/","title":"Understanding Fine-Tuning - Large Language Models","text":""},{"location":"Task7/#fine-tuning-llms","title":"Fine-Tuning LLMs","text":"<p>Goals: Equip audience with knowledge and practical skills in finetuning techniques accompanied by code examples. Discuss the approach to finetuning LLMs.</p> <p>The section centers around fine-tuning LLMs, addressing their various aspects and methodologies. As the module progresses, the focus will be given to specialized instruction tuning techniques, namely \u00a0LoRA. It will examine domain-specific applications (Webex Calling), ensuring a holistic understanding of fine-tuning techniques and their real-world implications.</p> <ul> <li>Techniques for Finetuning LLMs: The lesson highlights the challenges, particularly the resource intensity of traditional approaches. We will introduce instruction tuning methods like \u00a0LoRA.</li> <li>Deep Dive into LoRA and SFT: This lesson offers an in-depth exploration of LoRA and SFT techniques. We will uncover the mechanics and underlying principles of these methods.</li> <li>Finetuning using LoRA : This lesson guides a practical application of LoRA and SFT to finetune an LLM to follow instructions, using data from the \u201cHuggingFace Dataset\u201d .</li> </ul>"},{"location":"Task7/#techniques-for-finetuning-llms","title":"Techniques for Finetuning LLMs","text":""},{"location":"Task7/#introduction","title":"Introduction","text":"<p>In this lesson, we will examine the main techniques for fine-tuning Large Language Models for superior performance on specific tasks. We explore why and how to fine-tune LLMs, the strategic importance of instruction fine-tuning, and several fine-tuning methods, such as  Low-Rank Adaptation (LoRA), Supervised Finetuning (SFT). We also touch upon the benefits of the Parameter-Efficient Fine-tuning (PEFT) approach using Hugging Face's PEFT library, promising both efficiency and performance gains in fine-tuning.</p>"},{"location":"Task7/#why-we-finetune-llms","title":"Why We Finetune LLMs","text":"<p>While pretrained Large Language Models (LLMs) provide a broad understanding of language, it doesn't equip them with the specialized knowledge needed for complex tasks. For instance, a pre-trained LLM may excel at generating text but encounter difficulties when tasked with sentiment analysis or even providing information from your own Knowledge base. This is where fine-tuning comes into play.</p> <p>Fine-tuning is the process of adapting a pretrained model to a specific task by further training it using task-specific data. For example, if we aim to make an LLM proficient in answering questions about Webex Calling or Webex CC, we would fine-tune it using a dataset comprising Webex question-answer pairs. This process enables the model to recalibrate its internal parameters and representations to align with the intended task, enhancing its capacity to address domain-specific challenges effectively.</p> <p>However, fine-tuning LLMs conventionally can be resource-intensive and costly. It involves adjusting  the parameters in the pretrained LLM models, which can number in the billions, necessitating significant computational power and time. Consequently, it's crucial to explore more efficient and cost-effective methods for fine-tuning, such as Low-Rank Adaptation (LoRA).</p>"},{"location":"Task7/#a-reminder-on-instruction-and-conversational-finetuning","title":"A Reminder On Instruction and Conversational Finetuning","text":"<p>In Conversational fine tuning the model engages in a dialogue with the user, maintaining context over multiple turns.The interaction mimics a natural conversation, with the model responding in a way that feels like a human interlocutor.</p> <p>Instruction fine-tuning is a specific type of fine-tuning that grants precise control over a model's behavior. The objective is to train a Language Model (LLM) to interpret prompts as instructions rather than simply treating them as text to continue generating. </p>"},{"location":"Task7/#introduction-to-efficient-finetuning-with-parameter-efficient-fine-tuning-peft","title":"Introduction to Efficient Finetuning with Parameter-Efficient Fine-tuning (PEFT)","text":"<p>Parameter-Efficient Fine-tuning (PEFT) approaches address the need for computational and storage efficiency in fine-tuning LLMs. Hugging Face developed the PEFT library specifically for this purpose. PEFT leverages architectures that only fine-tune a small number of additional model parameters while freezing most parameters of the pretrained LLMs, significantly reducing computational and storage costs.</p> <p>PEFT methods offer benefits beyond just efficiency. These methods have been proven to outperform standard fine-tuning methods, particularly in low-data situations, and provide improved generalization for out-of-domain scenarios. Furthermore, they contribute to the portability of models by generating tiny model checkpoints that require substantially less storage space compared to extensive full fine-tuning checkpoints.</p> <p>The PEFT library supports popular methods such as Low-Rank Adaptation (LoRA) and Prompt Tuning. </p>"},{"location":"Task7/#a-reminder-of-the-techniques-for-finetuning-llms","title":"A Reminder of the Techniques For Finetuning LLMs","text":"<p>There are several techniques to make the finetuning process more efficient and effective:</p> <ul> <li> <p>Full Finetuning: This method involves adjusting all the parameters in the pretrained LLM models to adapt to a specific task. While effective, it is resource-intensive and requires extensive computational power, therefore it\u2019s rarely used. Not in our Scope</p> </li> <li> <p>Low-Rank Adaptation (LoRA): LoRA is a technique that aims to adapt LLMs to specific tasks and datasets while simultaneously reducing computational resources and costs. By applying low-rank approximations to the downstream layers of LLMs, LoRA significantly reduces the number of parameters to be trained, thereby lowering the GPU memory requirements and training costs. We\u2019ll also see QLoRA, a variant of LoRA that is more optimized and leverages quantization.</p> </li> </ul> <p>With a focus on the number of parameters involved in finetuning, there are multiple methods, such as:</p> <ul> <li> <p>Supervised Finetuning (SFT): SFT involves doing standard supervised finetuning with a pretrained LLM on a small amount of demonstration data. This method is less resource-intensive than full finetuning but still requires significant computational power. Scope of this lab</p> </li> <li> <p>Reinforcement Learning from Human Feedback (RLHF): RLHF is a training methodology where models are trained to follow human feedback over multiple iterations. This method can be more effective than SFT, as it allows for continuous improvement based on human feedback. We\u2019ll also see some alternatives to RLHF, such as Direct Preference Optimization (DPO), and Reinforcement Learning from AI Feedback (RLAIF).Not in our Scope</p> </li> </ul>"},{"location":"Task7/#conclusion","title":"Conclusion","text":"<p>In this lesson, we've learned that while pretraining equips LLMs with a broad understanding of language, fine-tuning is necessary to specialize these models for complex tasks. We've looked into various fine-tuning techniques, including Full Finetuning, Low-Rank Adaptation (LoRA), Supervised Finetuning (SFT), and Reinforcement Learning from Human Feedback (RLHF). </p>"},{"location":"Task8/","title":"Deep Dive into LoRA and SFT","text":""},{"location":"Task8/#introduction","title":"Introduction","text":"<p>In this lesson, we will dive deeper into the mechanics of LoRA, a powerful method for optimizing the fine-tuning process of Large Language Models, its practical uses in various fine-tuning tasks, and the open-source resources that simplify its implementation. We will also introduce QLoRA, a highly efficient version of LoRA. By the end of this lesson, you will have an in-depth understanding of how LoRA and QLoRA can enhance the efficiency and accessibility of fine-tuning LLMs.</p>"},{"location":"Task8/#the-functioning-of-lora-in-fine-tuning-llms","title":"The Functioning of LoRA in Fine-tuning LLMs","text":"<p>LoRA, or Low-Rank Adaptation, is a method developed by Microsoft researchers to optimize the fine-tuning of Large Language Models. This technique tackles the issues related to the fine-tuning process, such as extensive memory demands and computational inefficiency. LoRA introduces a compact set of parameters, referred to as low-rank matrices, to store the necessary changes in the model instead of altering all parameters.</p> <p>Here are the key features of how LoRA operates:</p> <ul> <li>Maintaining Pretrained Weights: LoRA adopts a unique strategy by preserving the pretrained weights of the model. This approach reduces the risk of catastrophic forgetting, ensuring the model maintains the valuable knowledge it gained during pretraining.</li> <li>Efficient Rank-Decomposition: LoRA incorporates rank-decomposition weight matrices, known as update matrices, to the existing weights. Rank decomposition is a way to simplify a large, complex matric by breaking it down into the product of smaller matrices. These update matrices have significantly fewer parameters than the original model, making them highly memory-efficient. By training only these newly added weights, LoRA achieves a faster training process with reduced memory demands. These LoRA matrices are typically integrated into the attention layers of the original model.</li> </ul> <p>By using the low-rank decomposition approach, the memory demands for training large language models are significantly reduced. This allows running fine-tuning tasks on consumer-grade GPUs, making the benefits of LoRA available to a broader range of researchers and developers.</p> <p></p>"},{"location":"Task8/#open-source-resources-for-lora","title":"Open-source Resources for LoRA","text":"<p>The following libraries offer a mix of tools that enhance the efficiency of fine-tuning large language models. They provide optimizations, compatibility with different data types, resource efficiency, and user-friendly interfaces that accommodate various tasks and hardware configurations.</p> <ul> <li>PEFT Library: Parameter-efficient fine-tuning (PEFT) methods facilitate efficient adaptation of pre-trained language models to various downstream applications without fine-tuning all the model's parameters. By fine-tuning only a portion of the model's parameters, PEFT methods like LoRA, Prefix Tuning, and P-Tuning, including QLoRA, significantly reduce computational and storage costs.</li> <li>Lit-GPT: Lit-GPT from LightningAI is an open-source resource designed to simplify the fine-tuning process, making it easier to apply LoRA's techniques without manually altering the core model architecture. Models available for this purpose include Vicuna, Pythia, and Falcon. Specific configurations can be applied to different weight matrices, and precision settings can be adjusted to manage memory consumption.</li> </ul> <p>In our lab, we\u2019ll focus on the PEFT library.</p>"},{"location":"Task8/#qlora-an-efficient-variant-of-lora","title":"QLoRA: An Efficient Variant of LoRA","text":"<p>QLoRA, or Quantized Low-Rank Adaptation, is a popular variant of LoRA that makes fine-tuning large language models even more efficient. QLoRA introduces several innovations to save memory without sacrificing performance.</p> <p>The technique involves backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low-Rank Adapters. This approach significantly reduces memory usage, enabling the fine-tuning of even larger models on consumer-grade GPUs. For instance, QLoRA can fine-tune a 65 billion parameter model on a single 48GB GPU while preserving full 16-bit fine-tuning task performance.</p> <p>QLoRA uses a new data type known as 4-bit NormalFloat (NF4), which is optimal for normally distributed weights. It also employs double quantization to reduce the average memory footprint by quantizing the quantization constants and paged optimizers to manage memory spikes.</p> <p>The practical implementation of QLoRA for fine-tuning LLMs is very accessible, thanks to open-source libraries and tools. For instance, the BitsAndBytes library offers functionalities for 4-bit quantization. In the coming sections we will see a code example showing how to use QLoRA with PEFT.</p>"},{"location":"Task8/#understanding-quantization","title":"Understanding Quantization","text":"<p>As large language models, grow more advanced, their increasing number of parameters leads to significant memory usage. This, in turn, increases the costs of hosting and deploying these tools.</p> <p>In this section, we will quickly learn about quantization. Quantization is a process that converts data that\u2019s in your memory from high bit to low bit rate.</p>"},{"location":"Task8/#quantization-in-machine-learning","title":"Quantization in Machine Learning","text":"<p>Large Language Models (LLMs) like GPT or Llama have billions of parameters (floating point values) representing their weights and biases, a characteristic known as precision. The precision of these data types directly impacts the amount of memory required by the model. Using higher precision types, such as Float32 or Float64, offers greater accuracy but demands more memory.</p> <p>Quantization can significantly reduce this memory requirement by representing parameters with fewer bits. For example, converting from 32-bit to smaller bit representations allows us to perform inference with the models more efficiently. Once models are quantized, they can be deployed on edge devices like phones, smartwatches, etc. However, using lower precision types, such as Float16 or BFloat16 or Int8, while saving memory, may result in a slight loss of accuracy. To overcome this loss in accuracy, techniques like Lora and QLoRA can be utilized.</p> <p>Benefits of Quantization:</p>"},{"location":"Task8/#fp16-quantization","title":"FP16 Quantization:","text":"<ul> <li>Reduces the model size by half.</li> <li>Maintains higher precision than INT8.</li> <li>Suitable for GPUs and TPUs that support FP16 arithmetic.</li> </ul>"},{"location":"Task8/#int8-quantization","title":"INT8 Quantization:","text":"<ul> <li>Significantly reduces model size and improves latency.</li> <li>Suitable for CPUs and hardware accelerators that support INT8 arithmetic.</li> </ul> <p>By applying quantization, we can make large models more efficient and capable of running on a variety of devices, ensuring a balance between performance and resource utilization.</p> <p>In the figure below, you can see the main floating point data types.</p> <p></p>"},{"location":"Task8/#why-quantization-uses-less-memory","title":"Why Quantization uses less memory","text":"<p>As mentioned above, the primary advantage of using quantization (4-bit) is the reduction in model size and memory usage. Here's a simple explanation:</p> <ul> <li>A float32 number takes up 32 bits of memory.</li> <li>A 4-bit quantized number takes up only 4 bits of memory. So, theoretically, you can fit 8 times more 4-bit quantized numbers into the same memory space as float32 numbers. This allows you to load larger models into the GPU memory or use smaller GPUs that might not have been able to handle the model otherwise.</li> </ul> <p>Note: The amount of memory used by an integer in a computer system is directly related to the number of bits used to represent that integer.</p> <p>Memory Usage for 4-bit Integer A 4-bit integer uses 4 bits of memory</p> <p>Memory Usage for 32-bit Integer A 32-bit integer uses 32 bits of memory.</p> <p>Conversion to Bytes To convert these to bytes (since memory is often measured in bytes):</p> <pre><code>1 byte = 8 bits\nA 4-bit integer would use ( 4/8 = 0.5 ) bytes.\nA 16-bit integer would use ( 16/8 = 2 ) bytes.\n</code></pre> <p>Lets Understand by taking Llama 2 as an example:</p> <p>For example, you may come across config like this in Llama 2 model:</p> <p>Note: The bitsandbytes library will be used for quantization and fine-tuning of models. It provides tools to reduce memory footprint and improve computational efficiency.</p> <pre><code>bnb_config = transformers.BitsAndBytesConfig(\nload_in_4bit=True,\nbnb_4bit_quant_type='nf4',\nbnb_4bit_use_double_quant=True,\nbnb_4bit_compute_dtype=bfloat16\n)\nload_in_4bit=True //Enables 4-bit quantization.\nbnb_4bit_quant_type='nf4' //Specifies the type of 4-bit quantization.\nbnb_4bit_use_double_quant=True //Enables double quantization for better accuracy.\nbnb_4bit_compute_dtype=bfloat16 //Specifies the data type for computation, which is bfloat16 here.\n</code></pre> <p>By using 4-bit quantization, you can load the Llama 2 model with significantly less GPU memory, making it more accessible for devices with limited resources.</p>"},{"location":"Task8/#understanding-quantization-lab-example","title":"Understanding Quantization - Lab Example","text":"<p>We will be using and running the code in Google Colab. This code demonstrates how much memory can be saved by using 4-bit quantization compared to 32-bit floating-point precision for a hypothetical model with 1000 weights. It calculates the total memory required for both 32-bit and 4-bit representations, determines the memory savings, and converts the result to bytes for easier understanding.</p> <p>By quantizing the model weights from 32-bit to 4-bit, significant memory savings are achieved, making it more efficient to deploy models on resource-constrained devices like phones and smartwatches.</p> <p>If you have not yet created a Google Colab account, please refer to the following section</p> <ul> <li>Browse to Google Colab and create a new Notebook</li> </ul> <p></p> <ul> <li>Select your runtime envoirnment</li> </ul> <p></p> <p> NOTE:  As we will be running a hypothetical model in Google Colab, you can use the CPU to run the code below. Our goal is to understand how quantization can help us use less memory.</p> <p> Reminder:  Whenever you want to copy the below code  into Google Colab and run it on individual line, be sure to click on + Code to add a new code cell. Also make sure to press the Play button to execute the code</p> <p></p> <p></p> <ul> <li>Let's break down the code step by step and explain what it does:</li> </ul> <p> Step 1:  Define Memory Weight for Float32:</p> <pre><code>float32_memory = 32  # in bits\nnum_weights = 1000  # hypothetical number of weights\n</code></pre> <ul> <li>float32_memory is set to 32 bits, which is the memory required for each weight when using 32-bit floating-point precision (Float32).</li> <li>num_weights is set to 1000, representing a hypothetical number of weights in the model.</li> </ul> <p> Step 2:  Calculate Total Memory for Float32 Weights:</p> <pre><code>float32_total_memory = float32_memory * num_weights  # in bits\n</code></pre> <ul> <li>float32_total_memory calculates the total memory required for all the weights when using Float32. This is done by multiplying the memory per weight (float32_memory) by the number of weights (num_weights).</li> </ul> <p> Step 3:  CDefine Memory Per Weight for 4-bit Quantization:</p> <pre><code>bit4_memory = 4  # in bits\n</code></pre> <ul> <li>bit4_memory is set to 4 bits, which is the memory required for each weight when using 4-bit quantization.</li> </ul> <p> Step 4:  Calculate Total Memory for 4-bit Quantized Weights:</p> <pre><code>bit4_total_memory = bit4_memory * num_weights  # in bits\n</code></pre> <ul> <li>bit4_total_memory calculates the total memory required for all the weights when using 4-bit quantization. This is done by multiplying the memory per weight (bit4_memory) by the number of weights (num_weights).</li> </ul> <p> Step 5:  Calculate Memory Saved:</p> <pre><code>memory_saved = float32_total_memory - bit4_total_memory  # in bits\nmemory_saved_in_bytes = memory_saved / 8  # convert bits to bytes\n</code></pre> <ul> <li>memory_saved calculates the total amount of memory saved by using 4-bit quantization instead of Float32. This is done by subtracting the total memory required for 4-bit weights (bit4_total_memory) from the total memory required for Float32 weights (float32_total_memory).</li> <li>memory_saved_in_bytes converts the saved memory from bits to bytes by dividing by 8 (since there are 8 bits in a byte).</li> </ul> <p> Step 5:  Print the Result: <pre><code>print(f\"Memory saved by using 4-bit quantization: {memory_saved_in_bytes} bytes\")\n</code></pre>  OUTPUT </p> <p><code>Memory saved by using 4-bit quantization: 3500.0 bytes</code></p> <ul> <li>Will changing the model from 32-bit to 4-bit reduce the model's accuracy?  Yes </li> </ul> <p> NOTE:  Let's sets up the context of the code, indicating that reducing the bit-width from 32-bit to 4-bit quantization can introduce a loss of accuracy. Please us ethe same notebook as above to continue with your code.</p> <p> Step 1:  Import Python Library NumPY</p> <p><pre><code>import numpy as np\n</code></pre> * Import the NumPy library, which is used for numerical operations on arrays.</p> <p> Step 2:  Simulate Original float32 Weights:</p> <p><pre><code>original_weights = np.random.rand(1000).astype(np.float32)\n</code></pre> * Generate an array of 1000 random floating-point numbers between 0 and 1, simulating the original model weights with 32-bit precision (float32).</p> <p> Step 3:  Simulate 4-bit Quantized Weights:</p> <p><pre><code>quantized_weights = np.round(original_weights * 15).astype(np.uint8)\n</code></pre> * Normalize the original weights to a range of 0 to 15 because 4-bit integers can represent 16 values (0 to 15). * Use np.round() to round the normalized weights to the nearest integer. * Convert the rounded values to 8-bit unsigned integers (np.uint8), which effectively simulates the 4-bit quantized weights.</p> <p> Step 4:  De-normalize to Get the Approximated Original Weights:</p> <p><pre><code>approximated_weights = quantized_weights / 15.0\n</code></pre> * De-normalize the quantized weights back to the original range by dividing by 15.0. This gives an approximation of the original weights after quantization.</p> <p> Step 5:  Calculate the Error</p> <p><pre><code>error = np.abs(original_weights - approximated_weights).mean()\n</code></pre> * Calculate the absolute difference between the original weights and the approximated weights. * Compute the mean of these differences to get the average quantization error.</p> <p> Step 6:  Print the Result:</p> <p><pre><code>print(f\"Average Quantization Error: {error}\")\n</code></pre> * Print the average quantization error to show the potential loss of accuracy introduced by reducing the bit-width from 32-bit to 4-bit.</p> <p> OUTPUT  </p> <p><code>Average Quantization Error: 0.016423573800129817</code></p> <p> Optional Step  Lets see the original generated randon floating point weights </p> <pre><code>original_weights\n</code></pre> <p><code>0.9047668 , 0.81753767, 0.15176044, 0.82469964, 0.26190883,        0.25543845, 0.91670144, 0.44082588, 0.8387138 , 0.5301147 ,        0.597587  , 0.28645265, 0.31866026, 0.8412814 , 0.87486124,        0.14635882, 0.00842665, 0.40396062, 0.7154632 , 0.92024857,        0.58492464, 0.369594  , 0.3199917 , 0.01947883, 0.5505348 ,        0.09161595, 0.75082487, 0.9818501 , 0.800069  , 0.55406606,        0.87903994, 0.6254798 , 0.8290317 , 0.73905164, 0.42614278,        0.6899402 , 0.03985518, 0.3132978 , 0.97768533, 0.42348453,        0.60850483, 0.80669063, 0.18966372, 0.79855096, 0.73046243,        0.3299927 , 0.79309106, 0.01327937, 0.01851322, 0.9269705 ,        0.8274083 , 0.45073316, 0.51967096, 0.7210426 , 0.41424063,        0.24505323, 0.90779364, 0.64724004, 0.65470797, 0.14317876,        0.91511154, 0.57397956, 0.83594376, 0.09163648, 0.3037902 ,        0.01362608, 0.92493033, 0.6721083 , 0.5777128 , 0.98029524,        0.29148692, 0.5840548 , 0.805403  , 0.17140059, 0.9880919 ,        0.01023464, 0.8627494 , 0.89710003, 0.76805186, 0.9278038 ,        0.9041322 , 0.19642776, 0.78401035, 0.16597563, 0.16709319,        0.41655192, 0.6231926 , 0.95298046, 0.6890473 , 0.79470664,        0.7721617 , 0.2800831 , 0.32485998, 0.83056533, 0.84955883,        0.9345336 , 0.20025656, 0.31240928, 0.0943019 , 0.92295516,        0.07533643, 0.2782531 , 0.34772104, 0.20622289, 0.6932999 ,        0.01451688, 0.05406734, 0.69295406, 0.56339973, 0.74457246,        0.8313878 , 0.51609147, 0.4473876 , 0.29768255, 0.9012652 ,        0.7957006 , 0.5588959 , 0.54948616, 0.41233262, 0.04758253,        0.09688636, 0.19657251, 0.60875326, 0.6305743 , 0.62236613,        0.53727114, 0.60597694, 0.27432016, 0.5396689 , 0.35570994,        0.7779527 , 0.62765694, 0.5046841 , 0.95913595, 0.37506285,        0.06490876, 0.5690066 , 0.5926366 , 0.8343261 , 0.00439525,        0.75573623, 0.89352626, 0.45979103, 0.16634902, 0.2184745,        0.39872894, 0.68869066, 0.08809475, 0.8567979 , 0.2187864,        0.21870795, 0.52296937, 0.16986693, 0.6928503 , 0.41990125,        0.5135413 , 0.45609972, 0.38769007, 0.00987561, 0.15869647,        0.80649745, 0.8347802 , 0.7420551 , 0.07340008, 0.6842359 ,        0.33549812, 0.6722021 , 0.3444218 , 0.7548876 , 0.67199737,        0.04117178, 0.24504112, 0.39629644, 0.6458071 , 0.4220033 ,        0.44719183, 0.00884627, 0.8429784 , 0.5802084 , 0.07012173,        0.2552524 , 0.23461716, 0.08043662, 0.17875864, 0.35063305,        0.99479717, 0.3229556 , 0.7217851 , 0.09103325, 0.00775268,        0.3796975 , 0.24862018, 0.32581842, 0.530216  , 0.71165705,        0.5170614 , 0.06309021, 0.6869316 , 0.9265822 , 0.04267561,        0.44075203, 0.28398433, 0.46985227, 0.5829586 , 0.23592392,        0.95805424, 0.52453846, 0.8329666 , 0.14974083, 0.48412293,        0.04571012, 0.8544567 , 0.89506125, 0.10473824, 0.29405326,        0.29569003, 0.59090966, 0.66576856, 0.35182896, 0.12008226,        0.49510583, 0.8308251 , 0.14861059, 0.5513083 , 0.45685318,        0.8333833 , 0.9543581 , 0.42451006, 0.6116371 , 0.76232755,        0.81956553, 0.89240205, 0.2516925 , 0.92773354, 0.9864879 ,        0.04117434, 0.78647643, 0.21779937, 0.3965154 , 0.6441789 ,        0.6636646 , 0.547766  , 0.8827409 , 0.24284157, 0.1969708 ,        0.5668731 , 0.78194785, 0.6832345 , 0.10209882, 0.97626406,        0.5010292 , 0.12451114, 0.32503366, 0.17754969, 0.53745407],       dtype=float32)</code></p> <p> Optional Step  Lets see the quantized_weights (4- bit)</p> <pre><code>quantized_weights\n</code></pre> <p><code>6,  1,  2,  1,  8, 14,  3,  9, 13,  1, 10,  5, 14, 12, 13,  9,  2,         4,  1,  6,  7,  5, 15,  3,  0,  9,  9, 12,  2,  6, 11,  4,  3, 13,         8,  7, 14,  4,  9, 10,  0,  8,  9,  6,  9, 10,  8,  4,  1, 13,  1,         7,  3, 13,  5,  3, 11,  6, 12,  8,  8,  2,  8, 13, 10,  6,  6,  0,         0,  3,  7, 10, 11,  4, 11, 13,  3,  8,  8,  1,  5,  5,  7,  2, 13,         5, 12,  4, 11,  6,  9,  6,  8, 13,  4, 13,  6,  6, 15, 13,  2,  0,         4, 15,  7,  7,  5,  5,  2,  4,  8,  8, 15,  6, 11,  9, 12,  1, 13,         3, 14,  8,  1,  5, 15,  1,  9,  7,  1,  1, 13, 11, 12,  4,  5,  9,        14, 11, 14,  0, 12, 12,  2,  4,  9, 10,  8,  7,  3,  7,  9,  1, 13,         7,  9,  7, 12, 14, 11,  3, 11,  9, 11,  1, 13,  9,  4,  5,  2, 15,         8, 13,  1,  9,  3, 15,  3,  3, 12,  8, 14,  7, 11, 13,  3,  0, 10,        15, 11, 14,  4, 11,  7,  6, 13,  1,  4, 10,  1, 11,  9,  2, 10, 11,         8,  1,  5,  7,  8, 11, 13,  3,  3, 14,  4, 10, 10,  1,  6, 12, 13,        12,  7,  6,  8,  9, 10,  7, 12, 11,  1,  7, 14, 13,  8, 14,  1, 13,         7,  9, 12, 14, 14,  1, 14, 14, 11, 13,  2,  9, 11,  1, 13,  1, 13,         2,  8, 12,  1,  6,  7,  1, 12,  7,  4,  6,  8,  9,  8,  4,  1,  7,        13,  7, 12,  0,  6,  7, 11, 15,  7,  6, 13, 12, 12, 10,  8,  9, 12,         0, 10,  1, 13,  9,  3,  2, 14,  9,  6,  9,  6,  4, 15,  5,  5,  5,        12,  5, 15,  4,  7,  0,  3,  3, 15, 15,  3, 10,  2, 15,  7,  8,  2,         2, 10, 11, 14,  4,  0,  8,  1,  8,  5,  1, 12,  5,  6, 14, 11, 10,         2,  6, 11, 12,  8, 12,  0, 14, 12,  4,  9, 11,  7, 14,  2,  4,  4,        10,  4, 13,  1, 14,  0, 15,  8,  6,  5, 12, 10,  6,  1,  4,  7,  7,         7, 10, 10,  5,  8, 12,  8,  6, 12,  2,  1, 14, 15, 13,  9,  3,  3,         8,  7,  3, 10,  7,  1, 10,  7,  6,  4,  8,  2, 11,  2, 15,  2, 11,        12, 10,  5,  7, 13,  2,  6, 15, 13,  6,  1, 10, 12, 10, 11,  4,  1,         8,  3,  4, 12, 15, 12,  9,  8,  1, 12,  2,  2, 14,  2,  9,  7,  4,         4,  3,  2, 11,  0,  5,  9, 12, 15, 14, 13,  1, 15,  3, 12, 10,  3,        13,  5,  8,  9, 15,  2, 14,  8,  8,  6,  7,  1,  6,  3, 15,  3,  0,        13,  9,  6, 12,  5,  5,  5,  9,  9,  2, 13, 15,  3,  9, 11,  1,  8,        13,  9, 12, 14,  5,  9,  2,  8, 14, 10,  4, 12,  8,  9, 10, 13, 14,         3,  1, 14,  9,  9, 12,  2,  1,  7,  4,  2,  0,  4, 10,  2,  6,  3,        10, 10,  9, 11,  9,  8, 15, 12,  3,  2,  5,  9,  6, 11,  1,  9,  5,        14,  7, 14,  4, 13, 14,  5,  8, 14, 10, 12,  3,  1, 12,  9, 14,  1,         2, 11, 11,  2,  2, 13, 12,  3,  4, 12,  7,  1,  9,  3,  4,  4, 14,        14,  8,  0, 12,  8,  6, 12, 13,  6, 11,  6, 10,  9,  6,  2, 14, 10,         4, 10,  2, 15,  9, 14,  0,  4,  0, 10,  1, 15,  5,  7, 10,  6, 12,        10, 11,  6,  7,  2, 14,  8, 12,  7, 14,  9, 13,  0,  5,  7,  2, 11,         9, 11, 13, 10, 14,  8,  8, 14,  4, 10,  1, 11, 15, 10,  0,  6,  0,        13, 14,  9,  4,  4, 12,  5,  9,  4,  6,  2, 10,  3,  4,  1, 14, 11,         6,  7,  1,  4, 11,  1,  7, 12,  8, 14,  2,  8,  8,  3, 11, 15, 10,         2, 14, 12,  2, 12,  4,  4, 14,  7, 13,  8,  9,  4,  5, 13, 13,  2,         0,  6, 11, 14,  9,  6,  5,  0,  8,  1, 11, 15, 12,  8, 13,  9, 12,        11,  6, 10,  1,  5, 15,  6,  9, 12,  3, 12, 11,  5, 12,  0,  0, 14,        12,  7,  8, 11,  6,  4, 14, 10, 10,  2, 14,  9, 13,  1,  5,  0, 14,        10,  9, 15,  4,  9, 12,  3, 15,  0, 13, 13, 12, 14, 14,  3, 12,  2,         3,  6,  9, 14, 10, 12, 12,  4,  5, 12, 13, 14,  3,  5,  1, 14,  1,         4,  5,  3, 10,  0,  1, 10,  8, 11, 12,  8,  7,  4, 14, 12,  8,  8,         6,  1,  1,  3,  9,  9,  9,  8,  9,  4,  8,  5, 12,  9,  8, 14,  6,         1,  9,  9, 13,  0, 11, 13,  7,  2,  3,  3, 14, 12, 11,  4, 13, 15,         4,  7, 13, 13,  0,  8, 13, 13, 15,  8, 15,  3, 11,  4,  8,  9,  2,         2, 11,  3,  7,  4,  7, 11,  6, 10,  1, 13,  3,  8,  3, 10,  6,  8,         7,  6,  0,  2, 12, 13, 11,  1, 10,  5, 10,  5, 11, 10,  1,  4,  6,        10,  6,  7,  0, 13,  9,  1,  4,  4,  1,  3,  5, 15,  5, 11,  1,  0,         6,  4,  5,  8, 11,  8,  1, 10, 14,  1,  7,  4,  7,  9,  4, 14,  8,        12,  2,  7,  1, 13, 13,  2,  4,  4,  9, 10,  5,  2,  7, 12,  2,  8,         7, 13, 14,  6,  9, 11, 12, 13,  4, 14, 15,  1, 12,  3,  6, 10, 10,         8, 13,  4,  3,  9, 12, 10,  2, 15,  8,  2,  5,  3,        dtype=uint8)</code></p>"},{"location":"Task8/#conclusion","title":"Conclusion","text":"<p>In this lesson, we explored LoRA and QLoRA, two powerful techniques for fine-tuning Large Language Models (LLMs). We examined how LoRA operates by preserving pretrained weights and introducing low-rank matrices, thereby enhancing the memory and computational efficiency of the fine-tuning process. We also highlighted open-source libraries like PEFT and Lit-GPT that facilitate the implementation of LoRA.</p> <p>Additionally, we discussed QLoRA, an efficient variant of LoRA that employs 4-bit NormalFloat and double quantization to further reduce memory usage. This technique allows for significant memory savings but can introduce quantization errors, impacting model accuracy.</p> <p>By quantizing weights to lower precision, we achieve reduced memory usage and potentially increased processing speed, at the cost of a possible minor loss in accuracy. The provided code example illustrates this trade-off by converting a set of 32-bit floating-point weights into 4-bit integer weights, demonstrating the balance between memory efficiency and model accuracy.</p> <p>Overall, you have successfully learned how to use quantization to optimize LLMs, understanding the benefits and potential drawbacks of this approach.</p>"},{"location":"Task8a/","title":"Fine Tuning - High Level Diagram","text":""},{"location":"Task8a/#introduction","title":"Introduction","text":"<p>This diagram represents the workflow of training and using a LLM. Here\u2019s a step-by-step explanation:</p> <ul> <li>Build Dataset: Once we have determined that fine-tuning is the right solution, we need to create a dataset to fine-tune our model. Datasets are collections of structured or unstructured data used to train machine learning models.</li> <li>Choose Base Model: A base model is selected. This could be a pre-trained model that will be fine-tuned with the new dataset e.g Llama2 or Llama3 (in our lab)</li> <li>Setup Lora Adapter: The LoRA (Low-Rank Adaptation) adapter is set up a technique used to fine-tune the model efficiently.</li> <li>Train and Monitor: The model is trained using the dataset. During training, the process is monitored to ensure it\u2019s proceeding correctly and to make adjustments if necessary.</li> <li>Merged Model: Once the model has been tested and validated, it is considered a merged model. This model is ready to handle user queries.</li> <li>User Interaction: Users can now interact with the model by sending queries. The model processes these queries and returns appropriate responses.</li> </ul>"},{"location":"Task8a/#datasets-creation","title":"Datasets Creation","text":"<p>There are several ways to create datasets:</p> <ul> <li>Using existing open source datasets e.g The Pile, Common Crawl , Wiki or even HuggingFace</li> <li>Using LLM to create synthetic datasets faster and can be less expensive</li> <li>Using your own custom datasets e.t.c &lt;-- Focus for our lab</li> </ul>"},{"location":"Task8a/#custom-datasets","title":"Custom datasets","text":"<p>As we will be using and fine tuning Llam2 and Llam3 base models we need to convert them(datasets) into a uniform format compatible with training regimens.</p>"},{"location":"Task8a/#example-format-for-llama3-instruct","title":"Example Format for Llama3 - Instruct","text":"Sample Format - Dont Copy - reference ONLY<pre><code>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\nYou are a helpful AI assistant for  Collaboration&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\nWhat can you help me with?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n</code></pre> <ul> <li>&lt;|begin_of_text|&gt;: Specifies the start of the prompt</li> <li>&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;: Specifies the role for the following message, i.e. \u201csystem\u201d You are a helpful AI assistant for Collaboration</li> <li>&lt;|eot_id|&gt;: Specifies the end of the input message</li> <li>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;: Specifies the role for the following message i.e. \u201cuser\u201d What can you help me with?: The user message</li> <li>&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;: Ends with the assistant header, to prompt the model to start generation.</li> </ul>"},{"location":"Task8a/#example-format-for-llama2","title":"Example Format for Llama2","text":"Sample Format - Dont Copy - reference ONLY<pre><code>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\n{{ system_prompt }}\n&lt;&lt;/SYS&gt;&gt;\n{{ user_message }} [/INST] Model answer &lt;/s&gt;\n</code></pre>"},{"location":"Task8a/#datasets-creation_1","title":"Datasets Creation","text":"<p>After defining the problem and confirming that LLMs are a suitable solution, the next step is to prepare a dataset for fine-tuning. If you already have a clean, high-quality dataset, that's great. However, let's assume you don't have one readily available.</p> <p>In my scenario, I was able to generate synthetic dataset from the Cisco Preferred Architecture for Webex Calling.</p>"},{"location":"Task8a/#creating-a-dataset-from-a-pdf-summary","title":"Creating a Dataset from a PDF - Summary","text":"<p>To delve into the intricacies of dataset creation, let's explore a scenario where we aim to train an AI to replicate our preferred architecture. In this lab, we will transform a PDF document into a dataset. Specifically, we will use a PDF about Webex Calling as our source material.</p> <p>Our goal is to convert the PDF into a dynamic and engaging dialogue, similar to an interview format. This will allow us to create a more interactive and informative dataset.</p> <p>Steps Involved</p> <p>Converting the PDF to Text</p> <ul> <li>Begin by extracting the text content from the PDF document.</li> </ul> <p>Paragraph Analysis and Segmentation:</p> <ul> <li>Once the PDF is in text form, analyze and segment it into individual paragraphs. Combine short paragraphs and split longer ones to ensure each segment is coherent and self-contained, while still contributing to the overall narrative.</li> </ul> <p>Generating Interview Questions:</p> <ul> <li>For each paragraph, create an artificial interview scenario where a language model (LLM) acts as the interviewer.</li> <li>Develop questions that naturally elicit responses based on the content of the paragraph.</li> <li>The objective is to simulate an insightful dialogue, making it seem as though the PDF is responding to questions about its content and experiences.</li> </ul> <p>By following these steps, we will convert the static information in the PDF into a dynamic and interactive dataset, facilitating a deeper understanding and more engaging learning experience. You can find the complete code that worked for me here.</p> <p>Here's an example of the resulting file. Please note that for this lab demonstration, only a few data points were converted from the PDF.</p> <p>Note: We will use the following file to create and eventually train downstream tasks for our LLM, specifically the Llama2 model.</p>"},{"location":"Task8a/#sample-file","title":"Sample File","text":"example.txt<pre><code>human_text: What is the solution provided by Webex Calling? assistant_text: Webex Calling provides enterprise-grade PBX features, functionality, and performance.\nhuman_text: What options are available for PSTN access in Webex Calling? assistant_text: The options are Cisco Calling Plans, Cloud Connected PSTN, and Premises-based PSTN.\nhuman_text: What type of devices does Webex Calling support? assistant_text: Webex Calling supports Cisco MPP Series IP Phones, Webex Devices, and a variety of third-party devices.\nhuman_text: How are Webex Calling data centers distributed? assistant_text: Webex Calling data centers are globally distributed and geo-redundant, located in six regions: US, Canada, Europe, UK, Australia, and Japan.\nhuman_text: What is the function of the Local Gateway in Webex Calling? assistant_text: The Local Gateway connects to the PSTN and registers with Webex Calling to handle all calls between the PSTN and Webex Calling.\nhuman_text: What are the key benefits of Webex Calling for large enterprises? assistant_text: Webex Calling offers scalability, advanced call management, and integration with collaboration tools for large enterprises.\nhuman_text: How does Webex Calling handle call encryption? assistant_text: SRTP and TLS are used in Webex Calling to encrypt media and signaling for secure communications.\nhuman_text: What features are included in the Webex Calling analytics suite? assistant_text: Call quality, usage patterns, and performance metrics are part of the Webex Calling analytics suite.\nhuman_text: How does Webex Calling integrate with third-party devices? assistant_text: Integration with third-party SIP phones provides flexibility in device choices for Webex Calling users.\nhuman_text: What disaster recovery mechanisms are available in Webex Calling? assistant_text: Geo-redundant data centers and automatic failover mechanisms support disaster recovery in Webex Calling.\nhuman_text: How does Webex Calling manage international call routing? assistant_text: A global network of data centers manages international call routing in Webex Calling.\nhuman_text: What role does the Webex Control Hub play in Webex Calling? assistant_text: Provisioning, configuring, and managing Webex Calling services are handled through the Webex Control Hub.\nhuman_text: What are the main security protocols used in Webex Calling? assistant_text: SRTP and TLS are the main security protocols used in Webex Calling.\nhuman_text: How does Webex Calling support mobile users? assistant_text: The Webex App enables mobile users to make and receive calls on smartphones and tablets in Webex Calling.\nhuman_text: What are the benefits of Webex Calling for remote teams? assistant_text: Remote teams benefit from enterprise-grade calling features, mobility, and collaboration tool integration in Webex Calling.\nhuman_text: How does Webex Calling ensure compliance with regulatory requirements? assistant_text: Compliance with regulatory requirements in Webex Calling is ensured through call recording, monitoring, and secure data handling.\nhuman_text: What redundancy features are built into Webex Calling? assistant_text: Geo-redundant data centers and automatic failover ensure continuous service in Webex Calling.\nhuman_text: What user management capabilities are available in Webex Calling? assistant_text: User settings and permissions are managed through the Webex Control Hub in Webex Calling.\nhuman_text: How does Webex Calling handle call routing? assistant_text: Dial plans and customizable routing rules manage call routing in Webex Calling.\nhuman_text: What kind of customer support does Cisco provide for Webex Calling? assistant_text: Technical assistance, training, and deployment resources are part of Cisco's support for Webex Calling.\nhuman_text: How does Webex Calling support hybrid deployments? assistant_text: Integration with cloud and on-premises PBX systems supports hybrid deployments in Webex Calling.\nhuman_text: What are the call management features in Webex Calling? assistant_text: Call management features in Webex Calling include call forwarding, call transfer, call hold, and voicemail.\nhuman_text: How does Webex Calling support video calls? assistant_text: Video calls are supported between video-capable devices in Webex Calling.\nhuman_text: What compliance features are available in Webex Calling? assistant_text: Compliance features in Webex Calling include call recording, logging, and monitoring.\nhuman_text: What is the function of SRTP in Webex Calling? assistant_text: SRTP encrypts media streams in Webex Calling to ensure secure communication.\nhuman_text: How does Webex Calling support call analytics? assistant_text: Webex Calling provides detailed call analytics on call quality, usage patterns, and performance.\nhuman_text: What kind of redundancy features are available in Webex Calling? assistant_text: Redundancy features in Webex Calling include geo-redundant data centers and automatic failover.\nhuman_text: How does Webex Calling ensure secure voice communications? assistant_text: Encryption protocols like SRTP for media and TLS for signaling ensure secure voice communications in Webex Calling.\nhuman_text: What integrations does Webex Calling offer with contact center solutions? assistant_text: Integration with Webex Contact Center offers advanced features like call routing, IVR, and analytics.\nhuman_text: How does Webex Calling handle call monitoring and recording? assistant_text: Call monitoring and recording features in Webex Calling support compliance, training, and quality assurance.\nhuman_text: What options are available for user authentication in Webex Calling? assistant_text: Webex Calling supports user authentication through secure protocols and integration with identity providers for SSO.\nhuman_text: What is the role of the Webex App in Webex Calling? assistant_text: The Webex App unifies messaging, video conferencing, and calling for a seamless communication experience.\nhuman_text: How does Webex Calling support international business operations? assistant_text: Global data centers support reliable and high-quality voice communication for international operations in Webex Calling.\nhuman_text: What management capabilities does Webex Calling provide? assistant_text: The Webex Control Hub provides centralized provisioning, configuration, and monitoring for Webex Calling services.\nhuman_text: How does Webex Calling handle voice and video integration? assistant_text: Integration with Webex Meetings and Webex Teams allows for seamless voice and video communication in Webex Calling.\nhuman_text: What are the benefits of Webex Calling for large enterprises? assistant_text: Scalability, advanced call management, and integration with collaboration tools are benefits for large enterprises using Webex Calling.\nhuman_text: What disaster recovery options are available with Webex Calling? assistant_text: Geo-redundant data centers and automatic failover provide disaster recovery options in Webex Calling.\nhuman_text: How does Webex Calling manage call quality? assistant_text: Call quality in Webex Calling is managed through network optimization, advanced codecs, and QoS policies.\nhuman_text: What user profile management features are available in Webex Calling? assistant_text: User profile management features in Webex Calling include settings and permissions configuration through the Webex Control Hub.\nhuman_text: How does Webex Calling integrate with CRM systems? assistant_text: Integration with CRM systems enhances customer interactions and business processes in Webex Calling.\nhuman_text: What compliance features does Webex Calling offer? assistant_text: Compliance features in Webex Calling include call recording, monitoring, and secure data handling.\nhuman_text: How does Webex Calling support remote teams? assistant_text: Remote teams benefit from enterprise-grade calling features, mobility, and collaboration tool integration in Webex Calling.\nhuman_text: What are the main security features of Webex Calling? assistant_text: Security features in Webex Calling include encryption, secure voice, and industry-standard compliance.\nhuman_text: How does Webex Calling handle emergency call routing? assistant_text: Predefined routing rules manage emergency call routing in Webex Calling.\nhuman_text: What kind of support does Cisco provide for Webex Calling? assistant_text: Technical assistance, training, and deployment resources are part of Cisco's support for Webex Calling.\nhuman_text: How does Webex Calling support hybrid work environments? assistant_text: Webex Calling supports hybrid work environments by integrating with cloud and on-premises PBX systems.\nhuman_text: What management tools are available in Webex Calling? assistant_text: Management tools in Webex Calling include the Webex Control Hub for provisioning, configuring, and managing services.\nhuman_text: How does Webex Calling handle call quality issues? assistant_text: Tools for monitoring and diagnosing issues ensure high-quality voice communication in Webex Calling.\nhuman_text: What kind of analytics does Webex Calling offer? assistant_text: Analytics on call quality, usage patterns, and performance metrics are available in Webex Calling.\nhuman_text: What devices are compatible with Webex Calling? assistant_text: Compatible devices for Webex Calling include Cisco IP Phones, Webex Room Devices, and third-party SIP phones.\nhuman_text: How does Webex Calling integrate with Webex Meetings? assistant_text: Integration with Webex Meetings provides a seamless experience for scheduling and joining video meetings in Webex Calling.\nhuman_text: What role does Webex Control Hub play in Webex Calling? assistant_text: Webex Control Hub provides a centralized interface for provisioning, configuring, and managing Webex Calling services.\nhuman_text: How does Webex Calling ensure security for voice communications? assistant_text: Encryption protocols like SRTP for media and TLS for signaling ensure security in Webex Calling.\nhuman_text: What features does Webex Calling offer for team collaboration? assistant_text: Group call management, conferencing, and Webex Teams integration are features of Webex Calling for team collaboration.\nhuman_text: How is call recording handled in Webex Calling? assistant_text: Built-in call recording features in Webex Calling support compliance, training, and quality assurance.\nhuman_text: What scalability options are available in Webex Calling? assistant_text: Scalability in Webex Calling supports small businesses to large enterprises, accommodating thousands of users.\nhuman_text: How does Webex Calling support international operations? assistant_text: Global data centers ensure reliable and high-quality voice communication for international operations in Webex Calling.\nhuman_text: What redundancy features are built into Webex Calling? assistant_text: Redundancy features in Webex Calling include geo-redundant data centers and automatic failover for continuous service.\nhuman_text: How does Webex Calling handle user authentication? assistant_text: Secure protocols and integration with identity providers for single sign-on (SSO) manage user authentication in Webex Calling.\nhuman_text: What reporting capabilities does Webex Calling provide? assistant_text: Detailed call analytics, usage patterns, and performance metrics are part of Webex Calling's reporting capabilities.\nhuman_text: What integrations does Webex Calling support with CRM systems? assistant_text: Integrations with CRM systems in Webex Calling enhance customer interactions and streamline business processes.\nhuman_text: How does Webex Calling support compliance with regulations? assistant_text: Compliance features in Webex Calling include call recording, monitoring, and secure data handling.\nhuman_text: What is the role of SRTP in Webex Calling? assistant_text: SRTP encrypts voice media streams to ensure secure communication in Webex Calling.\nhuman_text: How does Webex Calling manage user profiles? assistant_text: Webex Control Hub manages user profiles in Webex Calling, enabling configuration of settings and permissions by administrators.\nhuman_text: How does Webex Calling support video conferencing? assistant_text: Webex Calling supports video conferencing via integration with Webex Meetings.\nhuman_text: What is the role of a Local Gateway in Webex Calling? assistant_text: The Local Gateway connects to the PSTN and handles call routing between Webex Calling and on-premises PBX systems.\nhuman_text: How does Webex Calling handle number porting? assistant_text: Webex Calling allows users to port existing phone numbers from another provider.\nhuman_text: How does Webex Calling support disaster recovery? assistant_text: Geo-redundant data centers and automatic failover mechanisms support disaster recovery in Webex Calling.\nhuman_text: What is the Webex Calling architecture? assistant_text: Webex Calling has a cloud-based architecture with distributed data centers for high availability and resilience.\nhuman_text: How does Webex Calling support call encryption? assistant_text: SRTP and TLS are used to encrypt call signaling and media in Webex Calling.\nhuman_text: What are the benefits of Webex Calling for small businesses? assistant_text: Small businesses benefit from enterprise-grade calling features, scalability, and collaboration tool integration at a lower cost with Webex Calling.\nhuman_text: How does Webex Calling integrate with existing collaboration tools? assistant_text: Integration with Webex Teams and Microsoft Teams offers a seamless collaboration experience in Webex Calling.\nhuman_text: What analytics features are available in Webex Calling? assistant_text: Analytics features in Webex Calling include call quality monitoring, usage reports, and performance dashboards.\nhuman_text: How does Webex Calling support mobile devices? assistant_text: The Webex App enables Webex Calling users to make and receive calls on mobile devices.\nhuman_text: What is the role of the Webex App in Webex Calling? assistant_text: The Webex App offers a unified interface for messaging, video conferencing, and calling in Webex Calling.\nhuman_text: How does Webex Calling handle multi-location deployments? assistant_text: Centralized management and consistent calling features support multi-location deployments in Webex Calling.\nhuman_text: What troubleshooting tools are available in Webex Calling? assistant_text: Troubleshooting tools in Webex Calling include diagnostic reports, call logs, and real-time call quality monitoring.\nhuman_text: How does Webex Calling ensure voice quality? assistant_text: Voice quality in Webex Calling is ensured by network optimization, advanced codecs, and QoS policies.\nhuman_text: What integrations does Webex Calling support? assistant_text: Integrations with CRM systems, collaboration tools, and contact center solutions are supported by Webex Calling.\nhuman_text: How does Webex Calling handle call recording? assistant_text: Call recording in Webex Calling is available for compliance, training, and quality assurance.\nhuman_text: What role do data centers play in Webex Calling? assistant_text: Data centers provide redundancy, high availability, and disaster recovery in Webex Calling.\nhuman_text: What are the benefits of Webex Calling for remote teams? assistant_text: Enterprise-grade calling features, mobility, and collaboration tool integration benefit remote teams using Webex Calling.\nhuman_text: How does Webex Calling handle call forwarding? assistant_text: Flexible call forwarding options in Webex Calling allow users to redirect calls to other numbers or devices.\nhuman_text: What is Webex Calling? assistant_text: Webex Calling is a cloud-based phone system offering enterprise-grade calling features.\nhuman_text: What types of phones are supported by Webex Calling? assistant_text: Webex Calling supports Cisco MPP Series IP Phones, Webex Devices, and various third-party devices.\nhuman_text: How does Webex Calling integrate with other Webex services? assistant_text: Webex Calling integrates with Webex Meetings, Webex Teams, and Webex Contact Center.\nhuman_text: What are the deployment options for Webex Calling? assistant_text: Webex Calling offers cloud-only and hybrid deployment options.\nhuman_text: How is call quality maintained in Webex Calling? assistant_text: Advanced codecs, network optimization, and QoS policies maintain call quality in Webex Calling.\nhuman_text: What is the purpose of the Webex Control Hub? assistant_text: The Webex Control Hub centralizes management for provisioning, configuring, and managing Webex Calling services.\nhuman_text: How does Webex Calling support remote work? assistant_text: Webex Calling supports remote work by offering enterprise-grade calling features and connectivity from any location with internet access.\nhuman_text: What security measures are implemented in Webex Calling? assistant_text: Encryption, secure voice, and industry-standard compliance are key security measures in Webex Calling.\nhuman_text: How does Webex Calling handle emergency calls? assistant_text: Emergency calls in Webex Calling are handled by comparing the dial string with defined emergency numbers.\nhuman_text: What are the benefits of Webex Calling for enterprises? assistant_text: Lower maintenance costs, scalability, remote work support, and collaboration tool integration are benefits for enterprises using Webex Calling.\nhuman_text: How does Webex Calling manage call routing? assistant_text: Dial plans and route groups manage call routing in Webex Calling.\nhuman_text: What customer support is available for Webex Calling? assistant_text: Webex Calling customer support includes technical assistance, training, and deployment resources from Cisco.\nhuman_text: How does Webex Calling ensure high availability? assistant_text: Geo-redundant data centers and a redundant global backbone network ensure high availability in Webex Calling.\nhuman_text: What are the international calling capabilities of Webex Calling? assistant_text: International calling is supported by Webex Calling's global data center network for high-quality voice communication.\nhuman_text: What is the role of SIP in Webex Calling? assistant_text: SIP manages signaling and multimedia communication sessions in Webex Calling.\nhuman_text: How does Webex Calling handle compliance with local regulations? assistant_text: Compliance with local regulations is ensured by routing calls through regional data centers and supporting lawful intercept.\nhuman_text: What integration options are available for Webex Calling? assistant_text: Webex Calling integrates with Webex Meetings, Webex Teams, and third-party tools for enhanced productivity.\nhuman_text: How does Webex Calling support contact centers? assistant_text: Advanced contact center features like call routing, IVR, and analytics are provided through Webex Contact Center integration.\nhuman_text: What is the function of SRTP in Webex Calling? assistant_text: SRTP encrypts media streams to ensure secure communication in Webex Calling.\nhuman_text: How does Webex Calling handle video calls? assistant_text: Video calls are supported between video-capable MPP phones, Webex Devices, and the Webex App in Webex Calling.\nhuman_text: What are the benefits of Webex Calling for remote workers? assistant_text: Webex Calling offers remote workers enterprise-grade calling features and connectivity from any location with internet access.\nhuman_text: What is the role of the Webex Control Hub in managing Webex Calling? assistant_text: The Webex Control Hub centralizes management for provisioning, configuring, and managing Webex Calling services.\nhuman_text: How does Webex Calling integrate with existing on-premises PBX systems? assistant_text: Webex Calling integrates with on-premises PBX systems via Local Gateways for hybrid deployment.\nhuman_text: What is the significance of SRTP in Webex Calling? assistant_text: SRTP encrypts media streams in Webex Calling to ensure secure communication.\nhuman_text: How does Webex Calling support compliance with local regulations? assistant_text: Webex Calling complies with local regulations by using regional data centers and supporting lawful intercept.\nhuman_text: What features are available for call management in Webex Calling? assistant_text: Webex Calling offers call forwarding, call transfer, call hold, and voicemail features.\nhuman_text: What options are available for integrating Webex Calling with other collaboration tools? assistant_text: Webex Calling integrates with Webex Meetings, Webex Teams, and third-party tools for enhanced productivity.\nhuman_text: How does Webex Calling handle call routing? assistant_text: Dial plans and route groups manage call routing in Webex Calling.\nhuman_text: What security features are built into Webex Calling? assistant_text: Webex Calling features encryption, secure voice, and industry-standard compliance for communication security.\nhuman_text: How does Webex Calling support mobile users? assistant_text: The Webex App enables mobile users to make and receive calls on their devices with Webex Calling.\nhuman_text: How does Webex Calling integrate with Cisco devices? assistant_text: Cisco MPP phones and Webex Room devices integrate with Webex Calling for unified communication.\nhuman_text: What are the deployment models available for Webex Calling? assistant_text: Webex Calling supports cloud-only and hybrid deployment models.\nhuman_text: How does Webex Calling handle international calling? assistant_text: International calling is supported by Webex Calling's global data center network for high-quality voice communication.\nhuman_text: What features does Webex Calling offer for contact centers? assistant_text: Advanced contact center features like call routing, IVR, and analytics are offered through Webex Contact Center integration.\nhuman_text: How does Webex Calling support scalability? assistant_text: The cloud-based architecture of Webex Calling supports scalability for businesses of all sizes.\nhuman_text: What type of customer support is available for Webex Calling? assistant_text: Webex Calling customer support includes technical assistance, training, and deployment resources from Cisco.\nhuman_text: How does Webex Calling handle voice quality? assistant_text: High voice quality in Webex Calling is ensured by advanced codecs, network optimization, and QoS policies.\nhuman_text: What are the benefits of using Webex Calling over traditional PBX systems? assistant_text: Benefits of Webex Calling include lower maintenance costs, scalability, remote work support, and collaboration tool integration.\nhuman_text: What signaling and media protocols does Webex Calling use? assistant_text: Webex Calling uses SIP for signaling and SRTP for media.\nhuman_text: What is the role of the Webex Control Hub in Webex Calling? assistant_text: The Webex Control Hub provides connection parameters and digest credentials for SIP authentication during Local Gateway registration.\nhuman_text: What group features does Webex Calling provide? assistant_text: Webex Calling provides group features like unlimited subscriptions of auto-attendants, hunt groups, and call queues.\nhuman_text: What are the regional platforms for Webex Calling? assistant_text: Webex Calling operates regional platforms in the US, Canada, UK, Europe, APJC Japan, and APJC Australia.\nhuman_text: What role do load balancers play in Webex Calling datacenters? assistant_text: Load balancers are used to build a scalable, redundant datacenter architecture.\nhuman_text: What are the capabilities of the Webex App in Webex Calling? assistant_text: The Webex App supports mid-call features, rich presence, and control of the user's Cisco MPP phone.\nhuman_text: What is the role of the Webex Control Hub in Local Gateway registration? assistant_text: The Webex Control Hub provides connection parameters and digest credentials for SIP authentication during Local Gateway registration.\nhuman_text: What deployment options are available for Webex Calling? assistant_text: Webex Calling can be deployed as a cloud-only solution or as part of a hybrid cloud.\nhuman_text: What are the data center locations for Webex Calling in the US? assistant_text: Webex Calling data centers in the US are located in Dallas, Chicago, and New York.\nhuman_text: What capabilities does the Webex App offer when integrated with Webex Calling? assistant_text: The Webex App offers messaging, screen sharing, audio and video conferencing, and integrated calling with mid-call features or control of a user\u2019s desk phone.\nhuman_text: What are the considerations for video calls in Webex Calling? assistant_text: Video calls can be made within a single Webex Calling org between video-capable devices or Webex App, while PSTN only supports voice calls.\nhuman_text: What are the main functions hosted in each Webex Calling datacenter? assistant_text: Webex Calling datacenters host call routing functions, provide provisioning interface access, and host access and peering SBCs.\nhuman_text: How does Webex Calling ensure firewall traversal for calls? assistant_text: Webex Calling ensures firewall traversal by using TLS connections initiated by phones and Local Gateways, and sending traffic back through the same connection.\nhuman_text: How are trunks used in Webex Calling? assistant_text: Trunks connect Webex Calling with Local Gateways or Dedicated Instances.\nhuman_text: What is the Private Network Connect (PNC) solution? assistant_text: The Private Network Connect (PNC) solution extends private networks to the cloud for high quality of service and low latency.\nhuman_text: What type of subscription is Webex Calling based on? assistant_text: Webex Calling uses a subscription-based licensing model managed with the Cisco Collaboration Flex Plan.\nhuman_text: Where are Webex Calling data centers located? assistant_text: Webex Calling data centers are located in the US, Canada, Europe, UK, Australia, and Japan.\nhuman_text: How does Webex Calling handle emergency calls? assistant_text: Emergency calls are handled by comparing the dial string with emergency numbers defined in the national numbering plan.\nhuman_text: How are unknown numbers handled in Webex Calling? assistant_text: Unknown numbers are handled based on the 'Unknown Number Handling' and 'Calls to On-Premises Extension' settings.\nhuman_text: What is the purpose of dial plans in Webex Calling? assistant_text: Dial plans enable call routing to premises-based call control instances based on dial patterns.\nhuman_text: What are the benefits of using route groups in Webex Calling? assistant_text: Route groups provide redundancy and increased capacity by grouping multiple trunks together.\nhuman_text: What happens if ICE negotiation fails in Webex Calling? assistant_text: If ICE negotiation fails, media is anchored on the Webex Calling Access SBC, resulting in media flowing through the customer\u2019s Internet edge to the SBC and back to the destination endpoint.\nhuman_text: What are route groups in Webex Calling? assistant_text: Route groups provide redundancy or increased capacity by grouping multiple trunks together.\nhuman_text: What is required for Webex Calling endpoints to connect to the datacenters? assistant_text: Webex Calling endpoints use the public Internet to connect to datacenters and establish over-the-top TLS connections.\nhuman_text: What features does the Dedicated Instance option provide in Webex Calling? assistant_text: The Dedicated Instance option provides a Cisco Unified Communications Manager based stack of applications in a private cloud dedicated to a single customer.\nhuman_text: How does Webex Calling handle PSTN access? assistant_text: Webex Calling handles PSTN access through Cisco Calling Plans, Cloud Connected PSTN, and Premises-based PSTN.\nhuman_text: What are the benefits of the Webex Calling global backbone? assistant_text: The global backbone optimizes media round-trip times and ensures high availability with a multi-gigabit, fully redundant network.\nhuman_text: What is the purpose of a Local Gateway in Webex Calling? assistant_text: The Local Gateway connects to the PSTN and registers with Webex Calling to handle all calls between the PSTN and Webex Calling.\nhuman_text: What is the significance of media path optimization in Webex Calling? assistant_text: Media path optimization establishes a direct media path between entities to reduce bandwidth usage and improve call quality.\nhuman_text: What is Webex Edge Connect? assistant_text: Webex Edge Connect peers Webex meetings and Webex Calling traffic with an Equinix Cloud Exchange (ECX) location to improve user experience with guaranteed bandwidth and QoS.\nhuman_text: What is the Webex Calling solution overview? assistant_text: Webex Calling provides enterprise-grade PBX features, functionality, and performance.\nhuman_text: What connectivity options are available for Webex Calling? assistant_text: Connectivity options for Webex Calling include Over-the-top (OTT) Internet, Webex Edge Connect, and Private Network Connect.\nhuman_text: What type of patterns can be included in a Webex Calling dial plan? assistant_text: Dial plans can include numeric patterns and domain patterns for routing SIP URIs.\nhuman_text: What is the role of Local Gateways in Webex Calling? assistant_text: Local Gateways provide PSTN access and connect Webex Calling to existing on-premises call control services.\nhuman_text: How does Webex Calling handle calls between different customers? assistant_text: Calls between different Webex Calling customers are routed through the PSTN.\nhuman_text: What types of phones are supported by Webex Calling? assistant_text: Webex Calling supports all models of Cisco Multiplatform Phones (MPP).\nhuman_text: What is Webex Edge Connect? assistant_text: Webex Edge Connect peers Webex meetings and Webex Calling traffic with an Equinix Cloud Exchange location, improving user experience with guaranteed bandwidth and QoS.\nhuman_text: How can video calls be made in Webex Calling? assistant_text: Video calls in Webex Calling can be made between video capable MPP phones, Webex Devices, and Webex App.\nhuman_text: What is the Dedicated Instance option in Webex Calling? assistant_text: The Dedicated Instance option provides a Cisco Unified Communications Manager based stack of applications in a private cloud dedicated to a single customer.\nhuman_text: How many participants can Webex Meetings support when added to Webex Calling? assistant_text: Webex Meetings can support up to 1000 meeting participants.\n</code></pre> <p>Save the file as example.txt as we will be using it in the next step</p> <p>To finalize, we again convert the above dataset (example.txt) into Llama2 format. Lets look into those steps</p>"},{"location":"Task8a/#convert-dataset-into-llama2-format-and-upload-on-hugging-face","title":"Convert dataset into Llama2 format and upload on Hugging Face","text":"<ul> <li>Open Google Colab and create a new notebook. Click on \"File\" &gt; \"New notebook\" .Go to the \"Secrets\" section in the sidebar and ensure the Huuging Face toggle is enabled. Please refer to the following section to create Google Colab account.</li> </ul> <ul> <li>Make sure you are connected to a runtime. For this task, you can use the CPU as the runtime environment.</li> </ul> <ul> <li>Click on Folder and create a new folder called \"data\"</li> </ul> <ul> <li>Click on [...], select Upload</li> </ul> <ul> <li>Choose your example.txt file and click Open</li> </ul> <p>Note\u00a0 Ensure that your files are saved elsewhere. This runtime's files will be deleted when this runtime is terminated.</p> <p>Summary: So far, we have our raw dataset as example.txt, enabled Hugging Face in our Colab notebook, and uploaded the data into our folder. we will be convertig the dataset (example.txt) into Llama2 format and uploading on Hugging Face so it can be used for our fine tuning in the upcoming steps</p> <p> We will start by installing specific Python packages. </p> <pre><code>!pip install datasets huggingface_hub google-colab\n</code></pre> <p></p> <ul> <li> <p>The ! at the beginning is used in Jupyter notebooks or Google Colab to run shell commands. Using pip install we will install Python packages  </p> </li> <li> <p>datasets: This package is part of the Hugging Face ecosystem and provides tools for working with large datasets. It allows users to easily download, preprocess, and manage datasets, especially those used in machine learning and natural language processing (NLP).</p> </li> <li> <p>huggingface_hub: This package provides tools to interact with the Hugging Face Hub. </p> </li> <li> <p>google-colab: This package includes utilities specifically designed for Google Colab, a  Jupyter notebook environment that runs in the cloud.</p> </li> </ul> <p> Step 1: Import the required modules </p> <pre><code># Import required modules\nfrom datasets import Dataset\nfrom huggingface_hub import login\nimport os\nfrom google.colab import userdata\n</code></pre> <ul> <li> <p>from datasets import Dataset: imports the Dataset class from the datasets library. The Dataset class is used to create and manipulate datasets. This library is part of the Hugging Face ecosystem and is especially useful for handling datasets for machine learning and NLP tasks.</p> </li> <li> <p>from huggingface_hub import login: This imports the login function from the huggingface_hub library. The login function is used to authenticate with the Hugging Face Hub, allowing the user to upload and manage models and datasets on the platform.</p> </li> <li> <p>import os: This imports the os module, which provides a way of using operating system-dependent functionality like reading or writing to the file system, environment variables, and more. </p> </li> <li> <p>from google.colab import userdata: imports the userdata module from the google.colab library. The google.colab library contains utilities specifically designed for use with Google Colab. </p> </li> </ul> <p>Step 2:  Retrieve Hugging Face token from Colab secrets </p> <pre><code># Retrieve Hugging Face token from Colab secrets\nos.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n# Login to Hugging Face\nlogin(token=os.environ[\"HF_TOKEN\"])\n</code></pre> <p></p> <ul> <li> <p>This part of the code retrieves a previously stored Hugging Face token from Colab secrets. This token is essential for authenticating with the Hugging Face platform. Storing tokens in Colab secrets is a secure way to handle sensitive information without hardcoding it in your script.</p> </li> <li> <p>Login to Hugging Face: This line logs into Hugging Face using the retrieved token. Logging in allows our code to interact with the Hugging Face Hub, and allow us to upload datasets and models.</p> </li> </ul> <p> Step 3: Define a fuction that will help us convert our example.txt into Llama2 format </p> <pre><code># Define the generator function\ndef data_generator(file_path):\n    with open(file_path, 'r') as file:\n        for line in file:\n            if \"assistant_text: \" in line:\n                parts = line.split(\"assistant_text: \")\n                human_text = parts[0].replace(\"human_text: \", \"\").strip()\n                assistant_text = parts[1].strip()\n                reformatted_segment = f'&lt;s&gt;[INST] {human_text} [/INST] {assistant_text} &lt;/s&gt;'\n                yield {\"reformatted_segment\": reformatted_segment}\n</code></pre> <ul> <li> <p>def data_generator(file_path): defines a function named data_generator that takes a single argument, file_path.</p> </li> <li> <p>with open(file_path, 'r') as file:   opens the file specified by file_path in read mode ('r'). </p> </li> <li> <p>for line in file: iterates over each line in the file. </p> </li> <li> <p>if \"assistant_text: \" in line: checks if the string \"assistant_text: \" is present in the current line (example.txt) file </p> </li> <li> <p>parts = line.split(\"assistant_text: \"): splits the current line into two parts using \"assistant_text: \" as the delimiter.</p> </li> <li> <p>human_text = parts[0].replace(\"human_text: \", \"\").strip(): takes the first part of the split line (parts[0]), replaces \"human_text: \" with an empty string, and then removes any leading or trailing whitespace using the strip() method.</p> </li> <li> <p>assistant_text = parts[1].strip(): takes the second part of the split line (parts[1]) and removes any leading or trailing whitespace using the strip() method. </p> </li> <li> <p>reformatted_segment =  Creates a formatted string. The human_text and assistant_text variables are inserted into the string at the specified locations, to replicate Llama2 format.</p> </li> <li> <p>yield {\"reformatted_segment\": reformatted_segment}: the yield statement  return a dictionary containing the reformatted_segment. The yield statement makes this function a generator, allowing it to produce a sequence of values over time, rather than returning them all at once.</p> </li> </ul> <p> Step 4: Define the path for the file that we want to convert </p> <pre><code># Path to your data file\nfile_path = '/content/data/example.txt'\n</code></pre> <p> Note: How to get path for your file in Google COlab </p> <ul> <li>RIght click on your file (example.txt) and select Copy Path</li> </ul> <p></p> <p> Step 5: Calling data_generator function </p> <pre><code># Create the dataset from the generator\nds = Dataset.from_generator(data_generator, gen_kwargs={\"file_path\": file_path})\n</code></pre> <ul> <li>Creating a Dataset object by passing the file (example.txt) to the data_generator function</li> </ul> <p> Step 6: Verify if our data formatted </p> <pre><code># Access the dataset\nprint(ds[2])\n</code></pre> <p></p> <p> Step 7: Verify values in our varaiable ds </p> <pre><code># Access the dataset\nprint(ds)\n</code></pre> <p></p> <p> Step 8: Consolidates all the data and create new Dataset </p> <pre><code>reformatted_segments_list = []\n# Iterate through the dataset and collect reformatted segments\nfor example in ds:\n    reformatted_segment = example[\"reformatted_segment\"]\n    reformatted_segments_list.append(reformatted_segment)\n# Now you have all reformatted segments in reformatted_segments_list\nprint(\"Total reformatted segments:\", len(reformatted_segments_list))\nprint(\"First reformatted segment example:\", reformatted_segments_list[0])\n# Create a new Dataset object with these reformatted segments\nreformatted_ds = Dataset.from_dict({\"text\": reformatted_segments_list})  # Assuming downstream processes expect 'text'\n</code></pre> <ul> <li> <p>The  above code snippet iterates over the original dataset (ds), extracts the reformatted segments, and collects them into a list (reformatted_segments_list). This process consolidates all the relevant data into a single list for further processing.</p> </li> <li> <p>After collecting all the reformatted segments, we will create a new Dataset object (reformatted_ds) from the list. This new dataset is structured in a way that is required by downstream processes when fine tuning starts.</p> </li> </ul> <p></p> <p> Step 9: Push our new Dataset to Hugging Face Hub so it can be processed and used for fine tuning </p> <pre><code># Push the dataset to the Hub\nreformatted_ds.push_to_hub(\"WebexOne/test\")\n</code></pre> <p></p> <ul> <li> <p>The push_to_hub method is used to upload the dataset reformatted_ds to the Hugging Face Hub. This makes the dataset publicly available (or private, depending on the repository settings) for others to access and use.</p> </li> <li> <p>Repository Naming: The string \"WebexOne/test\" specifies the target repository on the Hugging Face Hub. </p> </li> </ul> <p>Let's login to Hugging Face and view our uploaded model</p> <p></p> <p>Our uploaded Dataset</p> <p></p> <p>Complete Code - FOR REFERENCE ONLY</p> <pre><code>!pip install datasets huggingface_hub google-colab\n# Import required modules\nfrom datasets import Dataset\nfrom huggingface_hub import login\nimport os\nfrom google.colab import userdata\n# Retrieve Hugging Face token from Colab secrets\nos.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')\n# Login to Hugging Face\nlogin(token=os.environ[\"HF_TOKEN\"])\n# Define the generator function\ndef data_generator(file_path):\n    with open(file_path, 'r') as file:\n        for line in file:\n            if \"assistant_text: \" in line:\n                parts = line.split(\"assistant_text: \")\n                human_text = parts[0].replace(\"human_text: \", \"\").strip()\n                assistant_text = parts[1].strip()\n                reformatted_segment = f'&lt;s&gt;[INST] {human_text} [/INST] {assistant_text} &lt;/s&gt;'\n                yield {\"reformatted_segment\": reformatted_segment}\n# Path to your data file\nfile_path = '/content/data/example.txt'\n# Create the dataset from the generator\nds = Dataset.from_generator(data_generator, gen_kwargs={\"file_path\": file_path})\n\nreformatted_segments_list = []\n# Iterate through the dataset and collect reformatted segments\nfor example in ds:\n    reformatted_segment = example[\"reformatted_segment\"]\n    reformatted_segments_list.append(reformatted_segment)\n\n# Now you have all reformatted segments in reformatted_segments_list\nprint(\"Total reformatted segments:\", len(reformatted_segments_list))\nprint(\"First reformatted segment example:\", reformatted_segments_list[0])\n\n# Create a new Dataset object with these reformatted segments\nreformatted_ds = Dataset.from_dict({\"text\": reformatted_segments_list})  # Assuming downstream processes expect 'text'\n\n# Save the dataset locally - OPTIONAL STEP\nreformatted_ds.save_to_disk('/content/data/reformatted_dataset')\n\n# Push the dataset to the Hub\nreformatted_ds.push_to_hub(\"WebexOne/test\")\n</code></pre>"},{"location":"conclusion/","title":"Conclusion","text":"<p>Thank you for taking the time to learn about the ThousandEyes Endpoint Agent. Please  provide feedback as I'm always looking at ways I can improve this  content. If you have ideas where you think the ThosuandEyes Endpoint Agent could be improved please submit them as well so we can better help you solve your end users issues and provide you more time to do the things you enjoy.</p>"},{"location":"overview/","title":"Overview - Understanding AI and Its Integration with Webex","text":"<p>Artificial Intelligence (AI) is transforming the way we work, enabling innovative solutions and enhancing productivity. This lab will guide you through the fundamental concepts of AI, various techniques, and how AI can be integrated with Webex to create efficient workflows.</p>"},{"location":"overview/#upon-completion-of-this-lab-you-will-be-able-to","title":"Upon completion of this lab you will be able to","text":"<ul> <li>Understand the basics of AI and its applications</li> <li>Learn about embedding techniques</li> <li>Explore vector databases</li> <li>Gain insights into Generative AI models</li> <li>Familiarize yourself with different AI frameworks</li> <li>Integrate AI with Webex to create seamless workflows</li> <li>Develop hands-on skills through practical exercises</li> <li>Understand Fine-tuning and Quantization</li> <li>Deploy Fine Tuning techniques</li> </ul>"},{"location":"overview/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic understanding of AI concepts is helpful but not required.</li> </ul>"},{"location":"overview/#disclaimer","title":"Disclaimer","text":"<p>The lab design and configuration examples provided are for reference purposes only. This is a sample deployment, and not all recommended features are used or enabled optimally. For design-related questions, please contact your representative at Cisco or a Cisco partner or TME team.</p>"},{"location":"overview/#lab-overview-enabling-hybrid-work-with-thousandeyes","title":"Lab Overview - Enabling Hybrid Work with ThousandEyes","text":"<ul> <li>Lab Login and Setup</li> <li>Quick AI Overview</li> <li>Configure and Access the Lab Systems</li> <li>Deploy AI Models and Techniques</li> <li>Set up and Configure AI Monitoring</li> <li>Review AI Integration with Webex</li> <li>Analyze Data and Optimize Workflows</li> <li>Wrap up and End the Lab</li> </ul> <p>Let's get started! Click on Task 1 - Google Collab- Accessing Google Collab and creating account.</p>"}]}