
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Webex and GenAI">
      
      
        <meta name="author" content="Omer Ilyas">
      
      
      
        <link rel="prev" href="../Task8d/">
      
      
        <link rel="next" href="../Task20/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.29">
    
    
      
        <title>Task 9 - GenAI Frameworks - Collaboration - Technical Marketing Engineering</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.76a95c52.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="custom" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#genai-frameworks" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Collaboration - Technical Marketing Engineering" class="md-header__button md-logo" aria-label="Collaboration - Technical Marketing Engineering" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Collaboration - Technical Marketing Engineering
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Task 9  - GenAI Frameworks
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="custom" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="custom" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../overview/" class="md-tabs__link">
          
  
    
  
  Lab

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


  

<nav class="md-nav md-nav--primary md-nav--lifted md-nav--integrated" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Collaboration - Technical Marketing Engineering" class="md-nav__button md-logo" aria-label="Collaboration - Technical Marketing Engineering" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    Collaboration - Technical Marketing Engineering
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Lab
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Lab
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../overview/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Overview
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Task1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Task 1  - Lab Setup - Google Colab, Hugging Face, Langchain, Ollama
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Task2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Task 2  - AI/ML Revolution Unveiled
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Task3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Task 3  - Tokenization
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Task4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Task 4  - Embeddings and Vector Database
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Task5/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Task 5  - Context Windows and Retrieval Augmented Generation (RAG)
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Task6/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Task 6  - Multimodal RAG
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Task7/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Task 7  - Understanding Fine-Tuning for Large Language Models
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Task8/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Task 8  - Fine Tuning - Deep Dive into Quantization , LoRA and SFT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Task8a/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Task 8a - Configuring and Fine Tuning - Using llama2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Task8b/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Task 8b - Configuring and Fine Tuning - Using llama3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Task8c/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Task 8c - Configuring and Fine Tuning - Using ChatGPT
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Task8d/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Task 8d - Model Merging
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Task 9  - GenAI Frameworks
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Task 9  - GenAI Frameworks
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#langchain" class="md-nav__link">
    <span class="md-ellipsis">
      Langchain
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#langchain-usecase-1" class="md-nav__link">
    <span class="md-ellipsis">
      Langchain - UseCase-1
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Langchain - UseCase-1">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#task-1-log-into-the-lab-environment" class="md-nav__link">
    <span class="md-ellipsis">
      Task 1: Log into the Lab Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-openai-token" class="md-nav__link">
    <span class="md-ellipsis">
      Set OpenAI token
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-langchain-tokens" class="md-nav__link">
    <span class="md-ellipsis">
      Set Langchain tokens
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set-ngrok" class="md-nav__link">
    <span class="md-ellipsis">
      Set NGROK
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuration" class="md-nav__link">
    <span class="md-ellipsis">
      Configuration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuration1" class="md-nav__link">
    <span class="md-ellipsis">
      Configuration1
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#enhancing-our-streamlit-app-to-send-messages-to-a-webex-space" class="md-nav__link">
    <span class="md-ellipsis">
      Enhancing Our Streamlit App to Send Messages to a Webex Space
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#building-using-ollama-modelsopensource-optional" class="md-nav__link">
    <span class="md-ellipsis">
      Building using Ollama Models(opensource) - OPTIONAL
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#langchain-usecase-2-advanced-rag" class="md-nav__link">
    <span class="md-ellipsis">
      Langchain - UseCase-2 - Advanced Rag
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Langchain - UseCase-2 - Advanced Rag">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#task-1-log-into-the-lab-environment_1" class="md-nav__link">
    <span class="md-ellipsis">
      Task 1: Log into the Lab Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuration_1" class="md-nav__link">
    <span class="md-ellipsis">
      Configuration
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summary" class="md-nav__link">
    <span class="md-ellipsis">
      Summary
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Task20/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Task 20 - Log out and Shutdown the Lab
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../conclusion/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Conclusion
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="genai-frameworks">GenAI- Frameworks</h1>
<p>(also explai langchian- langsmith - then show Fusion MultiQuery, raptor)</p>
<p>In the previous sections of this lab, we started by exploring  basics of neural networks/tokenization, which are key to understanding how modern AI models, particularly large language models (LLMs), operate. We moved on to embeddings and vector databases, which play a vital role in how these models represent and retrieve information.</p>
<p>We also looked into the concept of context windows and how they influence model performance. To address some of the limitations inherent in traditional context windows, we introduced Retrieval Augmented Generation (RAG). This approach enables models to dynamically retrieve relevant information, significantly improving their ability to generate accurate and contextually appropriate responses. To make you understand better, we even built a quick RAG application. Additionally, we touched on multimodal RAG, which takes this a step further by enabling the model to process and integrate information from multiple sources, such as text, images, and more, making it even more versatile.</p>
<p>While we’ve covered the basics, it’s crucial to recognize that the field of AI is rapidly advancing. Companies like OpenAI, Mistral, Google, Meta, and others are in a race to develop the most sophisticated large language models (LLMs). </p>
<p><img alt="genai1" src="../assets/task9/genai1.png" /></p>
<p>However, effectively using these powerful LLMs in real-world scenarios involves more than just sending and receiving API requests. Challenges such as fine-tuning models, integrating them with third-party systems e.g. Webex, and ensuring they perform well in complex environments require more than what LLMs alone can offer. </p>
<p><img alt="genai2" src="../assets/task9/genai2.png" /></p>
<p>This is where GenAI frameworks come into play, providing the essential infrastructure to bridge the gap between raw LLM capabilities and practical, scalable AI solutions. There are numerous frameworks available for use.</p>
<p><img alt="genai3" src="../assets/task9/genai3.png" /></p>
<p>The question of which framework to choose ultimately depends on your specific preferences and needs. Many of these frameworks have overlapping functionalities. In this section, we'll focus on <a href="https://python.langchain.com/v0.1/docs/get_started/introduction">LangChain</a>, exploring what it is and how it can be used to create AI-powered applications.</p>
<h2 id="langchain">Langchain</h2>
<p>LangChain is an open-source framework that enables developers to integrate large language models (LLMs) with external data sources and computational resources. Available as a Python, JavaScript, or TypeScript package, it offers a versatile pipeline abstraction that can be used for various purposes, from creating AI enabled applications to managing data ingestion from external sources.</p>
<p>While LLM like (GPT) has impressive general knowledge, LangChain is essential for retrieving information from your own documents or fine-tuning models for specific tasks. It allows you to connect LLMs to your data sources, enabling actions like sending messages to a Webex Space. The process involves breaking your data or documents into smaller chunks and storing them in a vector database as embeddings. LangChain supports various vector stores, such as Chroma, Faiss, and Cassandra e.t.c making data retrieval and action-taking both efficient and seamless. <a href="https://python.langchain.com/v0.1/docs/get_started/introduction">More info on langchain can be found here.</a></p>
<p><img alt="genai5" src="../assets/task9/genai5.png" /></p>
<p><img alt="genai6" src="../assets/task9/genai6.png" /></p>
<p><img alt="genai7" src="../assets/task9/genai7.png" /></p>
<p><span class="colour" style="color:red">More info about Embeddings can be found in <a href="../Task4/">Task4</a></span></p>
<p><img alt="genai8" src="../assets/task9/genai8.png" /></p>
<p><img alt="genai9" src="../assets/task9/genai9.png" /></p>
<p>Below is a summary from a <a href="https://blog.langchain.dev/langchain-state-of-ai-2023/">Langchain blog</a> that outlines the most commonly used LLMs, embeddings, and vector stores.</p>
<p><img alt="G1" src="../assets/task9/G1.png" /></p>
<p><img alt="G2" src="../assets/task9/G2.png" /></p>
<p><img alt="G3" src="../assets/task9/G3.png" /></p>
<p>The beauty of Langchain lies in its ability to provide a comprehensive ecosystem, regardless of the LLM model you choose to work with. Within the Langchain ecosystem, there's a tool called Langsmith, which is designed for monitoring and debugging your applications, think of it as LLM ops. If you're building applications as APIs, Langchain offers Langserve (create services in the form of APIs). Additionally, the ecosystem includes agents, chains, retrievers, and LLM tools, making it a versatile platform for various NLP tasks.</p>
<p><img alt="G6" src="../assets/task9/G6.png" /></p>
<p><span class="colour" style="color:red">Note: Chains are a sequence of steps or operations that are linked together to achieve a particular task. Each step in the chain can involve various processes, such as invoking a language model, performing a calculation, calling an external API, or manipulating data. Chains allow developers to structure these steps in a way that the output of one step becomes the input of the next, creating a pipeline of operations. </span></p>
<p><img alt="G4" src="../assets/task9/G4.png" /></p>
<p><img alt="G5" src="../assets/task9/G5.png" /></p>
<p><span class="colour" style="color:red">Note: The above image was obtained from <a href="https://js.langchain.com/v0.1/docs/get_started/introduction">Langchain</a></p>
<h2 id="langchain-usecase-1">Langchain - UseCase-1</h2>
<p><span class="colour" style="color:red">Note: Make sure you have created a Langchain account as explained in the "Getting Started with LangChain" section in <a href="../Task1/">Task1</a></span></p>
<p>Let's start building chatbot application using LangChain, I'll guide you through the basic steps. </p>
<p>With LangChain, when creating an application, you have the flexibility to use either paid LLMs or open-source LLMs. One way to integrate open-source LLMs is through Hugging Face, but since we're focusing on LangChain, we'll explore how this can be done within its ecosystem.</p>
<p><img alt="G8" src="../assets/task9/G8.png" /></p>
<p>We’ll build a chatbot using both paid models (like OpenAI) and open-source LLMs. LangChain provides modules that can seamlessly interact with both paid services and open-source models, allowing you to create versatile and powerful applications.</p>
<p><img alt="G7" src="../assets/task9/G7.png" /></p>
<p>In this example, we'll explore the use of Langchain and Langserve, starting with the installation of essential libraries like langchain_openai, langchain, and langchain_core. We'll demonstrate how to leverage Langserve for managing large language models (LLM) operations. Furthermore, I'll show you how to utilize Streamlit, a server-based application, to interactively visualize and interact with your code. Given the challenges of running Streamlit directly on Google Colab, we'll deploy NGROK to facilitate executing and accessing our code through a web interface.</p>
<h3 id="task-1-log-into-the-lab-environment">Task 1: Log into the Lab Environment</h3>
<ul>
<li>Open Google Colab and create a new notebook or use an existing one . Click on "File" &gt; "New notebook". Please refer to the <a href="../Task1/">following section</a> to create Google Colab account.</li>
</ul>
<p><img alt="GCOLAB" src="../assets/task3/gcolab.png" /></p>
<ul>
<li>Make sure you are connected to a runtime. For this task, you can use the CPU as the runtime environment.</li>
</ul>
<p><img alt="HL_Format_run" src="../assets/task8a/rungc.png" /></p>
<h3 id="set-openai-token">Set OpenAI token</h3>
<p><span class="colour" style="color:orange">Note: Skip this step if you have already created the OpenAI account. If this is the first  module you working with please, create an account from the <a href="https://platform.openai.com/">OpenAI official website</a>.</span></p>
<ul>
<li>Create a new project API key by browsing to <a href="https://platform.openai.com/api-keys">API Keys web page</a>. Select Create new secret key. The API key is automatically generated. Save the APi Key as we will be using it in the later steps .</li>
</ul>
<p><img alt="GPT1_apiKey" src="../assets/task8c/ap.png" /></p>
<ul>
<li>Within your existing Google Colab notebook navigate to the new “Secrets” section in the sidebar.</li>
</ul>
<p><img alt="HF_GT_sec" src="../assets/task8a/gensec.png" /></p>
<ul>
<li>
<p>Click on “Add a new secret.” Enter the name example: OPENAI_API_KEY and value of the secret(the API key created above). Note: The name is permanent once set. </p>
</li>
<li>
<p>The list of secrets is global across all your notebooks.</p>
</li>
<li>
<p>Use the “Notebook access” toggle to grant or revoke access to a secret for each notebook.</p>
</li>
</ul>
<p><img alt="HF_GT_sec_created_8c" src="../assets/task8c/kei.png" /></p>
<h3 id="set-langchain-tokens">Set Langchain tokens</h3>
<p><span class="colour" style="color:red">Note: Make sure you have created a Langchain account as explained in the "Getting Started with LangChain" section in <a href="../Task1/">Task1</a></span></p>
<ul>
<li>Copy your Langchain Key and save it </li>
</ul>
<p><img alt="key" src="../assets/task9/keyls.png" /></p>
<p><img alt="key1" src="../assets/task9/keyls1.png" /></p>
<ul>
<li>Within your existing Google Colab notebook navigate again to the new “Secrets” section in the sidebar. Click on “Add a new secret.” Enter the name example: LANGCHAIN_API_KEY and value of the secret(the API key created in Langchain). </li>
</ul>
<p><img alt="HF_GT_sec" src="../assets/task8a/gensec.png" /></p>
<p><img alt="key11" src="../assets/task9/key11.png" /></p>
<h3 id="set-ngrok">Set NGROK</h3>
<ul>
<li>
<p>Browse to <a href="https://ngrok.com/">NGROK</a> and click on Login</p>
</li>
<li>
<p>I’ll be using the "Login with Google" option to access my account, but feel free to choose the login method that works best for you.</p>
</li>
</ul>
<p><img alt="goolo" src="../assets/task9/goolo.png" /></p>
<ul>
<li>Get your NGROK auth key</li>
</ul>
<p><img alt="ngrk1" src="../assets/task9/ngrk1.png" /></p>
<p><span class="colour" style="color:red"> Note: Since we are using a free Ngrok account, we can only run up to 3 tunnels simultaneously. If you need to terminate any active tunnels, please use the following steps </span></p>
<div class="highlight"><pre><span></span><code><span class="err">!</span><span class="n">pkill</span> <span class="n">ngrok</span>
</code></pre></div>
<h3 id="configuration">Configuration</h3>
<ul>
<li>Lets start by installing all the necessary packages</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="err">!</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">langchain_openai</span> <span class="n">langchain</span> <span class="n">langchain_core</span> <span class="n">python</span><span class="o">-</span><span class="n">dotenv</span> <span class="n">streamlit</span> <span class="n">langchain_community</span> <span class="n">langserve</span> <span class="n">streamlit</span> <span class="n">pyngrok</span> <span class="n">ngrok</span>
</code></pre></div></td></tr></table></div>
<ul>
<li>We will import libraries</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">streamlit</span> <span class="k">as</span> <span class="nn">st</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">userdata</span>
<span class="kn">from</span> <span class="nn">pyngrok</span> <span class="kn">import</span> <span class="n">ngrok</span>
<span class="kn">import</span> <span class="nn">subprocess</span>
<span class="kn">import</span> <span class="nn">os</span>
</code></pre></div></td></tr></table></div>
<p><img alt="lib" src="../assets/task9/lib.png" /></p>
<ul>
<li>Lets start by configuring the environment variables. We will use and define Langserve for our LLM ops</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">userdata</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;OPENAI_API_KEY&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">userdata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;OPENAI_API_KEY&#39;</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;LANGCHAIN_API_KEY&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">userdata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;LANGCHAIN_API_KEY&#39;</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;true&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_PROJECT&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WebexOne&quot;</span>
</code></pre></div></td></tr></table></div>
<p><span class="colour" style="color:red"> Note: Due to the challenges of running Streamlit directly on Google Colab, we'll use NGROK to enable the execution and access of our code via a web interface. The code below is for your understanding or if you're running it as Python packages in your own environment. To continue and run the code in this lab or on Google Colab, please proceed and copy the code from Configuration1 section. </span></p>
<ul>
<li>(Optional) As we creating a simple chatbot application lets create our prompt</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">prompt</span><span class="o">=</span><span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span><span class="s2">&quot;You are a helpful Cisco assistant. Please responsd to the user queries&quot;</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="s2">&quot;Question:</span><span class="si">{question}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="p">]</span>
<span class="p">)</span>
</code></pre></div></td></tr></table></div>
<ul>
<li>(Optional) Lets define Streamlit Framework</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">st</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Langchain using OPENAI API&#39;</span><span class="p">)</span>
<span class="n">input_text</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">text_input</span><span class="p">(</span><span class="s2">&quot;Search the topic u want&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<ul>
<li>(Optional) We will initialize the Language Model and set-up the Output parser </li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">llm</span><span class="o">=</span><span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o&quot;</span><span class="p">)</span>
<span class="n">output_parser</span><span class="o">=</span><span class="n">StrOutputParser</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<ul>
<li>(Optional) Langchain offers features that allow us to connect various components into a seamless workflow, known as chains. So far, we have created a chat prompt template, initialized the LLM (Language Model), and set up an Output Parser. Now, let's combine all these elements.</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">chain</span><span class="o">=</span><span class="n">prompt</span><span class="o">|</span><span class="n">llm</span><span class="o">|</span><span class="n">output_parser</span>
</code></pre></div></td></tr></table></div>
<ul>
<li>(Optional) Lets give our input as question and pass it to our chain </li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">if</span> <span class="n">input_text</span><span class="p">:</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s1">&#39;question&#39;</span><span class="p">:</span> <span class="n">input_text</span><span class="p">})</span>
    <span class="n">st</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
    <span class="n">send_webex_message</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p><img alt="ress1" src="../assets/task9/ress1.png" /></p>
<h3 id="configuration1">Configuration1</h3>
<ul>
<li>
<p>Running Streamlit apps directly within Google Colab can be challenging due to the need for a continuous web interface, which is not natively supported in Colab's environment. To address this, we will use NGROK, which creates a secure tunnel to make the Streamlit app accessible via a web interface.</p>
</li>
<li>
<p>The code will start by creating a Streamlit application using a prompt template, a language model (LLM), and an output parser. These components are combined into a chain to process user input and generate responses using  GPT-4o model. The app will prompt the user to input a query and then processes it through the chain, displaying the generated response in the Streamlit interface. We will save Streamlit app code to a file (app.py), and a subprocess is started to run the app.</p>
</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">streamlit_code</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">from langchain_openai import ChatOpenAI</span>
<span class="s2">from langchain_core.prompts import ChatPromptTemplate</span>
<span class="s2">from langchain_core.output_parsers import StrOutputParser</span>
<span class="s2">import requests</span>
<span class="s2">import streamlit as st</span>
<span class="s2">import os</span>
<span class="s2">from dotenv import load_dotenv</span>

<span class="s2">load_dotenv()</span>
<span class="s2"># Set up the prompt template</span>
<span class="s2">prompt = ChatPromptTemplate.from_messages(</span>
<span class="s2">    [</span>
<span class="s2">        (&quot;system&quot;, &quot;You are a helpful Cisco Live assistant. Please respond to the user queries&quot;),</span>
<span class="s2">        (&quot;user&quot;, &quot;Question:</span><span class="si">{question}</span><span class="s2">&quot;)</span>
<span class="s2">    ]</span>
<span class="s2">)</span>

<span class="s2"># Streamlit interface</span>
<span class="s2">st.title(&#39;Langchain using OPENAI API&#39;)</span>
<span class="s2">input_text = st.text_input(&quot;Search the topic you want&quot;)</span>

<span class="s2"># Initialize the OpenAI LLM</span>
<span class="s2">llm = ChatOpenAI(model=&quot;gpt-4o&quot;)</span>
<span class="s2">output_parser = StrOutputParser()</span>

<span class="s2"># Create the chain</span>
<span class="s2">chain = prompt | llm | output_parser</span>

<span class="s2">if input_text:</span>
<span class="s2">    response = chain.invoke({&#39;question&#39;: input_text})</span>
<span class="s2">    st.write(response)</span>
<span class="s2">    send_webex_message(response)</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="c1"># Save the streamlit app code to a file</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;app.py&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">streamlit_code</span><span class="p">)</span>

<span class="c1"># Start the Streamlit app using subprocess</span>
<span class="n">process</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">Popen</span><span class="p">([</span><span class="s1">&#39;streamlit&#39;</span><span class="p">,</span> <span class="s1">&#39;run&#39;</span><span class="p">,</span> <span class="s1">&#39;app.py&#39;</span><span class="p">])</span>

<span class="c1"># Start ngrok tunnel</span>
<span class="n">ngrok</span><span class="o">.</span><span class="n">set_auth_token</span><span class="p">(</span><span class="s2">&quot;Replace-with-your-ngrok-authtoken-created-above&quot;</span><span class="p">)</span>  
<span class="n">public_url</span> <span class="o">=</span> <span class="n">ngrok</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="mi">8501</span><span class="p">,</span> <span class="s2">&quot;http&quot;</span><span class="p">)</span>  <span class="c1"># Use http instead of specifying as a named argument</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Streamlit app is live at: </span><span class="si">{</span><span class="n">public_url</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p><img alt="ss" src="../assets/task9/ss.png" /></p>
<p><img alt="ss1" src="../assets/task9/ss1.png" /></p>
<p><img alt="ss2" src="../assets/task9/ss2.png" /></p>
<ul>
<li>Since we're using <a href="https://smith.langchain.com/">Langserve for llm-ops</a>, let's log in and explore its features there.</li>
</ul>
<p><img alt="ss3" src="../assets/task9/ss3.png" /></p>
<p><img alt="ss4" src="../assets/task9/ss4.png" /></p>
<p><img alt="ss5" src="../assets/task9/ss5.png" /></p>
<p><img alt="ss6" src="../assets/task9/ss6.png" /></p>
<p>This dashboard provides a summary of the performance, usage, and cost metrics for the "WebexOne" project within the last 7 days. It offers insights into how frequently the project has been run, the efficiency (in terms of error rates and latency), and the associated costs, helping users to monitor and optimize their LLM (Large Language Model) operations.</p>
<h3 id="enhancing-our-streamlit-app-to-send-messages-to-a-webex-space">Enhancing Our Streamlit App to Send Messages to a Webex Space</h3>
<p>We’ve seen how simple it is to interact with LLM and create an application using Langchain + Streamlit. Now, let’s take it a step further by modifying the application so that the responses are not only displayed in the Streamlit interface but are also sent directly to a specific Webex space.</p>
<p><span class="colour" style="color:red"> Note: Ensure you have the Webex Bearer Token and Space ID available. If you don't have them, please reach out to your lab proctor. </span></p>
<ul>
<li>let's modify our code to send the generated responses directly to a Webex space. This will enable real-time collaboration and communication with team members through Webex, enhancing the utility of our AI applications.</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="c1"># Create the Streamlit app code</span>
<span class="n">streamlit_code</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">from langchain_openai import ChatOpenAI</span>
<span class="s2">from langchain_core.prompts import ChatPromptTemplate</span>
<span class="s2">from langchain_core.output_parsers import StrOutputParser</span>
<span class="s2">import requests</span>
<span class="s2">import streamlit as st</span>
<span class="s2">import os</span>
<span class="s2">from dotenv import load_dotenv</span>

<span class="s2">load_dotenv()</span>

<span class="s2"># Webex setup</span>
<span class="s2">webex_access_token = &quot;Your-Webex-Token&quot;</span>
<span class="s2">webex_space_id = &quot;Your-Space-ID&quot;</span>

<span class="s2"># Function to send a message to Webex</span>
<span class="s2">def send_webex_message(content):</span>
<span class="s2">    url = &quot;https://webexapis.com/v1/messages&quot;</span>
<span class="s2">    headers = {</span>
<span class="s2">        &quot;Authorization&quot;: f&quot;Bearer </span><span class="si">{webex_access_token}</span><span class="s2">&quot;,</span>
<span class="s2">        &quot;Content-Type&quot;: &quot;application/json&quot;</span>
<span class="s2">    }</span>
<span class="s2">    payload = {</span>
<span class="s2">        &quot;roomId&quot;: webex_space_id,</span>
<span class="s2">        &quot;text&quot;: content</span>
<span class="s2">    }</span>
<span class="s2">    response = requests.post(url, headers=headers, json=payload)</span>
<span class="s2">    if response.status_code == 200:</span>
<span class="s2">        print(&quot;Message sent to Webex:&quot;, response.json())</span>
<span class="s2">    else:</span>
<span class="s2">        print(&quot;Failed to send message:&quot;, response.text)</span>

<span class="s2"># Set up the prompt template</span>
<span class="s2">prompt = ChatPromptTemplate.from_messages(</span>
<span class="s2">    [</span>
<span class="s2">        (&quot;system&quot;, &quot;You are a helpful Cisco Live assistant. Please respond to the user queries&quot;),</span>
<span class="s2">        (&quot;user&quot;, &quot;Question:</span><span class="si">{question}</span><span class="s2">&quot;)</span>
<span class="s2">    ]</span>
<span class="s2">)</span>

<span class="s2"># Streamlit interface</span>
<span class="s2">st.title(&#39;Langchain using OPENAI API&#39;)</span>
<span class="s2">input_text = st.text_input(&quot;Search the topic you want&quot;)</span>

<span class="s2"># Initialize the OpenAI LLM</span>
<span class="s2">llm = ChatOpenAI(model=&quot;gpt-4o&quot;)</span>
<span class="s2">output_parser = StrOutputParser()</span>

<span class="s2"># Create the chain</span>
<span class="s2">chain = prompt | llm | output_parser</span>

<span class="s2">if input_text:</span>
<span class="s2">    response = chain.invoke({&#39;question&#39;: input_text})</span>
<span class="s2">    st.write(response)</span>
<span class="s2">    send_webex_message(response)</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="c1"># Save the streamlit app code to a file</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;app.py&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">streamlit_code</span><span class="p">)</span>

<span class="c1"># Start the Streamlit app using subprocess</span>
<span class="n">process</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">Popen</span><span class="p">([</span><span class="s1">&#39;streamlit&#39;</span><span class="p">,</span> <span class="s1">&#39;run&#39;</span><span class="p">,</span> <span class="s1">&#39;app.py&#39;</span><span class="p">])</span>

<span class="c1"># Start ngrok tunnel</span>
<span class="n">ngrok</span><span class="o">.</span><span class="n">set_auth_token</span><span class="p">(</span><span class="s2">&quot;2lVZ7NIouJBlvkWTx7QNxNmpEvV_5HyXsbHunJSaJN2BPe4Ry&quot;</span><span class="p">)</span>  <span class="c1"># Replace with your ngrok authtoken</span>
<span class="n">public_url</span> <span class="o">=</span> <span class="n">ngrok</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="mi">8501</span><span class="p">,</span> <span class="s2">&quot;http&quot;</span><span class="p">)</span>  <span class="c1"># Use http instead of specifying as a named argument</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Streamlit app is live at: </span><span class="si">{</span><span class="n">public_url</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p><img alt="ss7" src="../assets/task9/ss7.png" /></p>
<p><img alt="ss8" src="../assets/task9/ss8.png" /></p>
<p><img alt="ss9" src="../assets/task9/ss9.png" /></p>
<h3 id="building-using-ollama-modelsopensource-optional">Building using Ollama Models(opensource) - OPTIONAL</h3>
<p><span class="colour" style="color:red"> Note: The code below will work if you are running Ollama locally on your machine as explained in <a href="../Task1/">Task1</a></span></p>
<p>In this example, we're leveraging LangChain to build an AI assistant using the LLama3 model, accessible through the Ollama API. The assistant is designed to help with queries and is implemented using Streamlit, which provides a user-friendly interface as shown above. </p>
<p>Note: Make sure you are running Ollama locally on your machine, as explained in <a href="../Task1/">Task1</a>, to ensure the Llama3 model is properly accessible.</p>
<div class="highlight"><table class="highlighttable"><tr><th colspan="2" class="filename"><span class="filename">Optional Code</span></th></tr><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">from</span> <span class="nn">langchain_community.llms</span> <span class="kn">import</span> <span class="n">Ollama</span>
<span class="kn">import</span> <span class="nn">streamlit</span> <span class="k">as</span> <span class="nn">st</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="n">load_dotenv</span><span class="p">()</span>
<span class="c1">## Prompt Template</span>
<span class="n">prompt</span><span class="o">=</span><span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_messages</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;system&quot;</span><span class="p">,</span><span class="s2">&quot;You are a helpful Cisco  assistant. Please responsd to the user queries&quot;</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;user&quot;</span><span class="p">,</span><span class="s2">&quot;Question:</span><span class="si">{question}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">st</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Langchain With LLAMA2 API&#39;</span><span class="p">)</span>
<span class="n">input_text</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">text_input</span><span class="p">(</span><span class="s2">&quot;Search the topic u want&quot;</span><span class="p">)</span>

<span class="c1"># ollama LLAma3 LLm </span>
<span class="n">llm</span><span class="o">=</span><span class="n">Ollama</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3:latest&quot;</span><span class="p">)</span>
<span class="n">output_parser</span><span class="o">=</span><span class="n">StrOutputParser</span><span class="p">()</span>
<span class="n">chain</span><span class="o">=</span><span class="n">prompt</span><span class="o">|</span><span class="n">llm</span><span class="o">|</span><span class="n">output_parser</span>

<span class="k">if</span> <span class="n">input_text</span><span class="p">:</span>
    <span class="n">st</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;question&quot;</span><span class="p">:</span><span class="n">input_text</span><span class="p">}))</span>
</code></pre></div></td></tr></table></div>
<p><img alt="ss10" src="../assets/task9/ss10.png" /></p>
<p><img alt="ss11" src="../assets/task9/ss11.png" /></p>
<h2 id="langchain-usecase-2-advanced-rag">Langchain - UseCase-2 - Advanced Rag</h2>
<p>In <a href="../Task5/">Task5</a>, we explored the basics of a simple Retrieval-Augmented Generation (RAG) application, where we utilized similarity search to find and retrieve information from our database. While this approach is effective for specific, straightforward use cases, it does come with some limitations among them being lack of flexibility.</p>
<p>One of the drawbacks of relying solely on similarity search is that it doesn’t easily accommodate more complex workflows or integration with additional tools. It’s powerful for direct matches but can struggle when the task requires multi-step logic, advanced filtering, or a combination of different data sources.</p>
<p>LangChain’s <a href="https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/">retrievals</a> and <a href="https://python.langchain.com/v0.1/docs/modules/chains/">chains</a>, on the other hand, provide a more versatile and scalable framework. They enable the creation of sophisticated RAG applications by allowing you to define workflows that can incorporate multiple retrieval strategies, conditional logic, and integration with other LLM's or APIs.</p>
<p>Let’s now shift our focus towards creating an advanced RAG application by leveraging the capabilities of LangChain’s chains and retrievers. This will not only improve the flexibility and power of our application but also open up new possibilities for handling complex queries and delivering more refined results.</p>
<p><img alt="ss12" src="../assets/task9/ss12.png" /></p>
<p><strong>Chain: A chain in LangChain is like a sequence of tasks that you want to accomplish step by step.</strong> </p>
<p><strong>Retriever: is like a search engine inside your application. Its job is to find and pull out the most relevant information from a vector database or a collection of documents.</strong></p>
<p><strong>A retrieval chain is when you combine the power of both chains and retrievers.</strong></p>
<h3 id="task-1-log-into-the-lab-environment_1">Task 1: Log into the Lab Environment</h3>
<ul>
<li>
<p>Open Google Colab and start a new notebook, or you can use an existing one. Ensure that your OpenAI and LangChain tokens are already set up and activated for this notebook, as described in Use Case 1.</p>
</li>
<li>
<p>Let's load our PDF files into Google Colab. For this example, we can use the article titled "Cisco Preferred Architecture for Webex Calling". You can  <a download="webex_calling.pdf" href="../assets/static/webex_calling.pdf" target="_blank">download the  article here</a> as we will be using in the next step.</p>
</li>
<li>
<p>Within Google Colab, Click on Folder and create a new folder called "data"</p>
</li>
</ul>
<p><img alt="HL_Format_fold" src="../assets/task8a/fold.png" /></p>
<ul>
<li>Click on [...], select Upload</li>
</ul>
<p><img alt="HL_Format_fold1" src="../assets/task9/fold1.png" /></p>
<ul>
<li>Choose your webex_calling.pdf file and click Open</li>
</ul>
<h3 id="configuration_1">Configuration</h3>
<ul>
<li>Lets start by installing all the packages</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="err">!</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">langchain_openai</span> <span class="n">langchain</span> <span class="n">langchain_core</span> <span class="n">python</span><span class="o">-</span><span class="n">dotenv</span> <span class="n">langchain_community</span> <span class="n">langserve</span> <span class="n">PyPDF</span> <span class="n">chromadb</span>
</code></pre></div></td></tr></table></div>
<ul>
<li>Lets start by configuring the environment variables. We will use and define Langserve for our LLM ops</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">userdata</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;OPENAI_API_KEY&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">userdata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;OPENAI_API_KEY&#39;</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;LANGCHAIN_API_KEY&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">userdata</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;LANGCHAIN_API_KEY&#39;</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_TRACING_V2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;true&quot;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LANGCHAIN_PROJECT&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;WebexOne&quot;</span>
</code></pre></div></td></tr></table></div>
<ul>
<li>We will import libraries</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">ChatOpenAI</span>
<span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
<span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">from</span> <span class="nn">google.colab</span> <span class="kn">import</span> <span class="n">userdata</span>
</code></pre></div></td></tr></table></div>
<p>If we examine the RAG pipeline, we typically need to follow these steps</p>
<p><img alt="ss13" src="../assets/task9/ss13.png" /></p>
<ul>
<li>Lets load our data source</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain_community.document_loaders</span> <span class="kn">import</span> <span class="n">PyPDFLoader</span>
<span class="n">loader</span><span class="o">=</span><span class="n">PyPDFLoader</span><span class="p">(</span><span class="s2">&quot;/content/data/webex_calling.pdf&quot;</span><span class="p">)</span>
<span class="n">docs</span><span class="o">=</span><span class="n">loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
<ul>
<li>We will do transformation to break data into smaller chunks</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain.text_splitter</span> <span class="kn">import</span> <span class="n">RecursiveCharacterTextSplitter</span>
<span class="n">text_splitter</span><span class="o">=</span><span class="n">RecursiveCharacterTextSplitter</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">documents</span><span class="o">=</span><span class="n">text_splitter</span><span class="o">.</span><span class="n">split_documents</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<ul>
<li>We will now convert chunks into embeddings and store in Vector database e.g. Chroma</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain_openai</span> <span class="kn">import</span> <span class="n">OpenAIEmbeddings</span>
<span class="kn">from</span> <span class="nn">langchain_community.vectorstores</span> <span class="kn">import</span> <span class="n">Chroma</span>
<span class="n">db</span> <span class="o">=</span> <span class="n">Chroma</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">,</span><span class="n">OpenAIEmbeddings</span><span class="p">())</span>
</code></pre></div></td></tr></table></div>
<ul>
<li>At this stage, I can query my vector database and retrieve information based on similarity search.</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;Explain me webex calling solution overview&quot;</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">similarity_search</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">page_content</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p><img alt="ss14" src="../assets/task9/ss14.png" /></p>
<ul>
<li>Let's now combine prompts with chains and retrievers to generate responses based on the prompts. Lets start by defining our LLM</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">llm</span><span class="o">=</span><span class="n">ChatOpenAI</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<p><span class="colour" style="color:red">Note: You can also use open source models like Ollama using the following code:
from langchain_community.llms import Ollama 
llm=Ollama(model="llama2") 
</span></p>
<ul>
<li>Let's design our chat prompt templates instead of querying our database using similarity search. We'll be utilizing <a href="https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html">langchain_core.prompts</a> for this purpose.</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">ChatPromptTemplate</span>
<span class="n">prompt</span><span class="o">=</span> <span class="n">ChatPromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">                                         Answer the following question based only on the provided context. If no answer is available just say I dont know.</span>
<span class="s2">                                         &lt;context&gt;</span>
<span class="s2">                                         </span><span class="si">{context}</span>
<span class="s2">                                         &lt;/context&gt;</span>
<span class="s2">                                         Question: </span><span class="si">{input}</span><span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<ul>
<li>Lets create our Chain now. Chains are sequences of operations that process input to produce the desired output. We will use <a href="https://python.langchain.com/v0.1/docs/modules/chains/">create_stuff_documents_chain</a></li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain.chains.combine_documents</span> <span class="kn">import</span> <span class="n">create_stuff_documents_chain</span>
<span class="n">document_chain</span><span class="o">=</span><span class="n">create_stuff_documents_chain</span><span class="p">(</span><span class="n">llm</span><span class="p">,</span><span class="n">prompt</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<ul>
<li>Now, let's define our retrievers. As previously mentioned, a retriever is an interface that returns documents based on an unstructured query. It is broader in scope than a vector store, as it doesn't need to store documents, only to retrieve them. While vector stores can serve as the foundation for a retriever, there are other types of retrievers available as well. For more information, you can refer to the documentation [here] (https://python.langchain.com/docs/modules/data_connection/retrievers/)</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">retriever</span><span class="o">=</span><span class="n">db</span><span class="o">.</span><span class="n">as_retriever</span><span class="p">()</span>
<span class="n">retriever</span>
</code></pre></div></td></tr></table></div>
<p><img alt="ss15" src="../assets/task9/ss15.png" /></p>
<ul>
<li>In this step, we'll set up a retrieval chain. This chain will start by taking a user inquiry and passing it to the retriever to fetch relevant documents. The retrieved documents, along with the original inputs, will then be sent to a language model (LLM) to generate a response. For further details on how to implement a retrieval chain, refer to the <a href="https://python.langchain.com/docs/modules/chains/">following doc</a></li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span>
<span class="normal">2</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="kn">from</span> <span class="nn">langchain.chains</span> <span class="kn">import</span> <span class="n">create_retrieval_chain</span>
<span class="n">retrieval_chain</span><span class="o">=</span><span class="n">create_retrieval_chain</span><span class="p">(</span><span class="n">retriever</span><span class="p">,</span><span class="n">document_chain</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
<ul>
<li>Now, let's combine our retriever and chain to generate responses:</li>
</ul>
<div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">1</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="n">response</span><span class="o">=</span><span class="n">retrieval_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">({</span><span class="s2">&quot;input&quot;</span><span class="p">:</span><span class="s2">&quot; what are the bandwidth required on the internet access for Webex Calling&quot;</span><span class="p">})</span>
</code></pre></div></td></tr></table></div>
<div class="highlight"><pre><span></span><code>response[&#39;answer&#39;]
</code></pre></div>
<p><img alt="ss17" src="../assets/task9/ss17.png" /></p>
<h3 id="summary">Summary</h3>
<p>In this lab, we've built an advanced Retrieval-Augmented Generation (RAG) system. The process begins with a user query, which is passed through a retrieval chain to fetch relevant documents from a vector database. These documents, along with the original query, are then processed by a Language Model (LLM) to generate a detailed and accurate response. This setup allows us to efficiently handle complex queries by combining the strengths of chains, retrieval and language models.</p>
<p><img alt="ss16" src="../assets/task9/ss16.png" /></p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Omer Ilyas - Technical Marketing Engineer - oilyas@cisco.com
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/in/omerilyas4ccie/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "navigation.sections", "toc.integrate", "navigation.tabs.sticky", "navigation.indexes", "navigation.instant", "navigation.top", "search.suggest", "search.highlight", "content.code.copy", "content.tabs.link", "content.code.annotation", "content.code.copy"], "search": "../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
      
    
  </body>
</html>